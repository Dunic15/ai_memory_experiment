% A LaTeX template for MSc Thesis submissions to 
% Politecnico di Milano (PoliMi) - School of Industrial and Information Engineering
%
% S. Bonetti, A. Gruttadauria, G. Mescolini, A. Zingaro
% e-mail: template-tesi-ingind@polimi.it
%
% Last Revision: October 2021
%
% Copyright 2021 Politecnico di Milano, Italy. NC-BY

% Use the PoliMi template when available; fall back to a standard class so the
% document still compiles if template files are missing (e.g., in a minimal export).
\newif\ifpolimiclass
\IfFileExists{Configuration_Files/PoliMi3i_thesis.cls}{\polimiclasstrue}{\polimiclassfalse}
\ifpolimiclass
  \documentclass{Configuration_Files/PoliMi3i_thesis}
\else
  \documentclass{book}
\fi

%------------------------------------------------------------------------------
%	REQUIRED PACKAGES AND  CONFIGURATIONS
%------------------------------------------------------------------------------

% CONFIGURATIONS
\usepackage{parskip} % For paragraph layout
\usepackage{setspace} % For using single or double spacing
\usepackage{emptypage} % To insert empty pages
\usepackage{multicol} % To write in multiple columns (executive summary)
\setlength\columnsep{15pt} % Column separation in executive summary
\setlength\parindent{0pt} % Indentation



% PACKAGES FOR TITLES
\usepackage{titlesec}
% \titlespacing{\section}{left spacing}{before spacing}{after spacing}
\titlespacing{\section}{0pt}{3.3ex}{2ex}
\titlespacing{\subsection}{0pt}{3.3ex}{1.65ex}
\titlespacing{\subsubsection}{0pt}{3.3ex}{1ex}
\usepackage{color}

% PACKAGES FOR LANGUAGE AND FONT
\usepackage[english]{babel} % The document is in English  
\usepackage[utf8]{inputenc} % UTF8 encoding
\usepackage[T1]{fontenc} % Font encoding
\usepackage{cmap} % Improve PDF search/copy-paste text extraction
\IfFileExists{glyphtounicode.tex}{\input{glyphtounicode}}{} % Unicode mapping for pdfLaTeX
\ifdefined\pdfgentounicode
  \pdfgentounicode=1
\fi
\usepackage[11pt]{moresize} % Big fonts
\usepackage{CJKutf8} % Enable Chinese text blocks under pdfLaTeX

% PACKAGES FOR IMAGES
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{transparent} % Enables transparent images
\usepackage{eso-pic} % For the background picture on the title page
\usepackage{subfig} % Numbered and caption subfigures using \subfloat.
\usepackage{tikz} % A package for high-quality hand-made figures.
% \usetikzlibrary{...} % Add TikZ libraries only if needed.
% Graphics search paths (support compiling from thesis folder or repo root)
% Note: the project contains directories with spaces in their names (e.g., "Final result", "Thesis latex"),
% so we add both escaped and \detokenize{} variants for robust file lookup across LaTeX engines/setups.
\graphicspath{{./Images/}{Images/}{report_assets/}{../report_assets/}{Thesis\ latex/report_assets/}{\detokenize{Thesis latex/report_assets/}}{\detokenize{Final result/report_assets/}}{\detokenize{../Final result/report_assets/}}{Final\ result/report_assets/}{../Final\ result/report_assets/}} % Directories of the images
\usepackage{caption} % Coloured captions
\usepackage{xcolor} % Coloured captions
\usepackage{amsthm,thmtools,xcolor} % Coloured "Theorem"
\usepackage{float}

% STANDARD MATH PACKAGES
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage[overload]{empheq} % For braced-style systems of equations.
\usepackage{fix-cm} % To override original LaTeX restrictions on sizes

% PACKAGES FOR TABLES
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{longtable} % Tables that can span several pages
\usepackage{colortbl}

% PACKAGES FOR ALGORITHMS (PSEUDO-CODE)
\usepackage{algorithm}
\usepackage{algorithmic}

% PACKAGES FOR REFERENCES & BIBLIOGRAPHY
\usepackage[colorlinks=true,linkcolor=black,anchorcolor=black,citecolor=black,filecolor=black,menucolor=black,runcolor=black,urlcolor=black]{hyperref} % Adds clickable links at references
\usepackage{cleveref}
\usepackage[square, numbers, sort&compress]{natbib} % Square brackets, citing references with numbers, citations sorted by appearance in the text and compressed
\bibliographystyle{abbrvnat} % You may use a different style adapted to your field

% OTHER PACKAGES
\usepackage{pdfpages} % To include a pdf file
\usepackage{afterpage}
\usepackage{lipsum} % DUMMY PACKAGE
\usepackage{fancyhdr} % For the headers
\fancyhf{}

% Input of configuration file (if present). Do not change config.tex unless needed.
\IfFileExists{Configuration_Files/config.tex}{\input{Configuration_Files/config}}{}

% Fallback no-ops when compiling without the PoliMi template.
\providecommand{\puttitle}[1]{}
\providecommand{\startpreamble}{}

%----------------------------------------------------------------------------
%	NEW COMMANDS DEFINED
%----------------------------------------------------------------------------

% EXAMPLES OF NEW COMMANDS
\newcommand{\bea}{\begin{eqnarray}} % Shortcut for equation arrays
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\e}[1]{\times 10^{#1}}  % Powers of 10 notation
\newenvironment{ChineseBlock}{\begin{CJK*}{UTF8}{gbsn}}{\end{CJK*}}

%----------------------------------------------------------------------------
%	ADD YOUR PACKAGES (be careful of package interaction)
%----------------------------------------------------------------------------

%----------------------------------------------------------------------------
%	ADD YOUR DEFINITIONS AND COMMANDS (be careful of existing commands)
%----------------------------------------------------------------------------

%----------------------------------------------------------------------------
%	BEGIN OF YOUR DOCUMENT
%----------------------------------------------------------------------------

\begin{document}

\fancypagestyle{plain}{%
  \fancyhf{}% clear all header and footer fields
  \fancyfoot[C]{\thepage}% page number in the centre of the footer
  \renewcommand{\headrulewidth}{0pt}% no header line
  \renewcommand{\footrulewidth}{0pt}% no footer line
}

%----------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------

\pagestyle{empty} % No page numbers
\frontmatter % Use roman page numbering style (i, ii, iii, iv...) for the preamble pages

\puttitle{
    title = {The AI Buffer: How Persistent
AI-Generated Representations Shape
Human Memory}, % Insert the final thesis title
    name = {Duccio Profeti}, % Authors on one line
    course = {  Management Engineering}, % Programme in English
    ID = {XXXXX \quad\quad YYYYY}, % Student IDs on one line
    advisor = {Prof. Sergio Terzi}, % Advisor in English
    coadvisor = {Prof. Patrick Rau}, % Leave empty if none
    academicyear = {Academic Year: 2026--27}, % Academic year in English
}
%----------------------------------------------------------------------------
%	PREAMBLE PAGES: ABSTRACT (inglese e italiano), EXECUTIVE SUMMARY
%----------------------------------------------------------------------------
\startpreamble
\setcounter{page}{1} % Set page counter to 1


% ABSTRACT IN ENGLISH
\chapter*{Abstract}
Generative AI systems are increasingly integrated into everyday cognitive tasks, raising questions
about how AI-assisted workflows influence human learning, memory, and cognitive offloading. To
study these broader cognitive effects in a realistic reading context, this thesis examines
AI-assisted reading using AI-generated summaries---a common form of support when people need to
process expository texts. It tests whether outcomes vary with summary \textbf{timing} (pre-reading,
synchronous, post-reading) and \textbf{structure} (integrated paragraph vs.\ segmented bullet
points).

Study~1 used a controlled reading-and-testing paradigm (\(N=36\); \(N_{\mathrm{AI}}=24\);
\(N_{\mathrm{NoAI}}=12\)). Participants read three \(\sim\)1{,}300-word articles and completed free
recall and a 14-item multiple-choice test per article. In the AI condition, timing was manipulated
within participants across three blocks and structure was manipulated between participants (\(2 \times
3\) mixed design). Outcomes included recognition performance (overall multiple-choice accuracy, and a
summary-sourced subset), free-recall quality (rubric-based scoring), behavioral engagement from
interaction logs (reading time, summary viewing time), subjective experience (post-block trust and
dependence), and epistemic-risk measures based on endorsement of designed false lures embedded in the
summaries.

AI assistance improved recognition performance relative to unaided reading (overall MCQ accuracy: AI
\(M=0.598\) vs.\ No-AI \(M=0.510\)), but did not reliably improve free recall. Within the AI
condition, \textbf{timing} strongly shaped learning for information covered by the summary:
pre-reading access yielded the highest summary-sourced recognition (pre \(M=0.833\) vs.\ synchronous
\(M=0.568\) vs.\ post \(M=0.641\)) and increased attention devoted to the summary without increasing
total time-on-task. In contrast, \textbf{structure} primarily affected epistemic safety: segmented
summaries substantially increased false-lure endorsement compared to integrated summaries (odds ratio
\(=5.93\), 95\% CI [1.63, 21.5]). Post-block ratings indicated the highest perceived reliance in
pre-reading blocks and higher dependence under segmented structure.

Overall, the results provide initial support for the \emph{AI Buffer model proposed in this thesis}: AI-generated
representations can function as persistent external buffers that enhance cue-based learning, while
also introducing source-monitoring vulnerabilities when provenance cues are weak. Practically, the
findings suggest providing summaries before reading (as advance organizers) and favoring integrated
formats in contexts where verification and accuracy are critical.
\\
\\
\textbf{Keywords:} generative AI; summarization; learning; memory; cognitive offloading; misinformation

% ABSTRACT IN ITALIAN
\chapter*{Abstract in lingua italiana}
I sistemi di IA generativa sono sempre pi\`u integrati nelle attivit\`a cognitive quotidiane,
sollevando interrogativi su come i flussi di lavoro assistiti dall’IA influenzino apprendimento,
memoria e offloading cognitivo. Per studiare questi effetti in un contesto di lettura realistico,
questa tesi analizza la lettura assistita dall’IA utilizzando riassunti generati automaticamente---un
supporto comune quando si devono elaborare testi espositivi---e valuta se gli effetti variano in
funzione della \textbf{tempistica} (prima della lettura, durante la lettura, dopo la lettura) e
della \textbf{struttura} (paragrafo integrato vs.\ punti elenco segmentati).

Lo Studio~1 ha adottato un paradigma controllato di lettura e test (\(N=36\);
\(N_{\mathrm{AI}}=24\); \(N_{\mathrm{NoAI}}=12\)). I partecipanti hanno letto tre articoli di circa
1{,}300 parole e, per ciascun articolo, hanno svolto un richiamo libero e un test a scelta multipla
(14 item). Nella condizione con IA, la tempistica \`e stata manipolata entro soggetto su tre blocchi
e la struttura tra soggetti (disegno misto \(2 \times 3\)). Le misure includevano: prestazioni di
riconoscimento (accuratezza complessiva e sottoinsieme basato sul riassunto), qualit\`a del richiamo
libero (valutazione tramite rubrica), indicatori comportamentali dai log di interazione (tempi di
lettura e di consultazione del riassunto), esperienza soggettiva (fiducia e dipendenza post-blocco)
e misure di rischio epistemico basate sull’accettazione di \emph{false lures} plausibili inserite nei
riassunti.

I risultati mostrano che l’assistenza dell’IA migliora le prestazioni di riconoscimento rispetto
alla lettura senza supporto (accuratezza MCQ: IA \(M=0.598\) vs.\ No-IA \(M=0.510\)), senza un
miglioramento affidabile del richiamo libero. All’interno della condizione con IA, la
\textbf{tempistica} modula l’apprendimento delle informazioni coperte dal riassunto: l’accesso
pre-lettura massimizza l’accuratezza nel sottoinsieme basato sul riassunto (pre \(M=0.833\) vs.\
sincrona \(M=0.568\) vs.\ post \(M=0.641\)) e aumenta l’attenzione dedicata al riassunto senza
incrementare il tempo totale. La \textbf{struttura} incide soprattutto sulla sicurezza epistemica: i
riassunti segmentati aumentano in modo sostanziale l’accettazione delle false lures rispetto ai
riassunti integrati (odds ratio \(=5.93\), IC 95\% [1.63, 21.5]). Le valutazioni post-blocco
indicano inoltre una maggiore percezione di affidamento nelle condizioni pre-lettura e una
dipendenza pi\`u elevata con formati segmentati.

Nel complesso, i risultati forniscono un supporto iniziale al modello \emph{AI Buffer} proposto in questa tesi: i riassunti generati dall’IA
agiscono come rappresentazioni esterne persistenti che favoriscono un apprendimento basato su cue, ma
possono aumentare la vulnerabilit\`a di \emph{source monitoring} quando i segnali di provenienza sono
deboli. In termini applicativi, i riassunti dovrebbero essere forniti prima della lettura (come
\emph{advance organizers}) e presentati in formati integrati quando \`e importante facilitare la
verifica e ridurre il rischio di disinformazione.
\\
\\
\textbf{Parole chiave:} IA generativa; riassunti; apprendimento; memoria; offloading cognitivo; disinformazione


%----------------------------------------------------------------------------
%	LIST OF CONTENTS/FIGURES/TABLES/SYMBOLS
%----------------------------------------------------------------------------

% TABLE OF CONTENTS
\thispagestyle{empty}
\tableofcontents % Table of contents 
\thispagestyle{empty}
\cleardoublepage

%-------------------------------------------------------------------------
%	THESIS MAIN TEXT
%-------------------------------------------------------------------------
% In the main text of your thesis you can write the chapters in two different ways:
%
%(1) As presented in this template you can write:
%    \chapter{Title of the chapter}
%    *body of the chapter*
%
%(2) You can write your chapter in a separated .tex file and then include it in the main file with the following command:
%    \chapter{Title of the chapter}
%    \input{chapter_file.tex}
%
% Especially for long thesis, we recommend you the second option.

\addtocontents{toc}{\vspace{2em}} % Add a gap in the Contents, for aesthetics
\mainmatter % Begin numeric (1,2,3...) page numbering

% Use only footer page numbers, no header
\pagestyle{fancy}
\fancyhf{}                % clear header & footer
\fancyfoot[C]{\thepage}   % page number centered in footer
\renewcommand{\headrulewidth}{0pt} % remove header line
\renewcommand{\footrulewidth}{0pt} % optional: no footer line

% --------------------------------------------------------------------------
% NUMBERED CHAPTERS % Regular chapters following
% -------------------------------------------------------------------------

% --------------------------------------------------------------------------
% NUMBERED CHAPTERS % Regular chapters following
% ------------------------------------------------------------------------
%=========================================================
% THESIS STRUCTURE (MEMORY-FOCUSED) - MAIN FILE SKELETON
%=========================================================

\mainmatter

%---------------------------------------------------------
\chapter{Introduction}
\label{ch:introduction}

Generative AI systems have rapidly become part of everyday information work. Students and knowledge workers increasingly rely on AI tools to summarize long texts, extract key points, and provide explanations on demand. This shift is not only a change in \emph{access} to information, but also a change in the \emph{cognitive environment} in which learning and remembering take place: when a system produces an external representation of a text (e.g., a summary), it can act as a cue, a scaffold, or even a substitute for internal processing.

From a cognitive perspective, the central question is whether such assistance strengthens memory by reducing extraneous load and guiding attention, or whether it weakens memory by encouraging shallow processing and cognitive offloading. Prior work suggests that external resources can both support performance and alter internal encoding strategies \citep{Risko2016CognitiveOffloading,Sparrow2011GoogleEffect}. However, evidence on generative AI specifically is still emerging, and there is limited understanding of \emph{which design choices} (when and how summaries are presented) shape memory outcomes \citep{Bai2023ChatGPTLearningMemory,Chan2024ConversationalAIFalseMemories}.

This thesis addresses that gap through a controlled reading-and-memory experiment in which AI assistance is operationalized as pre-generated summaries that vary in \emph{structure} (integrated vs.\ segmented) and \emph{timing} (pre-reading vs.\ synchronous vs.\ post-reading). The study compares AI-assisted reading against a No-AI baseline and evaluates not only recognition accuracy, but also free recall quality and susceptibility to misinformation via experimentally planted false lures.

\section{Motivation and Research Problem}
\label{sec:motivation}
The ability to learn from text depends on how readers encode information during reading and how they later retrieve it. Classic memory research shows that durable learning is supported by deeper semantic processing \citep{Craik1972LevelsOfProcessing}, the management of limited-capacity working memory resources \citep{Baddeley2012WorkingMemory}, and the availability of effective retrieval cues that match encoding conditions \citep{Tulving1973EncodingSpecificity}. In real learning contexts, readers often use external representations---notes, outlines, highlighted passages---to support these processes. Generative AI introduces a new, highly scalable form of external representation: a system can produce a structured summary of a text instantly and present it alongside the original material.

This development is attractive because it promises efficiency and comprehension support. Summaries can reduce the need to search for key points, clarify causal relations, and provide an organized ``map'' of the content. At the same time, AI summaries may change how readers allocate attention and what they choose to encode. When an external representation is available, individuals may engage in \emph{cognitive offloading}---relying on the external artifact rather than internal memory---which can reduce the formation of detailed, well-integrated memory traces \citep{Risko2016CognitiveOffloading,Sparrow2011GoogleEffect}. In addition, because AI outputs are not guaranteed to be complete or perfectly accurate, reliance on summaries introduces a risk of \emph{source-monitoring} errors: people may later misattribute information to the original text, accept plausible but incorrect statements, or blend sources during reconstruction \citep{Johnson1993SourceMonitoring,Chan2024ConversationalAIFalseMemories}.

Despite intense public interest, there is still a methodological need for controlled evidence on how generative AI assistance influences memory under realistic task constraints. Much existing work focuses on perceived usefulness or immediate task performance; fewer studies examine memory outcomes in a way that separates (i) recall versus recognition, (ii) item-specific versus relational understanding, and (iii) accurate memory versus acceptance of plausible misinformation. Moreover, AI assistance is often treated as a single ``on/off'' factor rather than a configurable system whose design choices may matter.

The research problem of this thesis is therefore twofold:
\begin{itemize}
  \item \textbf{Substantive problem:} to determine how generative AI assistance affects memory for complex expository information, and whether it changes the \emph{quality} of what is remembered (specificity, mechanistic depth, and structural alignment) rather than only the amount.
  \item \textbf{Design problem:} to identify how configurations of AI assistance---operationalized here through the timing and structure of AI-generated summaries---shape encoding and retrieval, including susceptibility to false memories and source confusions.
\end{itemize}
By isolating these factors experimentally, the thesis aims to move from broad claims about ``AI helps/hurts learning'' toward more precise conclusions about which AI configurations are beneficial, which are risky, and why.

\section{Research Objectives}
\label{sec:objectives}
Following common master-thesis conventions, the work is organized around a small set of concrete objectives that translate the research problem into measurable outcomes.

The main objectives are:
\begin{itemize}
  \item \textbf{O1 --- Establish a baseline comparison:} quantify differences in memory performance between AI-assisted reading and unaided reading under matched time constraints.
  \item \textbf{O2 --- Test design dimensions of AI assistance:} evaluate how summary \emph{timing} (pre-reading, synchronous, post-reading) and summary \emph{structure} (integrated vs.\ segmented) influence recall quality and recognition accuracy.
  \item \textbf{O3 --- Assess misinformation and source integrity:} measure susceptibility to false-lure acceptance and source-monitoring errors when AI-generated content is present.
  \item \textbf{O4 --- Link outcomes to behavior and experience:} relate memory outcomes to behavioral logs (e.g., summary viewing time, reading time, interaction patterns) and to experience-based measures such as post-block AI trust and perceived dependence.
  \item \textbf{O5 --- Provide interpretable implications:} derive theoretical and practical implications for the design of AI-supported reading interfaces in education and knowledge work.
\end{itemize}

\section{Research Questions}
\label{sec:research_questions}
The thesis addresses the objectives through the following research questions (RQs), framed to be answerable using the experimental design and measures reported in this work:
\begin{itemize}
  \item \textbf{RQ1 (AI vs.\ No-AI):} Compared to unaided reading, does access to AI assistance (operationalized as an AI-generated summary) improve or degrade memory performance for expository texts (free recall quality and recognition accuracy)?
  \item \textbf{RQ2 (Timing):} For AI-assisted participants, how does the timing of summary access (pre-reading, synchronous, post-reading) affect memory outcomes and behavioral indicators of reliance?
  \item \textbf{RQ3 (Structure):} Does summary structure (integrated paragraph summary vs.\ segmented bullet summary) differentially support relational understanding versus item-specific recall, and does it affect susceptibility to misinformation?
  \item \textbf{RQ4 (Individual differences and experience):} Do individual differences (e.g., prior knowledge) and post-block AI trust/dependence relate to reliance behavior and variability in memory outcomes within the AI conditions?
\end{itemize}
Together, these questions connect a practical interface choice (how and when to show AI summaries) to theoretically meaningful constructs in memory research: depth of processing, cue-dependent retrieval, cognitive offloading, and source monitoring.

\section{Scope and Focus of the Thesis}
\label{sec:scope}
This thesis focuses on memory for \emph{expository} reading materials under controlled laboratory conditions. The study is intentionally scoped to isolate the cognitive consequences of AI-assisted reading, operationalized here as pre-generated AI summaries, while keeping other variables constant.

\textbf{What is in scope:}
\begin{itemize}
  \item A controlled reading-and-testing protocol with fixed exposure windows, enabling comparison across AI and No-AI conditions.
  \item AI assistance operationalized as \emph{pre-generated} summaries (rather than interactive dialogue), enabling strict control over content, structure, and the inclusion of false lures.
  \item Multiple outcome measures capturing complementary aspects of memory: free recall quality, recognition accuracy, misinformation acceptance, confidence, perceived effort, and behavioral interaction logs.
\end{itemize}

\textbf{What is out of scope:}
\begin{itemize}
  \item Long-term retention over days/weeks and educational outcomes such as course grades; the study evaluates memory within the experimental session.
  \item Open-ended conversational use of AI systems where prompts, follow-up questions, and personalization introduce substantial variability.
  \item Neuroimaging or physiological measures; the thesis relies on behavioral performance, self-report, and interaction logs.
\end{itemize}

These boundary conditions are important for interpreting the results: the thesis aims to make strong causal claims about the tested AI summary configurations, while avoiding overgeneralization to all possible AI tools and learning contexts.

\section{Contribution of the Thesis}
\label{sec:contribution}
The thesis provides contributions at three levels---conceptual, methodological, and empirical---which is typical for an experimental master thesis in cognitive/behavioral research.

\textbf{Conceptual contribution.} The thesis proposes the \emph{AI Buffer} model (Chapter~3) as a compact way to describe how AI-generated representations can act as an external buffer that interacts with encoding, retrieval, and metacognitive monitoring. The model is not intended as a universal theory of AI, but as a targeted framework for interpreting the effects of AI summaries under controlled conditions.

\textbf{Methodological contribution.} The work develops and documents a browser-based experimental platform that implements strict timing control, counterbalancing, and fine-grained behavioral logging. The design includes a No-AI baseline and an AI condition with orthogonal manipulations of summary structure and timing, enabling direct evaluation of design choices rather than treating AI support as a unitary intervention.

\textbf{Empirical contribution.} The experiment provides evidence on how AI-generated summaries influence recall quality, recognition accuracy, confidence calibration, and susceptibility to false-lure acceptance. By combining recall scoring grounded in constructive memory theory \citep{Bartlett1932Remembering} with a recognition test that includes planted misinformation and source-sensitive indices, the thesis offers a nuanced characterization of when AI assistance helps, when it harms, and what kinds of memory representations are affected.

\section{Thesis Structure}
\label{sec:thesis_structure}
The remainder of the thesis is organized as follows:
\begin{itemize}
  \item \textbf{Chapter~2 (Literature Review)} synthesizes foundational theories of human memory and relevant work on cognitive offloading, external representations, and recent findings on generative AI and memory.
  \item \textbf{Chapter~3 (Conceptual Framework and Hypotheses)} introduces the AI Buffer model and derives hypotheses linking summary structure and timing to predicted memory outcomes.
  \item \textbf{Chapter~4 (Methodology)} describes the experimental design, materials, platform implementation, procedure, measures, and scoring methods (including the recall rubric and false-lure logic).
  \item \textbf{Chapter~5 (Data Analysis)} outlines preprocessing steps, exclusion criteria, and the statistical models used to test the hypotheses.
  \item \textbf{Chapter~6 (Results)} reports descriptive statistics and inferential results for recall, recognition, misinformation indices, and behavioral measures.
  \item \textbf{Chapter~7 (Discussion)} interprets findings in light of the theoretical framework, considers alternative explanations and limitations, and discusses implications for AI interface design.
  \item \textbf{Chapter~8 (Conclusion)} summarizes contributions, key takeaways, and directions for future research.
  \item \textbf{Appendices} provide experimental materials, survey instruments, statistical outputs, and ethical protocol details.
\end{itemize}


%---------------------------------------------------------

\chapter{Literature Review}
\section{AI and Human Cognition}

\subsection{Theoretical Foundations of Human Memory}

The study of human memory has evolved through several major theoretical frameworks that describe how information is encoded, stored, and retrieved. The earliest comprehensive account is the \emph{modal model} proposed by Atkinson and Shiffrin \citep{Atkinson1968HumanMemory}, which conceptualizes memory as a system composed of three structural stores: sensory memory, short-term memory (STM), and long-term memory (LTM). According to this framework, information enters through high-capacity but rapidly decaying sensory registers, is filtered into STM via attentional control, and may be consolidated into LTM through rehearsal and elaborative processing. This model introduced the influential distinction between temporary and durable memory systems and highlighted the central role of control processes—particularly attention and rehearsal—in regulating information flow.

A major theoretical shift occurred with Baddeley’s reconceptualization of STM as \emph{working memory} \citep{Baddeley2012WorkingMemory}. Rather than a passive buffer, working memory is understood as an active cognitive workspace composed of multiple subsystems (the phonological loop, visuospatial sketchpad, episodic buffer, and a central executive) that enable the integration of multimodal information. This framework supports reasoning, decision-making, and sustained mental effort, and it aligns with extensive neuropsychological evidence for domain-specific maintenance mechanisms and executive-control functions distributed across the cortex. In contemporary cognitive science, working memory is viewed as a core system for managing cognitive load, coordinating complex tasks, and mediating interactions between attention and long-term memory.

Building on these models, Tulving introduced the distinction between \emph{episodic} and \emph{semantic} memory \citep{Tulving1972EpisodicSemantic}. Episodic memory refers to the recollection of personally experienced, context-rich events, whereas semantic memory stores generalized knowledge about the world. This theoretical separation is supported by neuropsychological cases in which episodic-memory deficits coexist with preserved semantic knowledge, indicating partially distinct neural substrates. Tulving further proposed the \emph{encoding specificity} principle \citep{Tulving1973EncodingSpecificity}, which posits that retrieval is most successful when the cues available at encoding are reinstated at recall—a principle that has guided experimental memory research and is highly relevant for understanding how artificial systems might augment or interfere with human recall.

Additional work has refined these classical perspectives. Among influential refinements, Craik and Lockhart’s \emph{Levels of Processing} framework \citep{Craik1972LevelsOfProcessing} emphasized that the durability of a memory trace depends not on which store it resides in, but on the \emph{depth} and \emph{semantic elaboration} of processing applied to the information. Deep, meaning-oriented encoding produces more persistent memory traces than shallow, perceptual processing—a finding repeatedly confirmed by subsequent behavioral and neuroscientific studies.

Neurophysiological research has extended classical models by illuminating the biological basis of encoding and retrieval. Single-neuron recording studies, for example, show that hippocampal and medial-temporal lobe neurons respond selectively to abstract concepts, forming building blocks of episodic and semantic representations \citep{Rutishauser2021ArchitectureHumanMemory}. Electrophysiological evidence further demonstrates that stronger neural activation during encoding predicts greater likelihood of later retrieval success—a phenomenon known as the subsequent-memory effect \citep{Caplan2009EEGAssociativeOrder}. Such findings bridge cognitive theory and neural mechanism, linking constructs like “depth of processing” to measurable brain activity.

Memory research has also embraced ecological and contextual perspectives. Studies using immersive virtual environments indicate that spatially rich, multisensory contexts enhance episodic encoding \citep{Plancher2018VREpisodicMemory},
, while “real-life” neuroscience approaches argue that memory should be studied in dynamic, socially embedded settings rather than in isolation \citep{ShamayTsoory2019RealLifeNeuroscience}.
. These views highlight the adaptability of memory systems and their sensitivity to environmental structure and attentional framing. At a systems level, contemporary accounts emphasize that memory emerges from distributed interactions across cortical–hippocampal networks. For instance, Moscovitch and colleagues \citep{Moscovitch2006CognitiveNeuroscienceRemoteMemory} propose that the hippocampus continuously binds contextual details during encoding while neocortical regions extract semantic regularities over time. Consistent with this, recent neuroimaging shows that retrieving a memory can reactivate the same cortical patterns that were present during its encoding \citep{Liu2021TransformativeNeuralRepresentations}, supporting the idea that remembering is a dynamic process of neural reactivation and transformation.

Taken together, these theoretical and empirical contributions establish several foundational principles: (1) memory is organized across multiple interacting systems with specialized functions; (2) effective encoding and retrieval depend critically on attentional control and the depth of semantic processing; and (3) memory operates as an adaptive, reconstructive, context-sensitive process shaped by neural, behavioral, and environmental interactions. These principles provide a conceptual basis for analyzing how artificial intelligence systems may support, modify, or extend human memory in contemporary professional and technological contexts.

\subsection{How Readers Learn from Expository Texts}

Humans do not memorize information in a vacuum; when reading expository texts (such as scientific articles or technical reports), they actively construct a mental representation of the content that integrates new information with prior knowledge. According to discourse comprehension theory, readers form a hierarchical \emph{situation model} of a text, which entails establishing local coherence between consecutive ideas and global coherence across the entire discourse \citep{vanDijk1983Discourse}. In building this situation model, the reader must identify the text’s structure, discern relationships between key concepts, and continuously update their understanding as new information is introduced. This process places substantial demands on attention and working memory: readers need to filter and organize details so as not to overload memory, while still maintaining the “big picture” of what the text means.

Because of these cognitive demands, the way an expository text is structured — and the presence of guiding cues or previews — can significantly influence learning outcomes. Decades of research in educational psychology show that providing learners with an overview or outline of a text \emph{before} reading can facilitate comprehension and recall of the material’s main ideas. Such pre-reading aids, often termed \emph{advance organizers} \citep{Ausubel1960AdvanceOrganizers,Ausubel1968EducationalPsychology}, are thought to activate relevant prior knowledge (schemas) and highlight the organizational framework of the forthcoming content. Empirical studies have found that students who receive a conceptual outline or summary before delving into a complex text tend to understand and retain the information better than those who do not. For example, offering a brief summary or set of learning objectives in advance can change how learners mentally represent technical passages and improve their performance on comprehension tasks \citep{MayerBromage1980AdvanceOrganizersRecall, HartleyDavies1976PreinstructionalStrategies}. Even simpler structural cues embedded in the text — such as descriptive headings and subheadings — guide attention to important content and enhance memory for the organization of ideas \citep{LorchLorch1996HeadingsRecall}. By signaling the hierarchy and logical flow of the material, these devices help readers allocate their mental resources more effectively.

Notably, the benefits of improving a text’s coherence and providing previews often appear in measures of understanding and application (e.g. answering questions about the content) even if straightforward free recall of details is unchanged. In some cases, advance organizers do not increase the total facts remembered, but they do lead to better \emph{integration} of those facts. For instance, Hartley and Davies’s classic review \citep{HartleyDavies1976PreinstructionalStrategies} found that giving overviews did not always boost verbatim recall, yet it consistently improved learners’ ability to grasp and transfer key concepts. Similarly, Stull and Mayer \citep{Stull2007GraphicOrganizers} reported that students who were provided with instructor-generated graphic organizers before reading outperformed others on conceptual post-tests, despite showing no advantage on immediate factual recall. These findings suggest that structural aids primarily influence \emph{how} information is encoded — promoting organized, meaningful learning — rather than simply \emph{how much} information is remembered verbatim. In other words, when readers have a guiding framework, they tend to focus on and mentally integrate the most important ideas (improving comprehension and recognition of those ideas later on), even if their ability to freely recall every detail remains limited. This distinction between comprehension and raw recall will be important in evaluating the effects of AI-generated summaries, which might similarly boost readers’ understanding of content without necessarily increasing memory for unprompted details.

\subsection{Cognitive Effects of AI Systems}

Artificial intelligence systems are increasingly interwoven with how individuals acquire, process, and retrieve information, positioning these systems as potential partners in human memory and cognition. Classic memory theories describe encoding, storage, and retrieval as an interaction of internal processes, but AI introduces an external agent that can dynamically contribute to those processes. Current research indicates that AI tools can influence cognition through mechanisms such as prompting deeper processing, offloading cognitive tasks, providing meta-cognitive feedback, and offering context-specific cues. At the same time, these tools also carry risks related to over-reliance, reduced internal encoding, and memory distortions. This section reviews empirical findings on both the enhancing and disruptive cognitive effects of AI.

One widely discussed effect of AI assistance is its impact on memory encoding and learning. Studies have shown that engaging with generative AI systems can improve learning outcomes when used in ways that encourage reflection and elaboration. For example, Bai et al. \citep{Bai2023ChatGPTLearningMemory} found that students who interacted with a large language model (ChatGPT) by asking questions and explaining answers achieved better learning results than those who studied without AI. The AI dialogue, in essence, prompted learners to articulate and refine their understanding, invoking deeper semantic processing consistent with the Levels of Processing principle \citep{Craik1972LevelsOfProcessing}. In a related vein, Haider et al. \citep{Haider2024AICognitiveFunctions} reported that AI-assisted learning can enhance short-term recall, long-term retention, and problem-solving performance while also reducing cognitive anxiety (which is known to impair the encoding and retrieval of information). These findings suggest that AI systems, when used interactively, can serve as cognitive \emph{scaffolds}—encouraging users to engage in elaborative rehearsal, self-explanation, and other effective learning strategies that strengthen memory traces.

Beyond initial learning, AI can shape how people consolidate and recall information by providing external memory supports. One example is the use of conversational AI platforms to reinforce knowledge. The Memoro system \citep{Memoro2024RealTimeMemoryAugmentation}, for instance, offers a real-time dialogue interface that prompts users to restate and verify information during study. This resembles the well-established benefits of retrieval practice and rehearsal in human memory research: by externalizing the rehearsal process and giving immediate feedback, the AI system helps to consolidate new information and integrate it with existing memory. Likewise, preliminary neuroscience evidence (e.g., from research at MIT Media Lab \citep{MITMediaLab2023BrainOnChatGPT}) suggests that AI-assisted writing—where an AI model helps generate or organize content—engages brain regions associated with attention and semantic processing, hinting that AI-mediated cognition may directly influence neural activity during memory formation.

AI tools can also support \emph{meta-cognition}, the monitoring and regulation of one’s own cognitive processes. Sun et al. \citep{Sun2025GenerativeAICreativity} demonstrated that generative AI can improve users’ meta-cognitive calibration in creative tasks by prompting them to evaluate alternatives, articulate reasoning, and reflect on uncertainties. These kinds of AI-driven reflections can lead users to identify gaps in their knowledge and adjust their learning strategies, which in turn facilitates deeper encoding and more accurate confidence judgments. The ability of AI to prompt meta-cognitive reflection ties into traditional memory theories that stress the importance of self-monitoring (e.g., feeling-of-knowing, source confidence) for effective learning and recall.

However, the cognitive benefits of AI are coupled with significant risks. Perhaps the most cited concern is \emph{cognitive offloading}—the tendency to rely on external aids rather than one’s own memory. Research on internet use provides clear evidence of this effect: people quickly learn to offload information to digital sources and become less inclined to encode or retain it internally \citep{Sparrow2011GoogleEffect}. Over time, habitual offloading shifts cognitive effort from memorization to simply remembering how or where to find information \citep{Risko2016CognitiveOffloading}. A recent meta-analysis by Gong and Yang \citep{Gong2024GoogleEffectsMetaAnalysis} confirms this “Google effect,” showing that frequent reliance on search engines leads to decreased recall of content but improved memory for sources. By extension, AI systems that automatically summarize or answer questions could encourage users to invest less effort in understanding or remembering raw information, since the AI is available to supply it on demand. This trade-off raises the concern that AI assistance might produce an illusion of competency—or “hollow” expertise \citep{Oakley2025MemoryParadox}—in which a user appears knowledgeable with AI help but has not internalized the information. Oakley et al. argue that while AI can provide correct answers, it bypasses the effortful semantic integration required for true understanding, resulting in shallower learning. This echoes the worries of Craik and Lockhart \emph{et al.} that without deep processing, memories remain fragile, and of Baddeley that without engaging working memory’s active components, new information won’t be firmly integrated.

Another emerging risk is the introduction of \emph{false memories} or misinformation through AI interaction. Chan et al. \citep{Chan2024ConversationalAIFalseMemories} showed that a large language model-based conversational agent can inadvertently implant false details in users’ recollections during tasks analogous to witness interviews. In their study, the AI’s subtle suggestions and leading questions led participants to later “remember” specifics that the AI had fabricated. This finding extends decades of human research on the misinformation effect (where misleading post-event information becomes incorporated into memory) into the realm of AI. It also highlights an important epistemic concern: when AI-generated content is interwoven with a person’s own thoughts, it may blur the line between external information and personal memory. Users might struggle to distinguish what they read in the original source from what was suggested by an AI summary or chat — an issue fundamentally related to source-monitoring errors. (We further discuss memory distortion and source confusion in a later subsection.)

In educational settings, over-reliance on AI can diminish active cognitive engagement. For example, a systematic review by Zhai et al. \citep{Zhai2024OverRelianceAIDialogue} found that students who heavily depend on AI tutors or chatbots for answers tend to exhibit poorer problem-solving skills and weaker long-term retention compared to students who use AI more sparingly and continue to practice retrieval on their own. These results reinforce the idea that while AI can provide support, it may also reduce the very practices (like self-testing or elaboration) that lead to strong memory formation.

At a broader societal level, researchers have started to examine how pervasive AI tools might be shifting cognitive habits. Gerlich \citep{Gerlich2025AIToolsSociety}, for instance, argues that as AI systems handle more information processing tasks, humans may be reallocating their cognitive resources — spending less effort on memorizing details and more on managing interfaces or interpreting AI outputs. According to this view, we are witnessing a cultural shift in which external systems take on the role of “memory partners,” and human cognition evolves to work in tandem with these systems. Such a shift could have long-term effects on how we learn, make decisions, and even what we consider important to remember.

Finally, AI may affect the emotional and contextual aspects of memory. Some studies in the emerging field of neuromarketing suggest that AI-curated digital experiences can heighten emotional arousal and personalization, which in turn might strengthen certain memories (for example, making an encounter more memorable because it felt more personally resonant) \citep{Beyaria2024NeuromarketingAIEmotionMemory}. On the other hand, if AI feeds users a highly filtered or homogenous diet of information, it could narrow the diversity of experiences that form our memories, potentially weakening the richness or robustness of our knowledge base.

In sum, current evidence points to a dual role for AI in human cognition. On one hand, AI can act as a \emph{cognitive amplifier} — enhancing learning, memory, and metacognitive calibration by prompting deeper engagement with material and providing timely support. On the other hand, AI can function as a \emph{cognitive crutch} or even a disruptor — inducing complacency, shallower processing, false confidence, and vulnerability to misinformation. Understanding when and how AI serves as a beneficial extension versus a detrimental replacement of human memory is a key challenge. This thesis focuses on that challenge by examining specific mechanisms (the timing and format of AI-provided information) through which AI might support or undermine memory in knowledge-intensive tasks.

\subsection{Foundational Theories Motivating Timing and Structure}\label{subsec:theories-timing-structure}

Although controlled research on generative AI and memory is only beginning to emerge, the core experimental manipulations in this thesis are motivated by well-established findings in the learning sciences. In particular, theories of advance organizers, cognitive load and split attention, and source monitoring provide a strong theoretical rationale---and motivate testable hypotheses---about why \emph{when} an AI summary is provided (e.g. before reading vs.\ during vs.\ after) and \emph{how} it is presented (integrated into the text vs.\ a separate bullet list) may influence both learning outcomes and susceptibility to misinformation. In the present study, the AI-generated summary functions as a compact surrogate for the article’s key propositions. The timing of access to this summary is manipulated to test whether seeing a summary \emph{before} reading helps to organize and scaffold encoding of the full text, as opposed to seeing it partway through or only after reading. Meanwhile, the structure of the summary is manipulated to test whether presenting it as an \emph{integrated paragraph} (embedded within the article text) versus as \emph{segmented bullet points} affects the reader’s coherence building and source attributions. These manipulations, grounded in prior literature, motivate the directional hypotheses formalized in Chapter~3 and form the basis for this thesis’s experimental design.

Advance organizers and pre-reading scaffolds. Ausubel’s classic theory of advance organizers predicts that providing a high-level conceptual framework \emph{prior} to learning new information will enhance the integration of new knowledge by activating relevant schemas and guiding attention during encoding \citep{Ausubel1960AdvanceOrganizers, Ausubel1968EducationalPsychology}. Empirically, decades of research on pre-instructional strategies shows that giving learners a preview of a text (for example, an outline of the main topics or a summary of key points) can change how they mentally represent the material and improve subsequent comprehension performance \citep{MayerBromage1980AdvanceOrganizersRecall, HartleyDavies1976PreinstructionalStrategies}. Related evidence indicates that clear structural cues within a text (like informative headings) similarly guide attention and help readers remember the organizational structure of the content \citep{LorchLorch1996HeadingsRecall}. Applied to AI-generated summaries, this literature suggests that providing the summary \emph{before} the full article may be particularly effective, because an upfront summary can serve as a schema to orient the reader’s encoding of details that appear later. In contrast, if the summary is only seen after reading, it might function merely as a post-hoc reminder rather than an encoding scaffold. The advance-organizer account also implies a qualitative change in processing: early exposure to an overview should help readers select, chunk, and interpret the incoming information more efficiently. Consistent with this account, pre-reading access may increase accuracy on summary-relevant comprehension questions even when total study time or exposure is held constant.

Mid-reading interruptions and divided attention. In contrast to the benefits of a well-timed preview, presenting an AI summary \emph{synchronously} (i.e. in the middle of reading) may introduce costs due to task-switching and disrupted coherence. A reader who is partway through an article and then turns to an AI summary must divide attention between two streams of information, or pause one task to engage in another. Cognitive theories of divided attention and task interruption suggest that such context-switching imposes extra demands on working memory (to keep one task’s content in mind while addressing another) and can break the flow of comprehension. When an interruption occurs during reading, readers often need to backtrack or reread portions of the text to regain their situational context, indicating that the continuity of the situation model has been momentarily lost. In our context, a synchronous summary might momentarily draw the reader’s focus away from the article’s narrative, forcing a reorientation when returning to the text. This could weaken integration of information, especially if the summary’s content does not precisely align with the point of interruption. On the other hand, because the summary does contain relevant information, a motivated reader might use it to fill in knowledge gaps and then continue reading with renewed understanding. Thus, the net effect of a mid-reading summary is ambiguous: it might either hinder comprehension (through interruption and split attention) or modestly help (by providing clarifications) depending on how the reader handles the switch. Based on prior research on attention and interruption costs, synchronous presentation may be less beneficial than a pre-reading summary and, depending on how readers switch tasks, could be mildly disruptive to the reading process. Determining its actual impact is one aim of our experiment.

Cognitive load, split-attention, and integrated presentation. Cognitive load theory emphasizes the mind’s limited working-memory resources and predicts that formats which minimize extraneous processing demands tend to yield better learning \citep{Sweller1988CognitiveLoad}. One well-documented phenomenon is the \emph{split-attention effect}: when closely related information is separated across different sources or spaces, learners must expend mental effort to mentally integrate them, often impairing learning relative to a format in which the information is combined \citep{ChandlerSweller1992SplitAttention}. Applied studies confirm that interface design choices regarding integration vs. separation can produce measurable performance differences \citep{PociaskMorrison2008SplitAttentionRedundancy}. In the context of AI summaries, presenting the summary as an integrated paragraph within or alongside the article text may reduce unnecessary visual scanning and context-switching. An integrated format keeps all information in a contiguous flow, which likely supports coherence in the reader’s mental model and makes it easier to cross-check the summary’s assertions against the source text. By contrast, presenting the summary as a separate, segmented bullet list (for example, on a different screen or a separate section) may encourage piecemeal, item-by-item processing and increase the cognitive overhead of going back and forth between the summary and the full text. This has implications for both learning \emph{performance} and memory \emph{errors}. A well-integrated summary can make it more straightforward for the reader to verify each summary claim using the surrounding article context, thereby deepening understanding and catching any inconsistencies. A disjointed, segmented summary, on the other hand, could increase the likelihood that plausible-sounding statements are accepted at face value without sufficient contextual evaluation \citep{Sweller1988CognitiveLoad,ChandlerSweller1992SplitAttention}. In short, an integrated presentation is likely to lighten coordination demands and support coherence during study, whereas a segmented presentation can impose extra coordination demands that may diminish learning efficiency and promote shallow processing of the summary points.

\textit{(The literature above suggests that the timing and format of AI-provided information may not only affect how well readers learn, but also what errors they might make. In particular, if the format weakens cues about the source of information, readers could misattribute statements from the summary to the original text. The next subsection examines these risks of memory distortion in detail.)}

\subsection{Memory Distortion and Epistemic Risk}\label{subsec:distortion-risk}

Even as we consider ways that AI summaries might aid learning, we must also consider how they could distort memory or lead to false confidence in misinformation. Two established lines of research are especially relevant here: the Source Monitoring Framework from cognitive psychology, which explains how people attribute memories to sources, and the extensive literature on false memory and misinformation effects. Together, these perspectives highlight potential “epistemic risks” when humans incorporate AI-generated content into their own knowledge. These risks apply to AI-generated content broadly; summaries are a common instantiation in reading workflows.

According to the Source Monitoring Framework \citep{Johnson1993SourceMonitoring}, remembering is not just about recalling information, but also about determining where that information came from (e.g., “Did I read this in the article, or was it in the summary?”). People make source attributions by evaluating features of a memory: its associated contextual details (time, place, modality), its qualitative characteristics (like perceptual vividness or emotional tone), and the cognitive operations involved in generating it. If these characteristics match those typical of an internally generated memory, we conclude the memory is “ours”; if they match something we heard or read, we attribute it externally. However, source monitoring is a heuristic process and is prone to error. Laboratory studies show that when different sources present similar information, or when a memory lacks distinctive details, people can easily misattribute the memory’s origin. A classic example is the eyewitness misinformation paradigm: participants who witness an event and later receive misleading information about it often mistakenly recall the misleading details as part of the original event. This occurs because the post-event misinformation is stored in memory but without a clear tag that “this came from a subsequent narrative,” especially if the misinformation fits the narrative of the event. In such cases, people confuse the source (they misremember the misleading narrative’s details as originating from the witnessed event). Dozens of experiments by Loftus and colleagues have demonstrated this effect \citep{LoftusPalmer1974Misinformation, Loftus2005PlantingMisinformation}: for instance, simply phrasing a question as “How fast were the cars going when they smashed into each other?” can lead witnesses to later falsely remember seeing broken glass at the accident, because the word “smashed” implies severity that wasn’t originally observed. The ability to correctly monitor sources can be further clouded when one is dealing with multiple information sources that share content overlap. Lindsay and Johnson \citep{LindsayJohnson1989SuggestibilitySource} found that when people encode an event and a post-event description with overlapping details, those who have difficulty recollecting the context of each detail show greater suggestibility—meaning they are more likely to incorporate misinformation into their event memory. In summary, decades of research warn that if an external source (like an AI summary) injects new details or interpretations, readers might later recall those additions but misattribute them to the original text, unless they have strong cues to discriminate source.

A related body of work on false memory (even without deliberate misinformation) underscores how our memory tends to retain the gist of an experience and can mistakenly produce details that were never present but are gist-consistent. A well-known example is the Deese–Roediger–McDermott (DRM) paradigm: people study a list of words (e.g., “bed, pillow, dream, snooze…”) all related to a theme word that is not actually on the list (e.g., “sleep”), yet many participants later falsely recall or recognize the theme word “sleep” as having been present. This happens because memory stored the general idea (sleep-related concepts) but not the precise boundaries of what was studied versus not studied. Roediger and McDermott’s findings \citep{RoedigerMcDermott1995FalseMemoriesDRM} demonstrated that such lure memories can be held with high confidence. Fuzzy Trace Theory provides an explanatory framework: it posits that we encode both verbatim traces (exact details) and gist traces (general meaning), and over time or under certain conditions, the gist tends to dominate. False memories often arise when we rely on gist in the absence of reliable verbatim traces \citep{BrainerdReyna2005ScienceFalseMemory}. Summarization by design emphasizes the gist of an article, extracting the core ideas and omitting specifics. Therefore, a reader who heavily relies on an AI summary might form a memory representation rich in gist but sparse in specific detail. This is fertile ground for false recognitions: later, when encountering a statement related to the article’s topic, the reader may accept it as true (“I think that was mentioned”) if it aligns with the gist they remember, even if that exact statement never appeared in the original text.

AI-generated summaries thus have a dual potential to distort memory: they might directly introduce new information (including errors) that gets misremembered as part of the source material, and they might encourage a gist-based encoding of the material at the expense of verbatim detail. If the summary contains a slight inaccuracy or a subtle exaggeration, a reader could easily carry that forward into their recollection of the article without remembering that it actually came from the summary. This risk is heightened if the summary is presented or perceived in a way that obscures its origin (for instance, if it’s not made explicit which content is AI-provided). From the source-monitoring perspective, anything that blurs context cues between the summary and the original text will increase the chance of confusion.

This thesis examines whether the \textbf{format} of summary presentation can mitigate or exacerbate these risks. An \emph{integrated summary} may support source tracking by providing stronger coherence cues and by facilitating verification, whereas a \emph{segmented summary} encourages item-by-item processing that may later blur with details from the article. Source monitoring theory suggests that segmented formats may increase misattribution risk by divorcing summary content from its original context, whereas integrated formats may better preserve source information by juxtaposing summary statements with the relevant article content \citep{Johnson1993SourceMonitoring}. Thus, understanding distortion and false-memory risks is crucial when evaluating AI-assisted reading: it’s not enough to ask whether an AI tool helps memory quantitatively — we must also ask whether it alters memory \emph{qualitatively}, by introducing new errors or uncertainties.

\subsection{AI--Augmented Memory in Organizations and Applied Systems}

Beyond individual cognition, the integration of AI into memory processes has significant implications for
organizations and knowledge-intensive work. Modern enterprises increasingly rely on digital ecosystems in which
AI systems support information recall, contextual understanding, and knowledge retrieval, effectively functioning
as external, collective memory infrastructures. This development is reshaping not only how individuals encode and
retrieve information, but also how organizations store institutional knowledge, activate relevant information
across teams, and transfer expertise over time.

AI tools can act as cognitive extensions in professional environments. For example, companies are beginning to
deploy intelligent knowledge management platforms that organize personal and collective knowledge into searchable,
semantically structured archives (similar in spirit to systems like Mem.ai or Notion). These platforms reduce
cognitive load on employees by offloading the need to remember specific documents or decisions and instead
providing contextual recall at opportune moments \citep{Memoro2024RealTimeMemoryAugmentation,Gerlich2025AIToolsSociety}.
By surfacing prior meeting notes, decision histories, or relevant research exactly when they are needed, such
systems externalize elements of working memory and help maintain continuity in fast-paced workflows. This dynamic
aligns with the concept of cognitive offloading \citep{Sparrow2011GoogleEffect,Risko2016CognitiveOffloading},
extending it into organizational settings where information fragmentation is a constant challenge. In this sense,
AI augmentation can link fragmented pieces of organizational knowledge, helping ensure that insights are not lost
but remain readily retrievable when similar contexts arise.

The strategic value of AI-augmented memory becomes particularly evident as the volume and velocity of information
increase in knowledge-intensive industries. In large projects or long-running initiatives, the ability to quickly
retrieve past insights, reconstruct why certain decisions were made, and sustain continuity between phases or team
members is critical. Research in organizational cognition emphasizes the importance of \emph{knowledge continuity}
--- a firm’s ability to reactivate and leverage past experiences in ongoing decision-making \citep{Zhang2024KnowledgeImbalanceAI}.
AI systems that provide contextually relevant retrieval cues (for instance, suggesting related prior projects or
pulling up analogous cases from the company’s archives) directly reinforce this capability. They allow workers to
navigate vast, heterogeneous information stores with minimal friction, thus reducing redundant work and preserving
the rationale behind past choices. In effect, the organization’s memory becomes distributed across a human--AI
network: humans contribute intuition, creativity, and critical judgment, while AI contributes breadth of recall,
speed, and consistency in retrieving information.

This perspective aligns with broader research on technology-assisted knowledge management, which highlights a
complementary relationship between AI systems and human expertise \citep{Pai2022AIKnowledgeManagement}. AI can
rapidly organize and filter information, but it is most effective when guided by humans who provide context and
interpretive insight. Together, they can sustain an organizational memory that is more comprehensive and accessible
than unaided human memory, yet more flexible and meaningful than data left to accumulate without human
interpretation.

While the promise of AI-augmented organizational memory is compelling, these transformations also raise concerns.
One issue is that over-reliance on AI for informational recall may lead to surface-level engagement by employees.
If workers come to depend on AI to answer questions or find information, they might invest less effort in deeply
understanding new knowledge or in practicing retrieval on their own. Over time, this could erode individuals’
internal knowledge structures --- a risk mirroring what we have seen in individuals’ Google Effect. Indeed, early
evidence suggests that when professionals heavily rely on AI outputs, they may experience a form of ``knowledge
atrophy,'' retaining fewer details personally and defaulting to consulting the AI even for things they used to
know. This shift from active understanding toward passive consultation can increase \emph{epistemic vulnerability}:
the organization becomes vulnerable if the AI tools fail or produce incorrect output, or if employees have not
developed the intuition to detect when the AI is wrong. Oakley et al. \citep{Oakley2025MemoryParadox} refer to
this as the ``memory paradox'' in the age of AI --- we have more information at our fingertips than ever, yet risk
becoming less knowledgeable in a fundamental, internalized way. Similarly, a recent analysis by Zhang et al.
\citep{Zhang2024KnowledgeImbalanceAI} warns that AI-advised decision-making can create a \emph{knowledge imbalance}:
decision-makers may place too much trust in AI recommendations without fully understanding them, leading to
overconfidence and reduced critical scrutiny. In an organizational context, such epistemic dependence could result
in decisions that are made quickly on AI advice but are not fully vetted by human experts, with potentially serious
consequences if the AI has biases or errors.

Another challenge is ensuring that AI systems are transparent and their outputs are interpretable. Pasquale
\citep{Pasquale2015BlackBoxSociety} highlights the risks of ``black-box'' algorithms in society: if an AI system
surfaces a piece of information or makes a recommendation, but its rationale is opaque, employees might blindly
follow it or ignore relevant context, thus compounding errors. Organizational memory augmented by AI must
therefore be managed with attention to source monitoring and critical evaluation --- just as individuals need to
monitor sources when integrating AI summary content, organizations need policies and cultures that encourage
verifying AI-provided information against original data or expert opinion. Maintaining metacognitive awareness
(knowing what we know and do not know) and preserving ``interpretive autonomy'' (the human ability to contextualize
and question information) are crucial in these hybrid human--AI environments \citep{Pasquale2015BlackBoxSociety}.
Firms may need to train employees not just in how to use AI tools, but in how to remain vigilant about the origins
and accuracy of the information those tools provide.

The practical relevance of AI-augmented memory is also evident in applied systems across sectors that rely heavily
on information processing, recall accuracy, and adaptive reasoning. Applied AI--memory systems illustrate how
algorithmic tools can function as external supports for encoding, retrieval, and metacognitive regulation in
real-world settings, reshaping human performance in domains such as healthcare, education, knowledge work, and
everyday life through augmented reality.

In healthcare and neurotechnology, AI-driven systems have emerged as tools for diagnosis, rehabilitation, and
cognitive support. Neuroadaptive frameworks, for example, integrate EEG or fMRI brain monitoring with machine
learning algorithms to track attention and detect moments of memory lapse, then deliver timely, personalized
stimuli or reminders. Such systems can act as external memory aids for patients. Preliminary studies suggest that
direct brain--machine interventions can improve memory performance: for instance, AI-guided electrical stimulation
of the hippocampus (delivered only at times when the AI predicts it will be beneficial) has been shown to enhance
episodic recall in clinical trials \citep{Kucewicz2023BrainStimulationMemory}. These advances illustrate how
biological memory processes can be externally scaffolded, forming hybrid therapeutic loops that combine neural
signals with algorithmic predictions.

In education, memory augmentation takes the form of intelligent tutoring systems (ITS) and generative AI tutors
that adapt to learners’ needs. Decades of research on ITS have confirmed that computerized tutors can improve
learning outcomes by tailoring practice and feedback to the individual \citep{Ma2014ITSMetaanalysis,Alshaikh2021ITS}.
These systems monitor student performance and dynamically adjust the difficulty, pacing, or review schedule of
material, thereby enhancing retention and metacognitive accuracy. Active learning studies show that engaging
students in deep processing (through activities like self-explanation or testing) yields more durable memory than
passive reading \citep{Freeman2014ActiveLearning}. Generative AI tutors can leverage this principle by sustaining a
reflective dialogue with the student --- asking questions, providing hints, and prompting the student to recall or
apply concepts. Early studies indicate that such AI tutors can reinforce encoding depth and retention. For
instance, Abrar et al. \citep{Abrar2025Cognitive} found that an AI tutor which asks students to recollect previous
topics at strategic points helps integrate new knowledge with old, functioning much like a human tutor who
periodically reviews past lessons. Likewise, the findings by Bai et al. on ChatGPT (mentioned earlier) suggest a
potential for generative AI to keep learners actively thinking and explaining, rather than passively consuming
information. Collectively, these systems illustrate AI’s capacity to transform educational memory processes into
interactive, personalized cognitive ecosystems.

In the productivity and knowledge-work domain, AI tools increasingly serve as ``second brains'' or organizational
memory companions. They help professionals manage the deluge of digital information by unifying fragmented data
sources into a coherent, context-aware retrieval system. For instance, an AI assistant integrated with a company’s
knowledge base can automatically generate contextual summaries of project documentation, or remind a team member of
relevant past decisions when a similar problem arises. These tools often provide semantic search, automatic
summarization of documents or meeting transcripts, and proactive cueing. Such capabilities enable workers to recall
project histories, technical rationales, or cross-department knowledge with minimal effort
\citep{Memoro2024RealTimeMemoryAugmentation,Gerlich2025AIToolsSociety}. As these tools scale up in enterprises, the
locus of institutional memory shifts: instead of residing only in veteran employees’ heads or scattered documents,
memory becomes an accessible resource distributed across human--AI networks. This can reduce knowledge loss from
staff turnover and help prevent teams from ``reinventing the wheel'' by enabling rapid access to what was tried
before. At the same time, it underscores the importance of ensuring that the AI’s knowledge base is accurate and
updated --- the organization’s memory is only as reliable as the data and metadata that the AI has.

A rapidly emerging frontier is the use of augmented reality (AR) interfaces as memory augmentation tools in
everyday life. For example, the ``AR Secretary Agent'' described by El Haddad et al. \citep{ElHaddad2025ARSecretaryAgent}
combines wearable AR glasses with an LLM-based assistant. This system can provide real-time conversational recall
and contextual reminders visually overlaid onto the user’s environment. Imagine walking into a meeting and seeing
through AR glasses the names and roles of the people in front of you, along with a summary of your last
conversation with them --- all generated by an AI accessing personal notes and calendar information. This kind of
embodied memory augmentation means that recollection becomes a continuous, on-demand process: part of the memory is
in the user’s brain, and part is filled in by the AI through the AR interface. Early prototypes can, for instance,
listen to ongoing conversations and display relevant facts or definitions to the user, aiding memory and
comprehension without interrupting the flow of interaction.

Across these domains, a common pattern emerges: memory is evolving from a purely internal cognitive function into a
hybrid process supported by adaptive, context-aware, interactive technology. These applications underscore the need
for updated theoretical frameworks that can account for this distributed nature of memory. Traditional memory
research dealt mostly with how individuals encode and retrieve information on their own; the examples above show
situations where encoding and retrieval are shared between a person and an AI system. As we proceed, the
experimental work in this thesis will inform one such framework --- the AI Buffer model --- by examining a simpler
instance of human--AI memory integration (reading with AI-provided summaries). Before introducing that model, we
summarize the gaps in current knowledge and the specific research questions that drive this thesis.

\subsection{Research Gaps and Predictions for this Thesis}

Research on human memory has a rich history, and the advent of AI has spurred new interest in how technology might extend or alter memory processes. However, there remain important gaps in both theory and evidence regarding the interaction of AI systems with core cognitive functions. Classical models of memory provide detailed accounts of how people encode, consolidate, and retrieve information on their own \citep[e.g.,][]{Atkinson1968HumanMemory, Baddeley2012WorkingMemory, Tulving1972EpisodicSemantic}, and contemporary perspectives emphasize that memory is dynamic, context-dependent, and distributed across brain networks and environments \citep{Moscovitch2006CognitiveNeuroscienceRemoteMemory, Liu2021TransformativeNeuralRepresentations, ShamayTsoory2019RealLifeNeuroscience}. Yet, these frameworks were developed in eras when cognition operated without intelligent external agents. As AI systems become embedded in everyday cognitive activities, existing theories struggle to account for the hybrid nature of human–AI memory interactions. Fundamental questions remain unanswered: Does having an AI “partner” fundamentally change the way we form and recall memories, or does it simply shift some of the burden outside the mind? Does AI support lead to qualitatively different memory traces (perhaps more schematic or more dependent on recognition cues) compared to unaided memory? At present, we lack definitive answers, because controlled experimental evidence on AI-assisted vs. unaided cognition is scarce. AI tools are widely presumed to \emph{support} memory (and indeed often marketed as such), but it is unclear whether their presence truly enhances internal encoding and retrieval processes, merely shifts memory externally, or potentially introduces new forms of bias and distortion.

A closely related gap is the absence of an \emph{integrated cognitive theory} explaining how AI influences memory mechanisms. While initial empirical studies (including those reviewed in this chapter) suggest that engaging with AI can sometimes improve learning outcomes \citep{Bai2023ChatGPTLearningMemory, Haider2024AICognitiveFunctions}, these findings are not yet well-connected to foundational memory principles. For instance, if AI-based prompting leads to deeper processing of material, one could interpret it through the lens of the Levels of Processing framework \citep{Craik1972LevelsOfProcessing}. If AI provides context cues that aid retrieval, one might invoke the encoding specificity principle \citep{Tulving1973EncodingSpecificity}. However, such connections remain speculative. There is not yet a unifying account of how intelligent external systems might extend, interrupt, or reorganize classic memory operations like encoding and retrieval. Developing such an account is challenging because it requires interdisciplinary insight — combining what we know from cognitive psychology (e.g., about attention, generation effects, retrieval practice) with what we observe about AI usage behavior. This thesis takes a step in that direction by proposing the AI Buffer model in the next chapter, aiming to bridge some of these theoretical gaps.

Beyond broad theory, there are specific dimensions of AI assistance that have been under-investigated. Much of the existing research on AI’s cognitive effects has been observational or focused on learning outcomes in educational settings \citep[e.g.,][]{Zhai2024OverRelianceAIDialogue, Ma2014ITSMetaanalysis}. These studies are valuable in highlighting potential benefits and pitfalls, but they rarely isolate individual factors in a controlled way. Key design variables — such as when and how AI support is given — have not been systematically studied. For example, it is unknown whether providing AI-generated summaries or answers \emph{before} exposure to the primary material yields better retention than providing them \emph{afterward}, or whether having AI information \emph{during} a task is distracting or beneficial. Likewise, we do not know how the \emph{format} of AI-provided information (narrative text vs. bullet-point list, integrated vs. separate) influences a user’s ability to learn from and remember that information. Although various AI applications exist that implicitly touch on these questions — e.g., an email client that offers a summary preview versus a search engine that provides answers after a query — there have been few if any experiments directly comparing such conditions. One recent system (Memoro \citep{Memoro2024RealTimeMemoryAugmentation}) simulates real-time “rehearsal” by interjecting summary prompts, and another study demonstrated AI-induced memory errors in conversation \citep{Chan2024ConversationalAIFalseMemories}, but neither line of work systematically manipulates presentation \emph{timings} or \emph{formats} to assess causal effects on memory performance. This thesis addresses that gap by experimentally varying when and how AI support is delivered.

Another unresolved issue emerges from the literature on cognitive offloading and transactive memory. We have robust evidence that people offload memory tasks to digital tools (like search engines or smartphone apps) and that this changes memory behavior \citep{Sparrow2011GoogleEffect, Risko2016CognitiveOffloading, Gong2024GoogleEffectsMetaAnalysis}. However, most of this work has examined fairly static or one-directional tools (you ask Google a question; it gives an answer). Modern AI assistants are interactive, proactive, and capable of structuring information on the fly. They do not just hold information; they transform and present it. As a result, existing offloading theories may not fully capture the “two-way” interplay between user and AI. In a back-and-forth with ChatGPT, for example, the user’s memory and the AI’s outputs continuously influence each other. This thesis aims to shed light on that interplay by focusing on a scenario where AI actively provides summaries during a learning task. By observing how participants use or don’t use the AI summary at different stages, we can better understand how AI alters study habits and memory encoding strategies, thereby informing an extension to offloading theory for interactive agents.

Finally, there is a clear gap regarding \emph{source monitoring and epistemic trust} in AI-assisted memory. As reviewed, initial studies raise red flags: users can confuse AI-provided information with their own memory (e.g., the Chan et al. false memory finding) and may become overconfident in answers that an AI helps generate. Zhang et al. \citep{Zhang2024KnowledgeImbalanceAI} describe how reliance on AI can lead to an imbalance where users defer to AI even on questions they could answer correctly themselves, potentially impairing their judgment. What’s missing is controlled research on how manipulating factors like summary format or explicit source labels in an AI interface might improve or worsen source monitoring outcomes. For instance, would users be less susceptible to false memories if the AI’s summary is clearly delineated or if they receive training in distinguishing AI content? Or do certain presentation modes (like an integrated summary) inherently reduce source confusion relative to others? Prior to this work, no experimental data directly addressed these questions. This thesis begins to fill that gap by measuring not only factual recall and recognition, but also false memory rates and source attribution errors under different AI assistance conditions. In doing so, it provides empirical evidence on whether specific interface choices can mitigate epistemic risks.

Taken together, these gaps point to the need for a focused investigation into whether and how AI assistance alters fundamental memory processes, and how specific properties of AI support (timing, format, etc.) modulate those effects. To tackle this, the next chapter introduces the \emph{AI Buffer Model}, a conceptual framework that hypothesizes how AI-generated external representations (like summaries) interface with human memory systems. This model, and the predictions derived from it, directly inform the experimental design of this thesis. In our study, we contrast unaided reading with AI-assisted reading while systematically varying the timing (pre-reading vs. synchronous vs. post-reading summary access) and structure (integrated vs. segmented summary format) of the AI support. By observing the resulting differences in recall accuracy, comprehension (recognition test performance), meta-memory judgments, and misinformation endorsement, we can draw conclusions about the cognitive impact of these dimensions.


In conclusion, this literature review has identified significant gaps in our understanding of AI and memory, and, through established cognitive theories, it has generated testable predictions about the impact of AI summary assistance. The next chapter introduces the AI Buffer Model, which provides a conceptual framework for these predictions, and the subsequent Methodology and Results chapters will detail how we empirically evaluate them. By investigating whether a well-timed, well-formatted AI summary can serve as an effective “memory buffer” — and what the risks are if it is poorly timed or formatted — this thesis aims to contribute both practical insights for interface design and theoretical insights into the evolving nature of memory in the age of AI.






%---------------------------------------------------------
\chapter{Conceptual Framework and Hypotheses}
\section{The AI Buffer Model}

The increasing use of artificial intelligence (AI) systems as cognitive support
tools challenges traditional assumptions about how memory processes are
organized and supported. Classical models of human memory conceptualize
encoding, storage, and retrieval as primarily internal processes, regulated by
attentional control, rehearsal, and executive coordination
\citep{Atkinson1968HumanMemory, Baddeley2012WorkingMemory, Tulving1972EpisodicSemantic}.
However, contemporary AI applications increasingly provide persistent,
content-based representations that accompany users during cognitive tasks,
thereby introducing an external layer of informational support.

To interpret the cognitive effects of such support, this thesis introduces the
\emph{AI Buffer model} as a parsimonious conceptual lens. The AI Buffer refers to
AI-generated external representations that persist during a task and have the
potential to influence memory encoding, retrieval, and metacognitive monitoring.
The model does not posit a new neurocognitive module, nor does it aim to provide
a general theory of AI-augmented work. Rather, it characterizes a functional role
that AI-generated representations may play in shaping memory-related behavior
under controlled experimental conditions.


The AI Buffer model is grounded in established theories of human memory. In the
modal model of memory \citep{Atkinson1968HumanMemory}, short-term memory functions
as a temporary buffer that regulates access to long-term storage through
rehearsal and attentional control. Subsequent developments, particularly
Baddeley’s working memory framework \citep{Baddeley2012WorkingMemory}, emphasized
the active and multicomponent nature of this buffer, highlighting its role in
managing cognitive load and integrating information across modalities.

The AI Buffer extends these perspectives by considering how external,
AI-generated representations may complement or partially substitute internal
buffering processes. Unlike traditional external aids, such as static notes or
search engines, AI systems can dynamically structure information, highlight
conceptual relations, and provide context-sensitive cues. This view aligns with
distributed cognition approaches, which emphasize that cognitive processes may
span internal and external representational resources
\citep{Gerlich2025AIToolsSociety}.

At the same time, the AI Buffer remains compatible with the encoding specificity
principle \citep{Tulving1973EncodingSpecificity}. By storing structured semantic
content and contextual cues, AI-generated representations may facilitate
retrieval by reinstating aspects of the original encoding context. However, such
external cues may also interfere with source monitoring, particularly when users
struggle to distinguish between internally encoded information and externally
generated content.



Within the proposed framework, the AI Buffer may influence memory through three
primary mechanisms.

First, during \emph{encoding}, the AI Buffer can shape attentional allocation and
depth of processing. Integrated, relational representations may promote deeper
semantic elaboration, consistent with the Levels of Processing framework
\citep{Craik1972LevelsOfProcessing}, whereas segmented or item-based
representations may encourage more superficial, item-specific encoding.

Second, during \emph{maintenance and consolidation}, the AI Buffer may externalize
rehearsal processes by allowing users to revisit, restate, or verify information.
Evidence from AI-supported learning environments suggests that such reflective
interaction can enhance retention when cognitive engagement is maintained
\citep{Bai2023ChatGPTLearningMemory, Memoro2024RealTimeMemoryAugmentation}.

Third, during \emph{retrieval}, the AI Buffer can function as a source of
contextual and semantic cues, potentially facilitating access to stored
representations. At the same time, reliance on external cues may reduce the need
for internal reconstruction, increasing susceptibility to cognitive offloading
\citep{Risko2016CognitiveOffloading, Sparrow2011GoogleEffect} and source-monitoring
errors \citep{Chan2024ConversationalAIFalseMemories}.



In the present thesis, the AI Buffer is operationalized through AI-generated
summaries in a controlled memory experiment (Study~1). This operationalization
allows systematic manipulation of the \emph{presence} of the AI Buffer (AI vs.\
No-AI), as well as its \emph{structure} (integrated vs.\ segmented) and
\emph{timing} (pre-reading, synchronous, post-reading). These dimensions enable
direct examination of how different configurations of external AI-generated
representations affect recall accuracy, recognition performance, source
monitoring, confidence calibration, and cognitive load.

Importantly, the AI Buffer model is introduced to interpret behavioral effects
observed under experimental conditions and does not claim to generalize across
all forms of AI interaction. Its purpose is to clarify how AI-generated
representations interact with core memory processes, providing a conceptual
foundation for the hypotheses tested in Study~1 and an interpretative reference
point for subsequent discussion.

\section{Conceptual Dimensions and Predictions}
\label{sec:conceptual-dimensions}

The AI Buffer model becomes empirically testable in Study~1 because it is
expressed as a small set of manipulable \emph{conceptual dimensions}. These
dimensions specify \emph{when} and \emph{how} an external AI-generated
representation can interact with encoding and retrieval, without implying a
single uniform ``AI effect''.

\paragraph{Presence of the AI Buffer (AI vs No-AI).}
The primary contrast is whether an external representation is present at all.
When a summary is available, the learner has access to an additional, condensed
representation of the text that can (i) provide extra semantic cues, (ii) reduce
the cost of re-accessing information, and (iii) shift the balance between
internal reconstruction and reliance on external support. Conceptually, this
dimension is expected to affect cue availability and the degree of cognitive
offloading, with the possibility that recognition-based outcomes benefit more
directly from added cues than free recall.

\paragraph{Timing of the AI Buffer (pre / synchronous / post).}
Timing determines \emph{when} the external representation can influence
processing. A pre-reading summary can function as an advance framework that
guides attention during encoding. Synchronous access allows consultation during
encoding but may also promote intermittent switching between sources. Post-reading
access occurs after initial encoding and is expected to primarily support review,
verification, and cue reinstatement before later retrieval. Conceptually, timing
is expected to influence recognition and other cue-supported outcomes more than
generative recall, because the timing manipulation primarily changes the
availability and placement of retrieval cues around the encoding phase.

\paragraph{Structure of the AI Buffer (integrated vs segmented).}
Structure determines \emph{how} information is organized in the external
representation. An integrated, paragraph-based summary emphasizes relations and
coherence, potentially supporting relational integration and a situation-model
representation. A segmented, bullet-based summary emphasizes discrete items and
may reduce coherence cues, which can change how information is bound and how
sources are attributed. Conceptually, structure is expected to shape the trade-off
between relational understanding and item-based processing, and to matter
especially for source monitoring and susceptibility to plausible but incorrect
information.

Together, these dimensions specify the ways in which the AI Buffer can interact
with core memory processes in a controlled setting, motivating a set of
directional hypotheses for Study~1.

\section{From Conceptual Framework to Hypotheses}
\label{sec:framework-to-hypotheses}

The central expectation derived from the proposed AI Buffer model is that AI assistance
will not uniformly enhance or impair memory. Instead, effects should depend on
\emph{when} the buffer is available (timing) and \emph{how} it organizes
information (structure), with potentially different consequences for cue-based
recognition versus generative recall. Timing is expected to primarily influence
encoding efficiency and cue-supported outcomes (such as multiple-choice
recognition), whereas structure is expected to primarily influence relational
integration and source-monitoring integrity.

\paragraph{Timing-based predictions (advance organizers and interruption costs).}
In line with advance-organizer theory, providing an AI-generated summary
\emph{before} reading may be especially effective because it can supply a schema
that guides attention and supports the integration of upcoming information. This
predicts that pre-reading access should yield the strongest learning for
summary-covered propositions, with a corresponding advantage in recognition
performance on summary-aligned items. By contrast, synchronous access may be
less effective because it interrupts reading and can introduce switching costs,
and post-reading access may function primarily as review because it arrives
after initial encoding. Overall, we therefore anticipate a timing benefit
ordering of \textbf{Pre \(>\) Post \(\geq\) Synchronous}, with larger effects on
recognition-based outcomes than on free recall quality.

\paragraph{Structure-based predictions (coherence cues and epistemic risk).}
Drawing on cognitive load and source monitoring accounts, integrated paragraph
summaries may provide richer coherence cues than segmented bullet lists,
supporting a more connected mental representation of the text and facilitating
verification. In contrast, segmented summaries emphasize discrete items and may
encourage decontextualized processing, increasing the risk that plausible
summary content is later misattributed to the article. This predicts weaker
source-monitoring integrity and higher false-lure endorsement under segmented
structure.

\paragraph{Potential interactions.}
In addition to main effects, the literature suggests that timing and structure
may interact: structural differences could matter most when the summary is
encountered \emph{during encoding} (pre-reading or synchronous access), whereas
format may matter less when the summary is only seen after reading. Study~1
allows exploratory tests of such interactions, although power for higher-order
effects is limited.

\paragraph{Individual differences and subjective reliance.}
Individual differences (e.g., prior knowledge and baseline AI trust/dependence)
are expected to shape how participants engage with the buffer and the extent to
which AI-provided cues are incorporated into memory judgments. In Study~1, prior
knowledge and baseline trust/dependence are treated as stable individual
differences, whereas post-block trust and dependence are captured as
condition-dependent (state) ratings after each article. This enables
exploratory tests of whether perceived reliance varies systematically across
timing and structure conditions and how it relates to behavioral reliance (e.g.,
summary viewing time/share) and performance within the same block.

These expectations are formalized below as directional hypotheses aligned with
the research questions.

\subsection{Hypotheses}
\label{sec:hypotheses}

\begin{enumerate}
  \item \textbf{H1 (AI vs No-AI).} Relative to unaided reading, access to AI assistance (operationalized as an AI-generated summary) is expected to improve recognition performance, with weaker or no improvement expected for free recall quality.

  \item \textbf{H2 (Timing; AI only).} Pre-reading access to the AI summary is expected to yield higher recognition performance than synchronous or post-reading access. Timing effects are expected to be larger for recognition-based outcomes than for free recall.

  \item \textbf{H3 (Structure; AI only).} Integrated summaries are expected to better support coherent mental-model construction than segmented summaries, resulting in higher recall quality and stronger performance on article-dependent outcomes.

  \item \textbf{H4 (Misinformation and source monitoring; AI only).} Segmented summaries are expected to increase susceptibility to misinformation relative to integrated summaries, operationalized as a higher rate of false-lure endorsement and reduced source-monitoring integrity.

  \item \textbf{H5 (Exploratory; individual differences and subjective reliance).} Prior knowledge is expected to modulate reliance on the summary (higher prior knowledge $\rightarrow$ less reliance). In addition, post-block AI trust and AI dependence are expected to vary across timing and structure conditions and to covary with behavioral reliance (e.g., summary viewing time/share) and within-block performance.
\end{enumerate}




% =========================
% 4 | Methodology
% =========================

\chapter{Methodology}
\label{ch:methodology}

\section{General methodology}
\label{sec:general-methodology}
This thesis uses a controlled laboratory paradigm to study how external representations influence memory for expository texts. Study~1 implements a timed reading-and-testing protocol in which AI assistance is operationalized as pre-generated summaries that vary in \emph{structure} and \emph{timing} relative to reading.

\section{Study 1: AI--Memory experiment design}
\label{sec:study1-design}

This study investigates whether and how AI assistance alters memory encoding and retrieval under controlled reading conditions. Specifically, it examines whether AI-generated summaries influence memory performance relative to unaided reading, and whether different summary configurations are associated with distinct cognitive outcomes.

The methodological choices are grounded in established theories of memory and cognition, including levels of processing \citep{Craik1972LevelsOfProcessing}, working memory constraints \citep{Baddeley2012WorkingMemory}, encoding specificity \citep{Tulving1973EncodingSpecificity}, and cognitive offloading \citep{Risko2016CognitiveOffloading,Sparrow2011GoogleEffect}.

\subsection{Participants and compensation}
\label{subsec:participants}
Thirty-six adults were retained for analysis (\(N_{\mathrm{AI}}=24\), \(N_{\mathrm{NoAI}}=12\)), including university students and early-career professionals. All participants provided informed consent prior to participation.

Participants were 18--35 years old (\(M=24.33\), \(SD=3.83\)) and were balanced overall by gender (18 women, 18 men). All participants reported Chinese as their native language.

\begin{table}[t]
\centering
\small
\caption{Participant demographics by group.}
\label{tab:participant_demographics}
\begin{tabular}{lccc}
\toprule
 & AI (\(n=24\)) & No-AI (\(n=12\)) & Total (\(N=36\)) \\
\midrule
Age, \(M\) (\(SD\)) & 24.50 (4.31) & 24.00 (2.76) & 24.33 (3.83) \\
Age range & 18--35 & 20--30 & 18--35 \\
Gender (F/M) & 13/11 & 5/7 & 18/18 \\
Native language (Chinese), \(n\) (\%) & 24 (100\%) & 12 (100\%) & 36 (100\%) \\
\bottomrule
\end{tabular}
\end{table}

Participants received a base compensation of 60~RMB plus a performance-based bonus of up to 60~RMB linked to recognition-test accuracy.

\subsection{Experimental design}
\label{subsec:design}
AI assistance was operationalized as AI-generated summaries. Within the AI-assisted participants, the core design forms a \(2 \times 3\) matrix (Structure: integrated vs.\ segmented; Timing: pre-reading vs.\ synchronous vs.\ post-reading): timing is manipulated within subjects and structure is manipulated between subjects. A separate No-AI baseline group provides an external comparison but does not include the timing manipulation, resulting in an asymmetric mixed design. This structure was selected to (i) increase statistical power for timing effects by using each participant as their own control across timing conditions and (ii) avoid carryover and format-adaptation effects by keeping summary structure constant for each participant across all three blocks.
\begin{itemize}
  \item \textbf{Summary structure (between-subjects):} No-AI baseline vs Integrated vs Segmented.
  \item \textbf{Summary timing (within-subjects, AI only):} Pre-reading vs Synchronous vs Post-reading.
\end{itemize}
Participants in the AI-assisted groups experienced all three timing conditions exactly once (one per article). Timing order was implemented via one of the six possible permutations of the three timing conditions (e.g., Sync--Pre--Post; Pre--Sync--Post; etc.), sampled per participant. Article order was independently randomized, so the mapping between a given article and a given timing condition varied across participants.

\paragraph{Structure conditions.}
Participants were assigned to one of three structure conditions:
\begin{itemize}
  \item \textbf{A0 (No-AI baseline):} articles presented without any AI summary.
  \item \textbf{A1 (Integrated summary):} a coherent paragraph-style summary (\(\sim\)250 words).
  \item \textbf{A2 (Segmented summary):} a bullet-point summary (7--10 bullets; \(\sim\)250 words).
\end{itemize}
In the final AI sample, structure groups were balanced (\(n=12\) Integrated; \(n=12\) Segmented).

\paragraph{Timing conditions (AI only).}
\begin{itemize}
  \item \textbf{Pre-reading:} a separate summary page shown \emph{before} reading (up to 3~min; participants could continue earlier), followed by a reading phase capped at 12~min.
  \item \textbf{Synchronous:} the summary could be opened/closed on demand during the reading phase; the reading window was 15~min.
  \item \textbf{Post-reading:} a reading phase capped at 12~min followed by a separate summary page shown \emph{after} reading (up to 3~min; participants could continue earlier).
\end{itemize}
The No-AI baseline used a reading phase capped at 15~min per article and had no timing manipulation.

% If you use booktabs, add \usepackage{booktabs} in the preamble.
\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.15}
\begin{tabularx}{\textwidth}{l l >{\raggedright\arraybackslash}X l}
\hline
\textbf{Condition} & \textbf{Summary timing} & \textbf{Summary access} & \textbf{Reading window} \\
\hline
No-AI & -- & None & 15 min \\
Pre-reading & Before reading & Separate page, \(\leq\) 3 min (early continue allowed) & 12 min \\
Synchronous & During reading & On-demand open/close & 15 min \\
Post-reading & After reading & Separate page, \(\leq\) 3 min (early continue allowed) & 12 min \\
\hline
\end{tabularx}
\caption{Implementation of timing conditions and exposure windows.}
\label{tab:timing-implementation}
\end{table}

\subsection{Materials and platform}
\label{subsec:materials}

\subsubsection{Materials}
\paragraph{Texts.}
Participants read three expository articles of approximately 1{,}300 words each (topics included urban heat islands, CRISPR, and semiconductor supply chains), selected to be comparable in difficulty and conceptual density. All participant-facing texts were administered in Simplified Chinese using fixed, pre-generated stimulus content; English translations are used throughout the thesis for exposition and reproduced alongside the Chinese originals in Appendix~A.

\paragraph{AI summaries (structure, incompleteness, and false lures).}
Summaries were generated and refined \emph{in advance} and were approximately \(\sim\)250 words. They were identical in informational content across the two AI structure conditions, differing only in formatting (integrated paragraph vs.\ segmented bullets). Summaries were intentionally incomplete (approximately 15--20\% of article information omitted) to discourage reliance on summaries alone and to ensure that some recognition items required article-based encoding.

To assess susceptibility to misinformation and source-monitoring-related errors, each article’s summary contained two plausible but incorrect statements (\emph{false lures}). These false-lure concepts also appeared as distractor options in the recognition test; selecting them was counted as false-lure acceptance.

\subsubsection{Platform and implementation}
\label{subsubsec:platform}
The experiment was delivered via a custom browser-based application built with a Python (Flask) backend and HTML/CSS/JavaScript front-end pages. We developed the platform to support (i) strict enforcement of timing constraints across conditions, (ii) automated counterbalancing and randomization, and (iii) fine-grained behavioral logging of reading and summary exposure. All stimuli (articles, summaries, and MCQs) were preloaded into the platform in advance, ensuring that content was identical across participants and removing runtime variability due to on-the-fly AI generation.

\paragraph{Participant management and device constraints.}
Participants selected the study language (English or Simplified Chinese) at the beginning of the session. At login, participants provided demographic information and the system generated an anonymized participant identifier (e.g., \texttt{P001}) to link all subsequent responses. Mobile devices were blocked using a browser user-agent check at the start of the session, and the interface requested full-screen mode on study entry.

\paragraph{Condition assignment and counterbalancing.}
For AI participants, summary structure was assigned between subjects (integrated vs.\ segmented). Summary timing was implemented within subjects by sampling one of the six possible permutations of the three timing conditions. Article order was randomized independently, so the mapping between a given article and a given timing condition varied across participants. The resulting assignment (structure, timing order, and article order) was stored in the session and written to the participant log for analysis.

\paragraph{Flow control and timing.}
The platform enforced condition-specific timing constraints on reading and summary exposure (see \Cref{tab:timing-implementation}) using on-screen countdown timers and automatic redirects at timeout. Reading had a 12-minute cap in the pre- and post-reading conditions and a 15-minute cap in the synchronous condition; the No-AI baseline used a 15-minute cap. Pre- and post-reading summaries were shown on a dedicated summary page for up to 3 minutes (participants could continue earlier), whereas in the synchronous condition the summary could be opened and closed on demand during the reading phase. After each reading phase, a non-skippable 3-minute break preceded testing; between articles, a short break of up to 2 minutes could be ended early.

Free recall lasted up to 5 minutes and MCQs lasted up to 7 minutes, with automatic submission at timeout. To reduce immediate skipping of recall, the continue button remained locked for the first 90 seconds. Throughout the session, back-navigation was disabled to prevent revisiting prior pages or materials.

\paragraph{Behavioral logging and export.}
The server recorded time-stamped events including reading completion time, scroll depth, page visibility changes, summary view counts and durations, recall responses, MCQ answers and response times, and post-article ratings. Data were exported as participant-level CSV logs and a separate participant file containing demographics and condition metadata.

\subsection{Procedure}
\label{subsec:procedure}
Participants were tested individually in a controlled lab setting. Sessions lasted approximately 80--100 minutes.

\begin{enumerate}
  \item \textbf{Consent, language selection, and instructions.} Participants selected the study language (English or Simplified Chinese) and were instructed to avoid distractions and to maximize performance. AI-group instructions stated that AI summaries are generally helpful but may contain omissions or minor mistakes.
  \item \textbf{Baseline questionnaires.} Demographics and a prior-knowledge questionnaire were administered. AI-group participants also completed baseline (general) measures of AI trust and AI dependence, reflecting overall attitudes toward AI-generated summaries prior to the reading task.
  \item \textbf{Three reading--testing blocks (one per article).} Each block followed this sequence:
  \begin{enumerate}
    \item \textbf{Summary exposure (AI only; timing-dependent):} implemented according to the assigned timing condition (see \Cref{tab:timing-implementation}).
    \item \textbf{Article reading (timed):} No-AI read for up to 15~min; AI read for up to 12~min in pre/post conditions and up to 15~min in synchronous.
    \item \textbf{Mandatory retention interval:} a non-skippable 3-minute break occurred after the reading phase (and, in post-reading, after the summary page), before testing began.
    \item \textbf{Free recall (timed):} participants produced an open-ended recall response (entered as bullet points) for up to 5~minutes. To reduce immediate skipping, the continue button was time-locked for the first 90~seconds.
    \item \textbf{Recognition test (timed):} a 14-item multiple-choice test per article (7~min time limit).
    \item \textbf{Post-article ratings:} participants reported mental effort and perceived difficulty on 7-point scales, plus an overall confidence rating for the MCQ block. AI-group participants additionally completed post-block (state) ratings of AI trust and AI dependence tied to the specific summary they had just used in that article block (\texttt{trust\_new}, \texttt{dependence\_new}; item wording in Appendix~\Cref{app:instruments}). These items were administered after the recall and MCQ tasks so that ratings reflected the full interaction experience for that condition.
  \end{enumerate}
  \item \textbf{Between-block breaks:} after each block, a short break of up to 2~minutes was available before the next article (self-paced; skippable).
  \item \textbf{Manipulation checks and debrief.} After the third block, participants completed manipulation-check items (e.g., perceived coherence/connectivity) and were debriefed.
\end{enumerate}

\paragraph{Rationale for baseline and post-block trust/dependence.}
To distinguish general predispositions from condition-dependent responses, AI-group participants completed baseline (general) trust and dependence items before the task and post-block (state) trust and dependence ratings after each article. This repeated measurement approach is theoretically motivated by frameworks of trust calibration and metacognitive monitoring, which propose that users update perceived trustworthiness and reliance based on recent tool performance, presentation cues, and task outcomes. Because timing and structure manipulations can change how participants use and evaluate the summary within a given block, post-block ratings provide a sensitive measure of how each condition shapes perceived trust and dependence in situ. Methodologically, the baseline measures help document prior attitudes (expected to be balanced by randomization), while the post-block ratings constitute within-participant repeated measures, allowing analyses to test whether trust/dependence systematically vary across timing and structure conditions and to explore how perceived reliance relates to time allocation and performance within the same block.

\subsection{Measures and scoring}
\label{subsec:measures}
\paragraph{Free recall (0--10).}
Free recall was scored as an index of episodic--semantic reconstruction of expository text rather than verbatim reproduction. In line with constructive memory theory \citep{Bartlett1932Remembering}, responses were evaluated for fidelity to the original conceptual structure (and penalized for plausible distortions), not for length or fluency.

Each article recall received a single ordinal score from 0 to 10 based on five rubric dimensions:
\begin{enumerate}
  \item \textbf{Factual accuracy:} consistency with the article content; inventions, incorrect mechanisms, or misplaced causal relations reduced the score.
  \item \textbf{Mechanistic reconstruction:} recall of causal or explanatory chains (e.g., linking albedo $\rightarrow$ heat storage $\rightarrow$ night-time release; or guide RNA $\rightarrow$ Cas enzyme $\rightarrow$ editing constraints), beyond outcome labels.
  \item \textbf{Structural alignment:} preservation of the article's internal organization (e.g., causes vs.\ consequences; constraints vs.\ solutions; technical vs.\ social dimensions), reflecting whether relational structure was maintained.
  \item \textbf{Specificity vs.\ gist:} preference for diagnostic, article-specific details (e.g., numeric ranges, named mechanisms, concrete constraints) over generic domain summaries, consistent with accounts of gist-dominated remembering \citep{BrainerdReyna2005ScienceFalseMemory}.
  \item \textbf{Source monitoring integrity:} penalties for intrusions and, in particular, recall of experimentally planted false-lure statements, consistent with source-monitoring theory \citep{Johnson1993SourceMonitoring}.
\end{enumerate}

To improve scoring consistency, anchor bands were used: 9--10 (high accuracy, strong mechanisms, preserved structure, minimal distortion), 7--8 (mostly accurate with minor omissions), 5--6 (gist-level recall with limited mechanistic detail), 3--4 (headline-level/definitional recall), and 1--2 (fragmentary or largely reconstructed memory).

\textbf{LLM-assisted free-recall scoring with full human review.} Free-recall responses were initially scored using an LLM rater applying the pre-specified 0--10 rubric (five dimensions and anchor bands). To increase scoring consistency and reduce variability associated with manual qualitative rating, the LLM was provided with the rubric definitions and returned a single ordinal score for each response. All LLM-generated scores were then reviewed by a human rater (100\% of responses) to confirm rubric adherence (including appropriate penalization for false-lure intrusions) and to correct any questionable cases according to the rubric.

\paragraph{Recognition performance.}
Recognition was measured as proportion correct on the 14 MCQs per article. Items were categorized by information source (summary-sourced vs article-only), enabling separate accuracy indices. Misinformation susceptibility was indexed by the number of false-lure options selected (0--2 per article; 0--6 overall).

\paragraph{Self-report and behavioral logs.}
Participants provided recall confidence and post-article ratings of mental effort and perceived difficulty (7-point scales), as well as an overall confidence rating for the MCQ block. AI-group participants additionally completed baseline (general) measures of AI trust and AI dependence and provided post-block (state) ratings of AI trust and AI dependence (\texttt{trust\_new}, \texttt{dependence\_new}; see \Cref{subsec:procedure}). Behavioral measures were computed from server logs, including reading time, summary view time, number of summary openings, and interaction timestamps.

\subsection{Data quality and exclusion criteria}
\label{subsec:exclusions}
Sessions were excluded prior to analysis if participants failed to complete the full protocol or showed clear non-compliance (e.g., implausibly short reading times indicating rushing, or non-substantive responses). The final analyzed sample comprised \(N=36\) complete sessions.

\subsection{Methodological rationale}
\label{subsec:rationale}
This design enables comparison between unaided reading and multiple controlled modes of AI-assisted encoding, testing whether AI support enhances, restructures, or disrupts memory formation. Manipulating summary structure targets relational vs item-specific encoding, while manipulating timing probes schema activation and reliance on external representations during encoding. The inclusion of false lures provides a controlled test of misinformation acceptance and source-integrity errors under AI assistance.

\chapter{Data Analysis}
\label{ch:data-analysis}

This chapter specifies the analysis procedures used to test the research questions. It describes (i) how raw study logs were converted into analysis datasets, (ii) how variables were defined and transformed, (iii) which statistical models were fit and why, and (iv) the conventions used for hypothesis testing and robustness checks. Consistent with thesis structure, this chapter reports procedures only; numerical outcomes are reported in the Results chapter and Appendix~C.

\section{Data and variables}
\label{sec:data-analysis-data}

\paragraph{Raw data.}
All participant responses and interaction events were recorded automatically by the experimental platform (see \Cref{subsubsec:platform}). After each session, the platform exported participant-level CSV files containing (i) condition assignments, (ii) questionnaire responses, (iii) free-recall text responses, (iv) item-level recognition responses, and (v) time-stamped behavioral logs (page entries, timers, and summary-view events). These CSV exports were treated as the source of truth for subsequent analysis.

\paragraph{Analysis dataset (long format).}
From the raw CSV exports, a single analysis dataset was constructed in \emph{long format}, with one row per participant \(\times\) article block. This representation preserves the repeated-measures structure of the design (three blocks per participant) and allows modelling at the block level while accounting for within-participant dependence. Each row contained: condition identifiers (\texttt{experiment\_group}, \texttt{structure}, \texttt{timing}, \texttt{article}); outcome variables (recall and recognition indices); post-block self-reports (e.g., mental effort and confidence); behavioral engagement measures derived from server logs; participant-level covariates (prior knowledge); and, for AI-group rows, post-block (state) ratings of AI trust and dependence for that specific article block (\texttt{trust\_new}, \texttt{dependence\_new}).

\paragraph{Variable derivation and merging logic.}
The dataset was built by joining (i) questionnaire exports, (ii) recognition-test response exports, (iii) free-recall responses and rubric scores, and (iv) event-log/timer exports using \texttt{participant\_id} and block identifiers (article and timing). Recognition indices were computed by scoring item responses and aggregating within block (overall accuracy and source-specific subsets). False-lure measures were computed by counting endorsement of pre-specified lure options within each block. Reading and summary exposure measures were derived from log timestamps and timer events; for synchronous timing, reading time was computed as net reading time excluding periods when the summary panel was open, and summary time was computed as cumulative summary-panel exposure. Internal consistency checks verified that each participant contributed exactly three blocks and that AI participants experienced each timing condition exactly once.

\paragraph{Experimental factors.}
Models used \texttt{experiment\_group} (AI vs No-AI), \texttt{structure} (Integrated vs Segmented; AI only), \texttt{timing} (Pre-reading vs Synchronous vs Post-reading; AI only), and \texttt{article} (text identity). Because the No-AI baseline has no timing manipulation, timing effects were evaluated within AI-assisted participants.

\paragraph{Outcome variables.}
Outcomes included \texttt{recall\_total\_score} (free-recall quality; rubric in \Cref{subsec:measures}), \texttt{mcq\_accuracy} (recognition accuracy), \texttt{article\_accuracy}, \texttt{ai\_summary\_accuracy} (AI only), \texttt{false\_lures\_selected} / \texttt{false\_lure\_accuracy} (AI only), post-block AI trust/dependence ratings (\texttt{ai\_trust}, \texttt{ai\_dependence}; AI only), \texttt{mental\_effort}, time-on-task measures (\texttt{reading\_time\_min}, \texttt{summary\_time\_sec}), and confidence ratings (\texttt{recall\_confidence}).

\paragraph{Covariates and preprocessing.}
When relevant, models included \texttt{prior\_knowledge\_familiarity} as a participant-level covariate. Post-block (state) AI trust and AI dependence (AI only; derived from \texttt{trust\_new}/\texttt{dependence\_new}) were analyzed primarily as outcomes (Structure \(\times\) Timing tests) and, in exploratory models, as block-level predictors of performance and time allocation. Skewed time measures were log-transformed using \(\log(x+1)\) when used as predictors. Categorical predictors were sum-coded (\texttt{contr.sum}) to support Type-III tests, and continuous predictors used in interaction models were mean-centered for interpretability.

\section{Statistical models}
\label{sec:data-analysis-models}

\paragraph{Repeated-measures approach.}
Because each participant contributed multiple blocks, trial-level outcomes are not independent. Primary analyses therefore used mixed-effects models with random intercepts for \texttt{participant\_id}. When appropriate, a random intercept for \texttt{article} was added to absorb systematic text-level differences.

\paragraph{AI-only Structure \(\times\) Timing tests.}
For each continuous dependent variable \(Y\) in the AI-assisted groups, the core model tested effects of summary \texttt{structure} and \texttt{timing}:
\[
  Y \sim \texttt{structure} \times \texttt{timing} + (1 \mid \texttt{participant\_id}) + (1 \mid \texttt{article}).
\]
When an interaction was supported, follow-up comparisons decomposed timing effects within each structure level using estimated marginal means.

\paragraph{AI vs No-AI comparisons.}
To compare AI-assisted reading to unaided reading under the asymmetric design, between-group models were fit on the long-format dataset with \texttt{experiment\_group} as the primary fixed effect, controlling for \texttt{article} and using a random intercept for \texttt{participant\_id}:
\[
  Y \sim \texttt{experiment\_group} + \texttt{article} + (1 \mid \texttt{participant\_id}).
\]
Moderation by prior knowledge was tested by adding the interaction term:
\[
  Y \sim \texttt{experiment\_group} \times \texttt{prior\_knowledge\_familiarity} + \texttt{article} + (1 \mid \texttt{participant\_id}).
\]

\paragraph{Mechanism-oriented models (summary accuracy as predictor).}
To test whether performance covaries with alignment to the AI representation, additional mixed models included \texttt{ai\_summary\_accuracy} while controlling for experimental factors:
\[
  Y \sim \texttt{ai\_summary\_accuracy} + \texttt{timing} + \texttt{structure} + (1 \mid \texttt{participant\_id}) + (1 \mid \texttt{article}).
\]
These models are interpreted as associational mechanism tests rather than causal mediation claims.

\paragraph{False-lure (misinformation) models.}
False-lure endorsement was analyzed using generalized mixed models. The primary specification treated lure endorsement as a binomial outcome across lure opportunities with a logit link:
\[
  \texttt{cbind(false\_lures\_selected, no\_lures)} \sim \texttt{structure} + \texttt{timing} + \texttt{ai\_summary\_accuracy} + \texttt{article\_accuracy} + (1 \mid \texttt{participant\_id}) + (1 \mid \texttt{article}).
\]
Sensitivity models treated \texttt{false\_lures\_selected} as a Poisson count, and expanded models added engagement-related predictors (e.g., \texttt{mental\_effort}, \texttt{reading\_time\_min}, \texttt{summary\_time\_sec}) to verify that structure effects do not reduce to time-on-task differences.

\paragraph{Moderation and calibration models.}
Moderation and reliance models were examined within the AI groups by testing whether block-level performance and engagement covary with \texttt{prior\_knowledge\_familiarity} and post-block (state) trust/dependence ratings, including exploratory \texttt{timing} \(\times\) predictor interactions while controlling for \texttt{structure}. Metacognitive calibration was evaluated by modelling recall quality as a function of confidence and group:
\[
  \texttt{recall\_total\_score} \sim \texttt{recall\_confidence} \times \texttt{experiment\_group} + \texttt{article} + (1 \mid \texttt{participant\_id}).
\]

\section{Inference, diagnostics, and robustness}
\label{sec:data-analysis-inference}

\paragraph{Estimation and fixed-effect tests.}
Linear mixed models were fit using restricted maximum likelihood (REML) for parameter estimation; when nested model comparisons were required, models were fit with maximum likelihood (ML). Fixed effects were evaluated using Type-III tests under sum coding; denominator degrees of freedom for \(F\)-tests were obtained via Satterthwaite approximation where applicable. For generalized models, fixed effects were evaluated using Wald/\(\chi^2\)-type tests as implemented by the modelling framework.

\paragraph{Multiplicity control and reporting conventions.}
All hypothesis tests were two-sided with \(\alpha = .05\). Post-hoc comparisons (e.g., pairwise timing contrasts; simple effects within structure; simple-slope contrasts) were corrected using the Holm procedure within each family of comparisons. Effect size metrics were pre-specified for reporting in the Results chapter (e.g., \(\eta^2\) variants for factorial effects; standardized mean differences for planned contrasts; and odds ratios/incidence-rate ratios for generalized models).

\paragraph{Model diagnostics and sensitivity checks.}
Model adequacy was assessed using residual diagnostics for linear models, convergence and random-effect variance checks for mixed models, and overdispersion/influence checks for generalized models. Robustness analyses verified that conclusions were not driven by idiosyncratic stimuli or modelling choices, including (i) counterbalancing verification across article--timing pairings, (ii) leave-one-article-out and leave-one-participant-out re-estimation, (iii) random-slope sensitivity where data support permitted, and (iv) distributional sensitivity for count outcomes (binomial vs Poisson). Robustness outputs are reported in Appendix~C.

\section{Missing data handling and exclusion criteria}
\label{sec:missingness-exclusions}
Missing data can arise from participant nonresponse (e.g., skipped questionnaire items), technical failures (e.g., incomplete log exports), or partial task completion (e.g., leaving the session before finishing all three article blocks). Because the experimental design relies on within-participant comparisons across three blocks (AI timing) and block-level outcomes derived from platform logs, the analysis required internally consistent long-format records with one row per participant \(\times\) article block and complete condition metadata.

\paragraph{Complete-case inclusion for primary analyses.}
The primary analysis dataset was restricted to participants with complete and coherent data across all required sources (questionnaires, recall responses/scores, MCQ responses, and timing/event logs). Participants with missing blocks (i.e., fewer than three valid article rows), mismatched identifiers, or incomplete outcome records were excluded prior to model fitting. As a result, the analyses reported in this thesis are based on the final retained sample described in \Cref{subsec:participants}, and can be interpreted as estimates conditional on successful completion of the protocol.

\paragraph{Rationale and general handling principles.}
In many empirical studies, missingness is addressed using approaches such as listwise deletion (complete-case analysis), pairwise deletion, single imputation, or model-based methods (e.g., multiple imputation) depending on the mechanism and extent of missingness. In the present study, listwise exclusion at the participant level was preferred for the primary analyses because (i) key variables are structurally linked across phases (e.g., timing assignment and log-derived exposure measures) and (ii) imputing missing blocks would require strong assumptions about unobserved reading, exposure, and test behavior. Where item-level missingness occurred within retained participants (e.g., occasional unanswered questionnaire items), analyses either used the available items for scale computation when permissible or omitted that predictor from models requiring it, so that primary outcome models remained based on observed performance and log-derived measures.

\paragraph{Robustness considerations.}
Because mixed-effects models can accommodate unbalanced data in principle, a more permissive approach could retain participants with partial blocks. However, given the small sample size and the design goal of comparing timing conditions within the same individuals, we prioritized a clean, fully observed repeated-measures dataset to reduce ambiguity about whether missingness reflected technical issues versus systematic dropout. Sensitivity checks and robustness analyses are reported alongside the main results (and in Appendix~C where applicable).

\chapter{Results}
\label{ch:results}

This chapter reports the empirical findings of Study~1, organized around the research questions and hypotheses introduced in Chapter~3 (\Cref{sec:research_questions,sec:hypotheses}). Analyses follow the mixed-design modelling strategy detailed in Chapter~5 (\Cref{ch:data-analysis}): within the AI group, \texttt{timing} is treated as a within-participant factor and \texttt{structure} as a between-participant factor, using mixed-effects models with random intercepts for \texttt{participant\_id} (and \texttt{article} where appropriate). Unless stated otherwise, all results use the final complete-case sample (\(N=36\); \(N_{\mathrm{AI}}=24\), \(N_{\mathrm{NoAI}}=12\)), with three article blocks per participant (108 block-level observations).

\section{Reporting conventions}
\label{sec:results-conventions}

For each outcome we report descriptive statistics (Mean \(\pm\) SD) followed by inferential tests. For Structure \(\times\) Timing tests within the AI group, effects are evaluated via Type-III \(F\)-tests from the mixed-effects models described in \Cref{sec:data-analysis-models}. Where post-hoc timing contrasts are reported, p-values are Holm-corrected within the corresponding family of comparisons (see \Cref{sec:data-analysis-inference}).

\section{RQ1: Does AI assistance improve memory relative to No-AI?}
\label{sec:results-rq1}

\textbf{Recognition performance (MCQ accuracy).}
\label{subsec:results-rq1-mcq}

Across all multiple-choice questions (MCQs), the AI group outperformed the No-AI group (AI: \(0.598 \pm 0.085\), No-AI: \(0.510 \pm 0.098\)). This corresponds to an absolute gain of \(0.088\) (8.8 percentage points) in recognition accuracy. A between-subjects model with \texttt{experiment\_group} indicates a reliable overall recognition benefit of AI assistance, \(F(1,34)=7.86\), \(p=.008\), \(\eta^2_p=.188\), Cohen’s \(d=0.98\). This establishes the baseline effect for \textbf{H1} and provides context for the within-AI timing and structure results reported next.

\begin{table}[H]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Group} & \textbf{Overall MCQ accuracy (Mean \(\pm\) SD)} & \textbf{\(N\)} \\
\midrule
AI-assisted & \(0.598 \pm 0.085\) & 24 \\
No-AI & \(0.510 \pm 0.098\) & 12 \\
\bottomrule
\end{tabular}
\caption{Overall MCQ accuracy by experiment group (all participants).}
\label{tab:results-mcq-by-group}
\end{table}

\textbf{Free recall performance.}
\label{subsec:results-rq1-recall}

In contrast to recognition, free recall quality shows no reliable improvement from AI assistance. The AI group’s mean recall score was \(5.535\) versus \(5.403\) in No-AI (difference \(=0.132\) points). A between-subjects mixed model with \texttt{experiment\_group} (controlling for \texttt{article}; random intercept for \texttt{participant\_id}) does not support a group effect on recall, \(F(1,34)=0.04\), \(p=.834\). Within the AI group, recall is also unaffected by the timing and structure manipulations (see \Cref{sec:results-recall-brief}).

\section{RQ2: Does timing of summary access change learning and reliance? (AI group)}
\label{sec:results-rq2}

\noindent RQ2 focuses on the mixed-design comparison \emph{within} the AI group: \texttt{timing} varies within participants across the three article blocks (pre-reading vs.\ synchronous vs.\ post-reading), while \texttt{structure} is between participants (integrated vs.\ segmented). Unless stated otherwise, the results below come from mixed-effects models that account for repeated measures within participants and variability across articles.

\textbf{Timing improves AI-summary-sourced recognition.}
\label{subsec:results-mf1}

To isolate learning that is specifically aligned with the AI summary, we analyze \textbf{AI-summary-sourced accuracy} (\texttt{ai\_summary\_accuracy}), defined as accuracy on the subset of MCQ items whose correct answers are covered by the summary content. Items were pre-tagged as AI-summary-covered during stimulus construction (Appendix~A; \Cref{tab:false_lure_mapping}).

\paragraph{Descriptives.}
Pre-reading produces the highest AI-summary-sourced accuracy (\(0.833 \pm 0.141\)), followed by post-reading (\(0.641 \pm 0.196\)) and synchronous access (\(0.568 \pm 0.239\)).

\begin{table}[H]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Timing condition} & \textbf{AI-summary-sourced accuracy (Mean \(\pm\) SD)} \\
\midrule
\textbf{Pre-reading} & \textbf{\(0.833 \pm 0.141\)} \\
Synchronous & \(0.568 \pm 0.239\) \\
Post-reading & \(0.641 \pm 0.196\) \\
\bottomrule
\end{tabular}
\caption{AI-summary-sourced accuracy by timing condition (AI group).}
\label{tab:results-ai-summary-acc-by-timing}
\end{table}

\paragraph{Mixed-design test (Structure \(\times\) Timing).}
Mixed-effects models show a strong timing main effect on \texttt{ai\_summary\_accuracy}, \(F(2,44)=14.00\), \(p=1.97\times 10^{-5}\), \(\eta^2_p=.389\). The structure main effect is not supported (\(F(1,22)=0.06\), \(p=.802\)), and the interaction is not supported (\(F(2,44)=0.61\), \(p=.547\)).

\paragraph{Post-hoc timing contrasts.}
Holm-corrected pairwise comparisons indicate that pre-reading outperforms both synchronous and post-reading access, while synchronous versus post-reading is not significant:

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Comparison} & \textbf{Difference} & \textbf{\(p\)} & \textbf{Cohen's \(d_z\)} \\
\midrule
Pre vs.\ Synchronous & \(+0.266\) & \(.00052\) & \(0.91\) \\
Pre vs.\ Post & \(+0.193\) & \(.00057\) & \(0.87\) \\
Synchronous vs.\ Post & \(-0.073\) & \(.148\) & \(-0.31\) \\
\bottomrule
\end{tabular}
\caption{Holm-corrected pairwise timing contrasts for AI-summary-sourced accuracy (AI group).}
\label{tab:results-ai-summary-acc-posthoc}
\end{table}

\noindent These contrasts show a large and reliable pre-reading advantage for summary-covered information, whereas synchronous and post-reading access do not differ reliably from each other.

\paragraph{Overall MCQ accuracy (context) and boundary condition.}
The same ordering appears for overall MCQ accuracy (all items): pre-reading \(0.699 \pm 0.125\), synchronous \(0.533 \pm 0.147\), post-reading \(0.562 \pm 0.127\), with a reliable timing effect \(F(2,42.18)=17.61\), \(p<.001\), \(\eta^2_p=.455\). Structure shows a modest main effect (\(F(1,22)=4.69\), \(p=.042\)), whereas the interaction is not supported (\(p=.670\)). However, timing does \textit{not} affect \texttt{article\_accuracy} (article-only items; \(p=.706\)), indicating that the timing manipulation primarily changes performance on summary-covered information rather than broadly improving comprehension of article-exclusive content.

\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{slide1_main_finding_1_ai_summary_accuracy.png}
        \vspace{2pt}
        \scriptsize (A) AI-summary-sourced accuracy by timing (AI group).
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{A1_plot_mcq_accuracy.png}
        \vspace{2pt}
        \scriptsize (B) Overall MCQ accuracy by structure \(\times\) timing (AI group).
    \end{minipage}

    \vspace{8pt}
    \begin{minipage}{0.78\linewidth}
        \centering
        \includegraphics[width=\linewidth]{A1_plot_article_accuracy.png}
        \vspace{2pt}
        \scriptsize (C) Article-only accuracy by structure \(\times\) timing (AI group).
    \end{minipage}

\caption{Timing effects are strongest for AI-summary-sourced learning, mirrored in overall MCQ accuracy, and absent for article-only accuracy. Error bars indicate \(\pm\)SE.}
    \label{fig:results-mf1}
\end{figure}

\textbf{Mechanism-oriented robustness checks for the timing advantage.}
\label{subsec:results-mf1-mechanism}

To test whether the timing advantage is explained by summary-aligned encoding and/or greater exposure to the summary, we report two complementary model-based checks (see \Cref{sec:data-analysis-models}).

\paragraph{Summary-alignment mechanism (adding \texttt{ai\_summary\_accuracy}).}
\texttt{ai\_summary\_accuracy} strongly predicts overall MCQ accuracy in a mixed model controlling for timing and structure (\(\beta=0.472\), \(p<.001\)). When \texttt{ai\_summary\_accuracy} is added to the timing model, the pre-reading advantage in overall MCQ accuracy shrinks substantially (pre--sync: \(\Delta=0.172 \rightarrow 0.043\); pre--post: \(\Delta=0.143 \rightarrow 0.044\)) and is no longer significant after Holm correction (\(p=.352\)). Model fit improves markedly (\(\chi^2(1)=46.38\), \(p<.001\)).

\paragraph{Exposure control (adding summary time).}
Controlling for log-transformed \texttt{summary\_time\_sec} reduces the timing coefficients only modestly (Pre--Sync: \(0.271 \rightarrow 0.258\); Pre--Post: \(0.214 \rightarrow 0.164\)), and the timing contrasts remain significant. Summary time itself is a positive predictor of AI-summary-sourced accuracy (\(\beta=0.062\), \(p=.031\)).

\begin{table}[H]
\centering
\makebox[\textwidth][c]{%
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Pre--Sync estimate} & \textbf{Pre--Post estimate} \\
\midrule
Base (timing only) & 0.271*** & 0.214*** \\
\(+\) log(\texttt{summary\_time\_sec}) & 0.258*** & 0.164** \\
\textbf{Reduction} & \textbf{5\%} & \textbf{23\%} \\
\bottomrule
\end{tabular}}
\caption{Timing contrasts on AI-summary-sourced accuracy with and without controlling for summary exposure.}
\label{tab:results-timing-contrast-with-summary-time}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.92\linewidth]{timing_decomposition.png}
\caption{Mechanism check: the overall MCQ timing coefficients shrink when summary-alignment and exposure variables are added (decomposition plot).}
    \label{fig:results-mf1-decomposition}
\end{figure}

\textbf{Timing redistributes attention toward the summary (process evidence).}
\label{subsec:results-mf3}

Behavioral logs provide a process-level trace of how participants allocated attention between the article and the summary. Pre-reading produced the greatest summary viewing time and summary share, synchronous was intermediate, and post-reading was lowest. Total time-on-task was stable across timing conditions, indicating a redistribution of attention rather than increased overall engagement.

\begin{table}[H]
\centering
\small
{\setlength{\tabcolsep}{5pt}
\begin{tabular}{lrrrr}
\toprule
\textbf{Timing} & \textbf{Summary time (s)} & \textbf{Reading time (min)} & \textbf{Total time (s)} & \textbf{Summary share (\%)} \\
\midrule
Pre-reading & 132.5 & 6.72 & 535.8 & 24.9 \\
Synchronous & 100.3 & 7.19 & 531.6 & 19.5 \\
Post-reading & 69.5 & 7.69 & 530.8 & 13.7 \\
\bottomrule
\end{tabular}}
\caption{Time allocation by timing (AI group; means). Summary share (\%) = \texttt{summary\_time\_sec} / (\texttt{summary\_time\_sec} + \texttt{reading\_time\_min}\(\times\)60) \(\times 100\).}
\label{tab:results-time-allocation-by-timing}
\end{table}

\paragraph{Mixed-design test (Structure \(\times\) Timing).}
\begin{itemize}
  \item \textbf{Summary time (s):} main effect of timing, \(F(2,43.25)=13.32\), \(p<.001\), \(\eta^2_p=.381\); no structure effect and no interaction.
  \item \textbf{Summary share:} main effect of timing, \(F(2,43.25)=16.20\), \(p<.001\), \(\eta^2_p=.428\); no structure effect and no interaction.
  \item \textbf{Reading time (min):} no timing effect, \(F(2,43.25)=1.16\), \(p=.324\); no structure effect and no interaction.
\end{itemize}

\paragraph{Post-hoc timing contrasts.}
Pre-reading \(>\) synchronous \(>\) post-reading for both summary time and summary share (all Holm-adjusted \(p \le .021\)). Reading time contrasts are not consistently different: only pre-reading versus synchronous differs (Holm-adjusted \(p=.023\)), while the other pairwise comparisons are not significant (Holm-adjusted \(p=.347\)).

\begin{table}[H]
\centering
\small
{\setlength{\tabcolsep}{6pt}
\begin{tabular}{lccc}
\toprule
\multicolumn{4}{l}{\textbf{Panel A. Reading time (min)}} \\
\textbf{Structure} & \textbf{Pre-reading} & \textbf{Synchronous} & \textbf{Post-reading} \\
\midrule
Integrated & 7.34 & 7.45 & 7.98 \\
Segmented & 6.11 & 6.93 & 7.40 \\
\addlinespace[0.3em]
\multicolumn{4}{l}{\textbf{Panel B. Summary share (\%)}} \\
\textbf{Structure} & \textbf{Pre-reading} & \textbf{Synchronous} & \textbf{Post-reading} \\
\midrule
Integrated & 24.6 & 19.0 & 14.1 \\
Segmented & 25.1 & 20.0 & 13.2 \\
\bottomrule
\end{tabular}}
\caption{Structure \(\times\) timing descriptives for reading time and summary share (AI group).}
\label{tab:results-time-structure-by-timing}
\end{table}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{A1_plot_summary_time_sec.png}
        \vspace{2pt}
        \scriptsize (A) Summary viewing time by structure \(\times\) timing (AI only).
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{A1_plot_summary_prop.png}
        \vspace{2pt}
        \scriptsize (B) Summary share of total time by structure \(\times\) timing (AI only).
    \end{minipage}
    \caption{Pre-reading increases summary exposure (absolute and relative) compared to synchronous and post-reading access. Error bars indicate \(\pm\)SE.}
    \label{fig:results-mf3-summary-exposure}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{A1_plot_reading_time_min.png}
        \vspace{2pt}
        \scriptsize (A) Article reading time by structure \(\times\) timing (AI only).
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{A1_plot_total_time_sec.png}
        \vspace{2pt}
        \scriptsize (B) Total time by structure \(\times\) timing (AI only).
    \end{minipage}
    \caption{Timing redistributes attention toward the summary without increasing overall time-on-task. Error bars indicate \(\pm\)SE.}
    \label{fig:results-mf3-total-time}
\end{figure}

\section{RQ3: Does summary structure affect misinformation susceptibility? (AI group)}
\label{sec:results-rq3}

\noindent RQ3 tests whether \texttt{structure} changes susceptibility to misinformation under AI assistance. We operationalize epistemic risk using false lures: plausible but incorrect statements included in each summary (two per article). Endorsing these lures in the MCQ block indicates that misleading summary content was accepted as true and incorporated into subsequent recognition judgments.

\textbf{Integrated summaries reduce false-lure endorsement.}
\label{subsec:results-mf2}

To assess misinformation susceptibility and source-monitoring errors under AI assistance, each AI summary contained two plausible but incorrect statements (false lures; see \Cref{subsec:materials}). False-lure endorsement was operationalized as (i) the number of false lures selected in the MCQ block (0--2 per article) and (ii) false-lure accuracy (proportion of lures correctly rejected; higher is better).

\paragraph{Descriptives.}
Segmented summaries lead to more false lures selected than integrated summaries (segmented: \(1.06 \pm 0.79\); integrated: \(0.58 \pm 0.69\)), corresponding to an average increase of \(0.48\) additional lures per article block. Expressed as endorsement probability (out of two lure opportunities per block), this corresponds to \(53\%\) in segmented versus \(29\%\) in integrated. The converging false-lure accuracy measure shows the same pattern (integrated: \(0.556 \pm 0.354\); segmented: \(0.375 \pm 0.302\)).

\begin{table}[H]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Structure} & \textbf{False lures selected (0--2; Mean \(\pm\) SD)} & \textbf{Endorsement probability} \\
\midrule
Integrated & \(0.58 \pm 0.69\) & 29\% \\
\textbf{Segmented} & \textbf{\(1.06 \pm 0.79\)} & \textbf{53\%} \\
\bottomrule
\end{tabular}
\caption{False-lure endorsement by summary structure (AI group).}
\label{tab:results-false-lures-by-structure}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Structure} & \textbf{False-lure accuracy (Mean \(\pm\) SD)} \\
\midrule
\textbf{Integrated} & \textbf{\(0.556 \pm 0.354\)} \\
Segmented & \(0.375 \pm 0.302\) \\
\bottomrule
\end{tabular}
\caption{False-lure accuracy (correct rejections) by structure (AI group).}
\label{tab:results-false-lure-accuracy-by-structure}
\end{table}

\paragraph{Mixed-design test (Structure \(\times\) Timing).}
For false lures selected, there is a structure main effect, \(F(1,22)=4.74\), \(p=.041\); timing and interaction are not supported. For false-lure accuracy, the structure effect is similar in direction and magnitude but marginal at \(\alpha=.05\), \(F(1,22)=4.20\), \(p=.053\); timing and interaction are not supported.

\paragraph{Generalized model (binomial GLMM).}
A binomial mixed model of endorsement probability corroborates the structure effect: segmented structure increases the odds of endorsing false lures by a factor of 5.93 (95\% CI [1.63, 21.5], \(p=.007\)), while timing terms are not significant.

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Predictor} & \textbf{Odds ratio} & \textbf{95\% CI} & \textbf{\(p\)} \\
\midrule
\textbf{Structure (Segmented)} & \textbf{5.93} & [1.63, 21.5] & .007 \\
Timing (Synchronous) & 0.46 & [0.12, 1.73] & .251 \\
Timing (Post-reading) & 0.67 & [0.18, 2.45] & .544 \\
\bottomrule
\end{tabular}
\caption{Binomial GLMM for false-lure endorsement (AI group).}
\label{tab:results-false-lure-glmm}
\end{table}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{A1_plot_false_lure_accuracy.png}
        \vspace{2pt}
        \scriptsize (A) False-lure accuracy by structure \(\times\) timing (AI only).
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{ORD_plot2_lure_prob_by_structure.png}
        \vspace{2pt}
        \scriptsize (B) Predicted false-lure probability by summary accuracy and structure (model-based).
    \end{minipage}
    \caption{Integrated presentation reduces misinformation risk; segmented format increases false-lure endorsement.}
    \label{fig:results-mf2}
\end{figure}

\section{RQ4: How do trust and dependence vary across conditions? (AI group)}
\label{sec:results-rq4}

\noindent RQ4 examines participants' subjective experience of AI assistance at the block level. Because trust and dependence were measured after each article block (post-block state ratings), these outcomes can vary across timing conditions within the same person, and can be directly compared to behavioral reliance measures such as summary viewing time and summary share.

\textbf{Secondary finding: post-block trust and dependence track timing (and dependence is higher in segmented).}
\label{subsec:results-secondary-trust-dep}

In addition to baseline (general) trust/dependence measures, AI-group participants provided post-block (state) trust and dependence ratings after each article block (\texttt{trust\_new}, \texttt{dependence\_new}; see \Cref{subsec:procedure}). These repeated ratings were analyzed as block-level outcomes to assess whether subjective reliance varies systematically with the timing and structure manipulations.

\paragraph{Mixed-design test (Structure \(\times\) Timing).}
Trust varies reliably by timing (\(F(2,43)=7.90\), \(p=.001\)), but does not show a reliable structure effect (\(p=.222\)) and does not show a reliable interaction (\(p=.194\)). Dependence shows both a timing effect (\(F(2,43)=7.74\), \(p=.001\)) and a structure effect (\(F(1,22)=6.21\), \(p=.021\)), with no interaction (\(p=.559\)). Post-hoc contrasts show that pre-reading is higher than synchronous and post-reading for dependence (Holm-corrected), whereas for trust the timing main effect is reliable but the pairwise contrasts are weaker after correction. Synchronous versus post-reading is not significant for either measure.
Full Holm-corrected post-hoc tables are reported in \Cref{app:stats} (\Cref{tab:A1_posthoc_timing_ai_trust,tab:A1_posthoc_timing_ai_dependence}).

\noindent Descriptively, pre-reading produces the highest trust and dependence values in both structures, and segmented conditions show consistently higher dependence than integrated conditions (see \Cref{tab:results-trust-dep-means}).

\begin{table}[H]
\centering
\begin{tabular}{llcc}
\toprule
\textbf{Structure} & \textbf{Timing} & \textbf{Trust} & \textbf{Dependence} \\
\midrule
Integrated & Pre-reading & 4.17 & 4.25 \\
Integrated & Synchronous & 3.58 & 3.83 \\
Integrated & Post-reading & 3.92 & 3.67 \\
Segmented & Pre-reading & 4.83 & 5.33 \\
Segmented & Synchronous & 4.17 & 4.50 \\
Segmented & Post-reading & 4.00 & 4.58 \\
\bottomrule
\end{tabular}
\caption{Post-block trust and dependence ratings by AI assistance condition (means).}
\label{tab:results-trust-dep-means}
\end{table}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{A1_plot_ai_trust.png}
        \vspace{2pt}
        \scriptsize (A) Trust by structure \(\times\) timing (AI group).
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{A1_plot_ai_dependence.png}
        \vspace{2pt}
        \scriptsize (B) Dependence by structure \(\times\) timing (AI group).
    \end{minipage}
    \hfill
    \begin{minipage}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{A1_plot_summary_prop.png}
        \vspace{2pt}
        \scriptsize (C) Summary share by structure \(\times\) timing (AI group).
    \end{minipage}
    \caption{Converging evidence for pre-reading reliance: post-block trust, post-block dependence, and summary share by structure and timing (AI group). Error bars indicate \(\pm\)SE.}
    \label{fig:results-secondary-trust-dep}
\end{figure}

\section{Free recall results}
\label{sec:results-recall-brief}

Free recall is included as a complementary memory outcome capturing generative retrieval (\Cref{subsec:measures}). In contrast to the robust timing effects observed for recognition, recall quality shows no reliable effects of timing or structure within the AI group. The three timing means are nearly identical (all \(\approx 5.5\) points), indicating that the timing manipulation does not meaningfully change participants' ability to reproduce article content without cues.

\begin{table}[H]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Timing} & \textbf{Recall total score (Mean \(\pm\) SD)} \\
\midrule
Pre-reading & \(5.50 \pm 1.92\) \\
Synchronous & \(5.54 \pm 1.99\) \\
Post-reading & \(5.56 \pm 2.17\) \\
\bottomrule
\end{tabular}
\caption{Recall scores by timing (AI group).}
\label{tab:results-recall-by-timing}
\end{table}

\noindent Mixed-design tests confirm the absence of effects: timing \(F(1.86,40.88)=0.03\), \(p=.969\), \(\eta^2_G<.001\); structure \(F(1,22)=0.40\), \(p=.536\); interaction \(F(1.86,40.88)=0.62\), \(p=.532\).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.78\linewidth]{A1_plot_recall_total_score.png}
    \caption{Recall total score by structure \(\times\) timing (AI group). Error bars indicate \(\pm\)SE.}
    \label{fig:results-recall-plot}
\end{figure}

\section{Robustness and design checks}
\label{sec:results-robustness}

Because the mixed design relies on within-participant timing comparisons, it is important to confirm that timing effects are not artifacts of the specific article assignments. Two checks support this: (i) the three articles differ in overall difficulty, but (ii) article identity is counterbalanced across timing orders. A chi-square test indicates that article assignment is independent of timing (\(\chi^2(4)=3.00\), \(p=.558\)). In addition, timing \(\times\) article interactions are not supported for summary accuracy (\(F(4,59.8)=1.88\), \(p=.125\)) or overall MCQ accuracy (\(F(4,60.5)=1.34\), \(p=.264\)).

\noindent Finally, the leave-one-article-out (LOAO) robustness check shows that the pre-reading advantage remains positive and significant when excluding each article in turn (\Cref{fig:results-robustness-loao}), indicating that the timing effect is not driven by a single stimulus topic.

\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{ORD_plot3_article_difficulty.png}
        \vspace{2pt}
        \scriptsize (A) Article difficulty gradient (overall MCQ accuracy; all participants).
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{EXP_fig_counterbalancing_timing_by_article.png}
        \vspace{2pt}
        \scriptsize (B) Timing \(\times\) article counterbalancing distribution (AI group).
    \end{minipage}
    \caption{Design checks: materials differ in difficulty, but timing is counterbalanced across articles.}
    \label{fig:results-robustness-design}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.86\linewidth]{slide4_robustness_leave_one_article_out.png}
    \caption{Leave-one-article-out robustness: the pre-reading advantage remains positive and significant when each article is excluded in turn.}
    \label{fig:results-robustness-loao}
\end{figure}

\section{Chapter summary}
\label{sec:results-summary}

Taken together, the results support three primary conclusions: (i) AI assistance improves recognition performance relative to No-AI, (ii) within AI assistance, timing strongly shapes learning for summary-covered information (pre-reading \(>\) synchronous/post-reading) and changes attention allocation toward the summary without increasing total time-on-task, and (iii) summary structure is the dominant driver of misinformation susceptibility, with segmented summaries increasing false-lure endorsement. Post-block reliance ratings (especially dependence) are consistent with pre-reading conditions being experienced as more reliance-inducing, while also indicating that the structure-driven misinformation effect should not be reduced to a simple ``over-trust'' account with these measures. The implications of these patterns are developed in the Discussion chapter.

\chapter{Discussion}
\label{ch:discussion}

This chapter interprets the findings reported in \Cref{ch:results} in relation to the research questions (\Cref{sec:research_questions}) and hypotheses (\Cref{sec:hypotheses}) derived from the \emph{proposed} AI Buffer model (\Cref{sec:framework-to-hypotheses}). The aim is to turn results into meaning: to explain what the findings suggest about \emph{how} AI-assisted reading---operationalized here as AI summaries---influences encoding, reliance, and memory distortion, how these effects map onto established theory, and what the implications are for both cognitive research and AI interface design.

\section{Overview of findings}
\label{sec:discussion-overview}

The results indicate that AI assistance does not produce a uniform ``memory improvement.'' Instead, outcomes depend on \emph{when} the summary is provided (timing) and \emph{how} it is presented (structure). Four high-level patterns frame the discussion.

First, AI assistance improves recognition performance relative to No-AI (overall MCQ accuracy: AI \(M=0.598\) vs.\ No-AI \(M=0.510\); \Cref{subsec:results-rq1-mcq}), while free recall quality shows no reliable improvement (\Cref{subsec:results-rq1-recall,sec:results-recall-brief}). Second, within the AI group, pre-reading access yields the strongest learning for summary-covered information (AI-summary-sourced accuracy: pre \(M=0.833\) vs.\ synchronous \(M=0.568\) vs.\ post \(M=0.641\); \Cref{subsec:results-mf1}) and shifts attention toward the summary without increasing overall time-on-task (\Cref{subsec:results-mf3}). Importantly, timing does not affect article-only accuracy, indicating that the strongest gains are specific to summary-covered information rather than broad improvements in article-only comprehension. In addition, timing effects on summary-sourced accuracy remain when controlling for summary viewing time, consistent with an effect that is not fully explained by differential exposure. Third, structure primarily affects epistemic risk: segmented summaries substantially increase false-lure endorsement relative to integrated summaries (binomial GLMM odds ratio \(=5.93\); \Cref{subsec:results-mf2}), whereas timing shows little impact on misinformation susceptibility. Fourth, post-block reliance ratings track timing (most robustly for dependence), and dependence is higher under segmented structure, consistent with differences in perceived reliance (\Cref{subsec:results-secondary-trust-dep}). Overall, the pattern aligns closely with the theoretical motivations in the literature on advance organizers, cognitive load, and source monitoring (\Cref{subsec:theories-timing-structure,subsec:distortion-risk}).

\section{Interpretation by research questions}
\label{sec:discussion-interpretation}

\noindent\textbf{RQ1 / H1: AI improves recognition more than free recall.}\par
\textbf{Recap.} Consistent with \textbf{H1}, the AI group shows higher overall MCQ accuracy than the No-AI group (AI \(M=0.598\) vs.\ No-AI \(M=0.510\); \(F(1,34)=7.86\), \(p=.008\); \Cref{subsec:results-rq1-mcq}), while free recall does not show a reliable improvement (\Cref{subsec:results-rq1-recall}).

\textbf{Interpretation: cue-based benefits and the limits of generative retrieval.} One interpretation is that AI summaries primarily support \emph{cue-dependent} learning and retrieval. Recognition tests can be solved using partial knowledge and familiarity-based cues, especially when items align with propositions highlighted in the summary. This is consistent with the proposed AI Buffer model: an external representation can increase accessibility for certain propositions without necessarily deepening the internally stored, richly interconnected representation needed for unguided recall.

In contrast, free recall places greater demands on generative retrieval (self-cueing, reconstructive search, and organization). If the summary encourages a more gist-oriented encoding strategy or reduces pressure to maintain details internally (cognitive offloading; \citep{Sparrow2011GoogleEffect,Risko2016CognitiveOffloading}), then recognition may improve while free recall remains unchanged. This pattern also aligns with classic distinctions between deeper semantic processing versus shallow exposure (\citep{Craik1972LevelsOfProcessing}): the presence of a summary can guide what is remembered, but it does not guarantee the elaborative integration that supports robust recall.

\textbf{Why recall may not shift (three plausible reasons).} Three non-exclusive factors may account for the null recall effects in this design:
\begin{itemize}
  \item \textbf{Task mismatch:} summaries may benefit cue-dependent recognition more directly than unaided generative recall.
  \item \textbf{Encoding depth:} the summary may increase accessibility for selected propositions without strengthening a coherent internal situation model.
  \item \textbf{Measurement constraints:} time-limited, distortion-penalizing rubric scoring may be less sensitive to selective accessibility gains.
\end{itemize}

\textbf{Implication.} In applied contexts, improved performance on cue-based tasks (e.g., targeted questions, verification, selection among options) should not be interpreted as evidence that AI summaries strengthen memory in a broad, recall-like sense. The data instead suggest that AI assistance is more likely to shift what becomes accessible under cueing than to increase unaided recall capacity.

\noindent\textbf{RQ2 / H2: Why pre-reading timing is most effective.}\par
\textbf{Recap.} \textbf{H2} is supported: pre-reading access yields the highest AI-summary-sourced accuracy (pre \(M=0.833\) vs.\ synchronous \(M=0.568\) vs.\ post \(M=0.641\); \Cref{subsec:results-mf1}) and increases summary engagement (time and share) without increasing total time (\Cref{subsec:results-mf3}). Decomposition checks are compatible with the interpretation that timing effects on overall MCQ accuracy are largely \emph{summary-aligned}, in the sense that the timing coefficients shrink substantially when \texttt{ai\_summary\_accuracy} is included (\Cref{fig:results-mf1-decomposition}).

\textbf{Advance-organizer account.} The timing pattern is consistent with advance-organizer theory (\citep{Ausubel1960AdvanceOrganizers,Ausubel1968EducationalPsychology}) and related evidence that pre-reading scaffolds improve comprehension and recognition for expository texts (\citep{MayerBromage1980AdvanceOrganizersRecall,HartleyDavies1976PreinstructionalStrategies}). When presented before reading, the summary can function as a schema-like framework that guides attention, highlights key relations, and supports coherent encoding of subsequent details. This would be expected to benefit summary-covered propositions in particular, which is the aspect targeted by \texttt{ai\_summary\_accuracy}.

\textbf{Why synchronous and post-reading are weaker.} Post-reading exposure arrives too late to guide initial encoding and may function as a review rather than an organizer. Synchronous exposure, while potentially helpful, introduces interruption and switching demands that may disrupt coherence building and increase extraneous load (\citep{Sweller1988CognitiveLoad,ChandlerSweller1992SplitAttention}). In this design, the data suggest that any benefits of mid-reading access are smaller than the advantages of having a conceptual scaffold before encoding begins.

\textbf{Process interpretation: timing redistributes attention.} The behavioral logs indicate that timing changes \emph{allocation} rather than \emph{total engagement}: summary time and summary share are highest in pre-reading, while total time is stable (\Cref{subsec:results-mf3}). This pattern is compatible with an account in which pre-reading timing positions the buffer as an organizing resource at the point of encoding, rather than simply increasing overall effort or exposure.

\textbf{Implication.} From a design perspective, if the objective is to improve learning of the key propositions an AI summary provides, the summary should be available \emph{before} reading, so it can operate as an organizer and shape encoding rather than acting as an after-the-fact recap.

\noindent\textbf{RQ3 / H3--H4: Structure affects epistemic risk more than memory quantity.}\par
\textbf{Recap.} \textbf{H4} is supported: segmented summaries increase false-lure endorsement relative to integrated summaries (odds ratio \(=5.93\); \Cref{subsec:results-mf2}). By contrast, the broader expectation in \textbf{H3} that integrated structure would improve recall quality and article-dependent outcomes is not strongly supported by the present results.

\textbf{Source-monitoring account of the structure effect.} The structure manipulation likely changes the \emph{conditions for source discrimination and evaluation}. In the integrated condition (paragraph summary), information is presented with richer within-summary discourse context and coherence cues, which may encourage relational integration of propositions and make it easier to evaluate statements in context. In contrast, segmented bullet points encourage item-by-item processing and may increase familiarity for plausible statements without preserving equally rich contextual features that help later source judgments (``was this in the article, or only in the summary?''). This interpretation aligns with the Source Monitoring Framework (\citep{Johnson1993SourceMonitoring}) and with distortion-risk accounts in which gist-consistent familiarity can increase false acceptance when source cues are weak (\Cref{subsec:distortion-risk}).

\textbf{Why timing matters less for misinformation here.} In this study, the lures are embedded in the summary itself, so the key risk factor is how the summary is processed and differentiated from the article—precisely what structure targets. The absence of timing effects suggests that, once misinformation is encountered, format-level source cues and verification affordances are more important than whether the summary appeared before or after reading.

Because lures were deliberately embedded to isolate source-monitoring vulnerability, this structure effect should be interpreted as a mechanism probe rather than a direct estimate of real-world hallucination rates.

\textbf{Boundary: not a simple ``over-trust'' explanation.} One might hypothesize that segmented summaries mislead because they elicit greater trust. However, trust does not reliably differ by structure with the post-block state measures, even though dependence does (\Cref{subsec:results-secondary-trust-dep}). This supports a more specific interpretation: structure affects misinformation primarily through source-monitoring conditions rather than through inflated credibility beliefs.

\textbf{Implication.} When the task requires accurate attribution to the original source (e.g., studying, professional decision-making, or writing), segmented bullet-point summaries may create higher epistemic risk than integrated formats. This suggests that interface design choices can meaningfully shift misinformation vulnerability even when learning benefits are held constant.

\noindent\textbf{RQ4 / H5: Trust and dependence as condition-dependent reliance signals.}\par
\textbf{Recap.} Post-block state ratings show that dependence is highest in the pre-reading condition (with robust pre-reading pairwise contrasts for dependence after Holm correction) and that dependence is higher under segmented structure; trust shows an overall timing effect but pairwise differences are less robust after correction (\Cref{subsec:results-secondary-trust-dep}). These measures were administered after each article block, making them sensitive to condition-specific experience.

\textbf{Interpretation: perceived reliance tracks timing and (partly) structure.} The alignment between pre-reading timing, higher summary engagement (\Cref{subsec:results-mf3}), and higher dependence supports a coherent interpretation: when the summary is available early, participants experience it as a stronger scaffold and report greater reliance on it. The segmented dependence effect adds nuance: users may feel more reliant on bullet-point summaries even when they do not report higher trust. One interpretation is that segmented lists encourage a checklist-like offloading strategy that increases perceived reliance independent of perceived credibility.

\textbf{Implication.} Methodologically, repeated post-block ratings complement baseline trait-like measures by capturing how design choices shape reliance \emph{in situ}. Substantively, the dissociation between structure effects on dependence versus trust suggests that reliance is not reducible to perceived credibility alone, reinforcing the value of measuring both constructs in AI-assisted cognition studies.

\noindent\textbf{Boundary conditions and alternative interpretations.}\par

Several null or selective effects clarify what the manipulations do \emph{not} change in this paradigm. Article-only accuracy does not show reliable timing effects, suggesting that the strongest learning gains occur for summary-covered propositions rather than for knowledge that depends primarily on article text. Recall quality is stable across timing and structure (\Cref{sec:results-recall-brief}), indicating that the manipulations do not substantially alter the depth of encoding required for unaided generative retrieval. Finally, timing changes summary exposure without increasing total time (\Cref{subsec:results-mf3}), and timing effects on summary-sourced accuracy remain when controlling for summary viewing time, suggesting that improved learning is not explained solely by increased time-on-task or summary exposure.

An alternative explanation is that the summary primarily changes perceived difficulty rather than encoding strategy. However, the combination of (i) summary-aligned learning effects (\Cref{subsec:results-mf1}), (ii) stable article reading time, and (iii) stable recall is more consistent with a targeted scaffolding/cue account than with a general ``ease'' account. Future work can separate these interpretations more cleanly using delayed retention, richer comprehension measures, and direct indices of coherence-building.

\section{Implications for theory and design}
\label{sec:discussion-implications}

\textbf{Implications for the AI Buffer model.} Overall, the results provide initial support for a design-sensitive view consistent with the proposed AI Buffer model. The pattern suggests that timing differences are most compatible with changes along an \textbf{organizer/cue pathway}: pre-reading access increases engagement with the buffer and improves accessibility of summary-covered propositions. Conversely, structure differences are most compatible with changes along a \textbf{source-monitoring pathway}: integrated presentation reduces false-lure endorsement relative to segmented presentation. This dissociation is theoretically useful because it implies that ``AI assistance'' is not a unitary intervention; rather, interface choices can map onto distinct cognitive benefits and vulnerabilities.

\textbf{Design implications.}

The findings suggest several actionable design principles:
\begin{itemize}
  \item \textbf{Use summaries as previews when learning is the goal.} Pre-reading access best supports summary-covered learning, consistent with advance-organizer logic.
  \item \textbf{Prefer integrated formats when source discrimination matters.} Integrated (paragraph) presentation reduces false-lure endorsement, consistent with the idea that richer coherence cues and more contextual evaluation reduce familiarity-driven acceptance of plausible misinformation.
  \item \textbf{Add provenance and verification affordances.} Especially when using segmented summaries or in higher-stakes settings, provide cues that link summary claims to source passages (e.g., inline citations, ``show in text'' links, or verification prompts) to strengthen source monitoring.
  \item \textbf{Design for calibrated reliance.} Because dependence can increase without higher trust, interfaces should support appropriate reliance while preserving cues that the summary is a distinct (and fallible) source.
\end{itemize}

More broadly, evaluation of AI reading aids should jointly consider performance outcomes and epistemic-risk outcomes. A tool can improve recognition performance while simultaneously increasing vulnerability to misinformation if its format weakens source monitoring.
This underscores the need to evaluate AI aids as tradeoffs between performance and epistemic safety, not as a single ``help vs.\ harm'' effect.

\section{Limitations and future work}
\label{sec:discussion-limitations}

Several limitations should be considered. First, the sample size (especially for interaction and moderation tests) limits power to detect small effects, so null interactions should be interpreted cautiously. Second, only three articles were used; although robustness checks support generalization across these stimuli (\Cref{sec:results-robustness}), broader generalization to other genres and domains remains uncertain. Third, the study uses a controlled reading protocol with time limits and restricted navigation to support internal validity; naturalistic reading involves different patterns of switching, revisiting, and self-paced verification that could change both learning and misinformation dynamics.

Fourth, false lures were inserted deliberately into summaries to test source-monitoring vulnerability. This is appropriate for isolating epistemic risk, but real-world AI errors can be subtler and may interact with user expertise differently. Finally, trust and dependence were measured via self-report. Post-block state ratings are well-suited to capturing condition-dependent experience, but they do not directly measure calibration accuracy (whether trust matches objective summary quality) and may be sensitive to demand characteristics.

\textbf{Future research.} The present findings motivate several extensions. A first priority is delayed retention to test whether timing and structure effects persist after consolidation. A second direction is to broaden stimuli (more texts, domains, and a wider range of summary quality) and include outcomes that capture transfer and deep comprehension. Third, future experiments could manipulate interface cues that directly target source monitoring (e.g., explicit provenance labels, inline verification prompts, or training in critical evaluation) to test whether the structure-driven misinformation effect can be mitigated without sacrificing learning benefits. Finally, larger samples would enable stronger tests of individual differences (prior knowledge, baseline trust/dependence) and their role in shaping reliance and vulnerability.

\section{Summary}
\label{sec:discussion-summary}

In sum, Study~1 provides initial support for a design-sensitive view of AI assistance. AI summaries improve recognition, but the strongest gains are specific to summary-covered propositions and depend on pre-reading access that allows the summary to function as an advance organizer. At the same time, summary structure has a distinct effect on epistemic risk: segmented summaries increase false-lure endorsement relative to integrated summaries. Post-block reliance ratings (especially dependence) are consistent with pre-reading conditions being experienced as more reliance-inducing, while also indicating that misinformation effects are better explained by source-monitoring conditions than by a simple over-trust account. These conclusions motivate both theoretical refinements of the AI Buffer model and concrete recommendations for safer, more effective AI-assisted reading interfaces.

\chapter{Conclusion}
\label{ch:conclusion}

This chapter closes the thesis by summarizing the main contributions and findings, clarifying
their implications for the AI Buffer model and for the design of AI-assisted reading
interfaces, and outlining the main boundaries of the present study and directions for future
work.

\section{Summary of Contributions}
\label{sec:summary_contributions}

This thesis examined how AI-generated summaries influence learning and memory for expository
texts, and whether these effects depend on \emph{when} the summary is available (pre-reading vs.\
synchronous vs.\ post-reading) and \emph{how} it is presented (integrated paragraph vs.\ segmented
bullets). To test these questions, Study~1 implemented a controlled reading-and-memory paradigm
with a \(2 \times 3\) mixed factorial design within the AI condition (Structure between
participants; Timing within participants across three article blocks) and a No-AI baseline
group for external comparison.

Across conditions, outcomes captured complementary aspects of memory and reliance: recognition
performance (overall MCQ accuracy and AI-summary-sourced accuracy), free recall quality (rubric
scoring), behavioral engagement derived from server logs (reading time, summary exposure, and
summary share), subjective experience (post-block trust and dependence), and epistemic risk
indexed by endorsement of false lures embedded in the summaries.

The thesis contributes at three levels:

\begin{itemize}
  \item \textbf{Conceptual contribution.} The \emph{AI Buffer} model, which characterizes
  AI summaries as persistent external representations that can (i) support cue-based learning
  and retrieval, while also (ii) creating source-monitoring vulnerabilities when users encode
  or retrieve information under weak provenance cues.

  \item \textbf{Methodological contribution.} A controlled summary-assistance paradigm that
  independently manipulates timing and structure, combines performance measures with
  fine-grained behavioral logging, and uses designed false lures to quantify misinformation
  susceptibility under AI assistance.

  \item \textbf{Empirical contribution.} Evidence that timing and structure have dissociable
  cognitive consequences: timing primarily shapes summary-aligned recognition benefits, whereas
  structure primarily shapes misinformation vulnerability.
\end{itemize}

\section{Key Takeaways}
\label{sec:key_takeaways}

Across the research questions, four findings are most consequential.

\begin{itemize}
  \item \textbf{AI vs.\ No-AI.} AI assistance improves recognition performance (overall MCQ
  accuracy) relative to unaided reading, but does not reliably improve free recall quality.
  This pattern is consistent with a cue-based benefit pathway in which the summary increases
  accessibility for highlighted propositions without necessarily strengthening the internally
  generated structure required for unguided, generative recall.

  \item \textbf{Timing.} Pre-reading access yields the strongest learning for information
  covered by the summary (AI-summary-sourced accuracy). Behavioral logs indicate that timing
  primarily redistributes attention toward the summary (greater summary exposure and summary
  share) without increasing total time-on-task, consistent with the interpretation that a
  pre-reading summary functions as an advance organizer that shapes encoding.

  \item \textbf{Structure.} Integrated summaries reduce false-lure endorsement relative to
  segmented summaries, indicating lower susceptibility to misinformation when the summary is
  presented in a context-preserving format. This supports a source-monitoring account in which
  integrated presentation strengthens provenance cues and facilitates verification, whereas
  segmented bullets promote decontextualized item processing and familiarity-driven acceptance.

  \item \textbf{Experience and reliance.} Post-block reliance ratings are highest in the pre-reading
  condition (most robustly for dependence). Dependence is higher under segmented structure,
  whereas trust shows an overall timing effect but weaker pairwise differences after correction.
  Together with the log measures, these ratings suggest that perceived reliance is sensitive to
  both timing and format, and that reliance can increase even when subjective trust does not show
  the same pattern.
\end{itemize}

Taken together, the findings refine the AI Buffer model by separating a \emph{benefit}
pathway and a \emph{risk} pathway. Timing predominantly modulates the benefit pathway by
positioning the buffer as an organizer and cue source during encoding, thereby improving
summary-aligned recognition. Structure predominantly modulates the risk pathway by altering
source-discrimination conditions, thereby shifting vulnerability to plausible misinformation.
This dissociation helps explain why recognition can improve without parallel gains in free
recall, and why interface format can change epistemic safety even when overall performance
effects are modest.

Practically, these results imply that AI reading aids should be designed with explicit cognitive
goals in mind. If the objective is to improve learning of the key propositions an AI summary
provides, the summary should be available \emph{before} reading so it can shape encoding. If the
objective includes preserving source integrity in high-stakes settings, summaries should be
presented in more integrated formats that support contextual checking and maintain clear
boundaries between article content and AI-generated content.

\section{Final Remarks}
\label{sec:final_remarks}

The present conclusions should be interpreted within clear boundaries. The sample size and the
limited set of stimuli constrain power to detect small interactions and moderation effects.
Moreover, the study tests memory within a single experimental session under a controlled
reading protocol with time limits, whereas real-world reading involves self-paced revisiting,
verification, and longer retention intervals.

Future work should therefore (i) test delayed retention to assess whether timing and structure
effects persist after consolidation, (ii) broaden stimuli across genres and domains and vary
summary quality more systematically, and (iii) directly evaluate interface interventions that
target source monitoring (e.g., provenance cues, inline verification prompts, or training in
critical evaluation) to reduce misinformation risk without sacrificing learning benefits.

In closing, the results support a simple but consequential conclusion: the cognitive impact of
AI summaries is design-dependent---\emph{timing} determines how strongly the summary supports
learning, and \emph{structure} determines how safely that support can be used.

%	BIBLIOGRAPHY
%-------------------------------------------------------------------------

\addtocontents{toc}{\vspace{2em}} % Add a gap in the Contents, for aesthetics
\bibliography{Thesis_bibliography} % The references information are stored in the file named "Thesis_bibliography.bib"

%-------------------------------------------------------------------------
%	APPENDICES
%-------------------------------------------------------------------------

\appendix
\input{appendix/appendix_materials}
\input{appendix/appendix_instruments}
\input{appendix/appendix_stats}

\end{document}
