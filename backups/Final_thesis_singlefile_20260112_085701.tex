% A LaTeX template for MSc Thesis submissions to 
% Politecnico di Milano (PoliMi) - School of Industrial and Information Engineering
%
% S. Bonetti, A. Gruttadauria, G. Mescolini, A. Zingaro
% e-mail: template-tesi-ingind@polimi.it
%
% Last Revision: October 2021
%
% Copyright 2021 Politecnico di Milano, Italy. NC-BY

\documentclass{Configuration_Files/PoliMi3i_thesis}

%------------------------------------------------------------------------------
%	REQUIRED PACKAGES AND  CONFIGURATIONS
%------------------------------------------------------------------------------

% CONFIGURATIONS
\usepackage{parskip} % For paragraph layout
\usepackage{setspace} % For using single or double spacing
\usepackage{emptypage} % To insert empty pages
\usepackage{multicol} % To write in multiple columns (executive summary)
\setlength\columnsep{15pt} % Column separation in executive summary
\setlength\parindent{0pt} % Indentation



% PACKAGES FOR TITLES
\usepackage{titlesec}
% \titlespacing{\section}{left spacing}{before spacing}{after spacing}
\titlespacing{\section}{0pt}{3.3ex}{2ex}
\titlespacing{\subsection}{0pt}{3.3ex}{1.65ex}
\titlespacing{\subsubsection}{0pt}{3.3ex}{1ex}
\usepackage{color}

% PACKAGES FOR LANGUAGE AND FONT
\usepackage[english]{babel} % The document is in English  
\usepackage[utf8]{inputenc} % UTF8 encoding
\usepackage[T1]{fontenc} % Font encoding
\usepackage[11pt]{moresize} % Big fonts
\usepackage{CJKutf8} % Enable Chinese text blocks under pdfLaTeX

% PACKAGES FOR IMAGES
\usepackage{graphicx}
\usepackage{transparent} % Enables transparent images
\usepackage{eso-pic} % For the background picture on the title page
\usepackage{subfig} % Numbered and caption subfigures using \subfloat.
\usepackage{tikz} % A package for high-quality hand-made figures.
\usetikzlibrary{}
\graphicspath{{./Images/}{./final_thesis/Images/}} % Images (compile from thesis dir or repo root)
\usepackage{caption} % Coloured captions
\usepackage{xcolor} % Coloured captions
\usepackage{amsthm,thmtools,xcolor} % Coloured "Theorem"
\usepackage{float}

% STANDARD MATH PACKAGES
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage[overload]{empheq} % For braced-style systems of equations.
\usepackage{fix-cm} % To override original LaTeX restrictions on sizes

% PACKAGES FOR TABLES
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{longtable} % Tables that can span several pages
\usepackage{colortbl}

% PACKAGES FOR ALGORITHMS (PSEUDO-CODE)
\usepackage{algorithm}
\usepackage{algorithmic}

% PACKAGES FOR REFERENCES & BIBLIOGRAPHY
\usepackage[colorlinks=true,linkcolor=black,anchorcolor=black,citecolor=black,filecolor=black,menucolor=black,runcolor=black,urlcolor=black]{hyperref} % Adds clickable links at references
\usepackage{cleveref}
\usepackage[square, numbers, sort&compress]{natbib} % Square brackets, citing references with numbers, citations sorted by appearance in the text and compressed
\bibliographystyle{abbrvnat} % You may use a different style adapted to your field

% OTHER PACKAGES
\usepackage{pdfpages} % To include a pdf file
\usepackage{afterpage}
\usepackage{lipsum} % DUMMY PACKAGE
\usepackage{fancyhdr} % For the headers
\fancyhf{}

% Input of configuration file. Do not change config.tex file unless you really know what you are doing. 
\input{Configuration_Files/config}

%----------------------------------------------------------------------------
%	NEW COMMANDS DEFINED
%----------------------------------------------------------------------------

% EXAMPLES OF NEW COMMANDS
\newcommand{\bea}{\begin{eqnarray}} % Shortcut for equation arrays
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\e}[1]{\times 10^{#1}}  % Powers of 10 notation
\newenvironment{ChineseBlock}{\begin{CJK*}{UTF8}{gbsn}}{\end{CJK*}}

%----------------------------------------------------------------------------
%	ADD YOUR PACKAGES (be careful of package interaction)
%----------------------------------------------------------------------------

%----------------------------------------------------------------------------
%	ADD YOUR DEFINITIONS AND COMMANDS (be careful of existing commands)
%----------------------------------------------------------------------------

%----------------------------------------------------------------------------
%	BEGIN OF YOUR DOCUMENT
%----------------------------------------------------------------------------

\begin{document}

\fancypagestyle{plain}{%
  \fancyhf{}% clear all header and footer fields
  \fancyfoot[C]{\thepage}% page number in the centre of the footer
  \renewcommand{\headrulewidth}{0pt}% no header line
  \renewcommand{\footrulewidth}{0pt}% no footer line
}

%----------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------

\pagestyle{empty} % No page numbers
\frontmatter % Use roman page numbering style (i, ii, iii, iv...) for the preamble pages

\puttitle{
    title = {Title of the Thesis}, % Insert the final thesis title
    name = {Duccio Profeti \quad\quad Edoardo Pinzauti}, % Authors on one line
    course = {  Management Engineering}, % Programme in English
    ID = {XXXXX \quad\quad YYYYY}, % Student IDs on one line
    advisor = { Prof. Sergio Terzi}, % Advisor in English
    coadvisor = {}, % Leave empty if none
    academicyear = {Academic Year: 2026--27}, % Academic year in English
}
%----------------------------------------------------------------------------
%	PREAMBLE PAGES: ABSTRACT (inglese e italiano), EXECUTIVE SUMMARY
%----------------------------------------------------------------------------
\startpreamble
\setcounter{page}{1} % Set page counter to 1

% ABSTRACT IN ENGLISH
\chapter*{Abstract} 
Here goes the Abstract in English of your thesis followed by a list of keywords.
The Abstract is a concise summary of the content of the thesis (single page of text)
and a guide to the most important contributions included in your thesis.
The Abstract is the very last thing you write.
It should be a self-contained text and should be clear to someone who hasn't (yet) read the whole manuscript.
The Abstract should contain the answers to the main scientific questions that have been addressed in your thesis.
It needs to summarize the adopted motivations and the adopted methodological approach as well as the findings of your work and their relevance and impact.
The Abstract is the part appearing in the record of your thesis inside POLITesi,
the Digital Archive of PhD and Master Theses (Laurea Magistrale) of Politecnico di Milano.
The Abstract will be followed by a list of four to six keywords.
Keywords are a tool to help indexers and search engines to find relevant documents.
To be relevant and effective, keywords must be chosen carefully.
They should represent the content of your work and be specific to your field or sub-field.
Keywords may be a single word or two to four words. 
\\
\\
\textbf{Keywords:} here, the keywords, of your thesis % Keywords

% ABSTRACT IN ITALIAN
\chapter*{Abstract in lingua italiana}
Qui va l'Abstract in lingua italiana della tesi seguito dalla lista di parole chiave.
\\
\\
\textbf{Parole chiave:} qui, vanno, le parole chiave, della tesi % Keywords (italian)

%----------------------------------------------------------------------------
%	LIST OF CONTENTS/FIGURES/TABLES/SYMBOLS
%----------------------------------------------------------------------------

% TABLE OF CONTENTS
\thispagestyle{empty}
\tableofcontents % Table of contents 
\thispagestyle{empty}
\cleardoublepage

% LIST OF FIGURES
\listoffigures
\addcontentsline{toc}{chapter}{List of Figures}
\cleardoublepage

% LIST OF TABLES
\listoftables
\addcontentsline{toc}{chapter}{List of Tables}
\cleardoublepage

% LIST OF ACRONYMS / GLOSSARY
\chapter*{List of Acronyms / Glossary}
\addcontentsline{toc}{chapter}{List of Acronyms / Glossary}
\begin{longtable}{p{3.2cm}p{11.6cm}}
AI & Artificial Intelligence \\
LLM & Large Language Model \\
PM & Product Manager / Product Management \\
PRD & Product Requirements Document \\
MCQ & Multiple-Choice Question \\
NLP & Natural Language Processing \\
RPA & Robotic Process Automation \\
QA & Quality Assurance \\
UHI & Urban Heat Island \\
GDPR & General Data Protection Regulation \\
EUV & Extreme Ultraviolet Lithography \\
SoC & System-on-a-Chip \\
\end{longtable}
\cleardoublepage

%-------------------------------------------------------------------------
%	THESIS MAIN TEXT
%-------------------------------------------------------------------------
% In the main text of your thesis you can write the chapters in two different ways:
%
%(1) As presented in this template you can write:
%    \chapter{Title of the chapter}
%    *body of the chapter*
%
%(2) You can write your chapter in a separated .tex file and then include it in the main file with the following command:
%    \chapter{Title of the chapter}
%    \input{chapter_file.tex}
%
% Especially for long thesis, we recommend you the second option.

\addtocontents{toc}{\vspace{2em}} % Add a gap in the Contents, for aesthetics
\mainmatter % Begin numeric (1,2,3...) page numbering

% Use only footer page numbers, no header
\pagestyle{fancy}
\fancyhf{}                % clear header & footer
\fancyfoot[C]{\thepage}   % page number centered in footer
\renewcommand{\headrulewidth}{0pt} % remove header line
\renewcommand{\footrulewidth}{0pt} % optional: no footer line

% --------------------------------------------------------------------------
% NUMBERED CHAPTERS % Regular chapters following
% -------------------------------------------------------------------------

% --------------------------------------------------------------------------
% NUMBERED CHAPTERS % Regular chapters following
% -------------------------------------------------------------------------


\chapter{Introduction}
\label{ch:introduction}
Generative AI systems are increasingly embedded in everyday information work. Two use cases are especially salient
for learning and decision-intensive knowledge work: (i) AI-generated summaries and explanations that accompany
reading and study, and (ii) AI-generated analyses and recommendations that accompany professional judgment. In both
cases, the AI produces \emph{external representations}---persistent, content-bearing artifacts that can guide
attention, provide cues, and reshape how information is encoded, retrieved, and acted upon.

These developments sit on top of a longer trajectory of AI research and deployment. Historical and bibliometric
accounts describe multiple waves of AI evolution---from early symbolic systems to contemporary deep-learning
approaches and the emergence of new ``generations'' of AI capabilities---that have expanded the feasibility of
natural-language interfaces and large-scale decision support \citep{SchmidhuberAli2022AnnotatedHistoryModernAI,ShaoRen2022EvolutionAIBibliometric,Zhang2020ThirdGenerationAI,DwivediDutot2023EvolutionAIResearchTFSC}.

From a cognitive perspective, external representations can support performance by reducing extraneous processing
costs, but they can also change encoding strategies via cognitive offloading
\citep{Risko2016CognitiveOffloading,Sparrow2011GoogleEffect}. From a decision-making perspective, the same
representations can shape reliance: users may defer to AI outputs without fully scrutinizing their provenance or
quality, creating epistemic and governance risks \citep{Pasquale2015BlackBoxSociety,Zhang2024KnowledgeImbalanceAI}.
These tensions are particularly relevant for roles such as product management, where decisions integrate
heterogeneous evidence (user research, analytics, technical constraints, and stakeholder input) and where
generative AI is increasingly used to summarize, prioritize, and communicate \citep{Pinzauti2025ReinventingPM,GnanasambandamHall2024GenAIAccelerateTimeToMarket}.

Across sectors, AI is framed as both an operational technology (automation of routine processes) and a strategic
capability (augmenting scenario exploration, planning, and coordination). Industry and practitioner sources
document diffusion of AI applications across corporate functions---from customer service and operations management
to finance and human resources---and emphasize that value capture requires changes in workflows, roles, and
governance rather than model deployment alone \citep{GoodwinQuinteroValencia2024AIBusinessIBM,RashidAhmed2024AIRevolutionizingIndustries,Luu2024AIApplicationsBestarion,EdlichIqbal2018BotsAlgorithmsCorporateSupport,McKinsey2025StateOfAIRewiring,McGrath2024AITransformingOperationsManagementIBM,NawazAhmad2024AIAdoptionHRMPractices,Bailey2025AITransformsScenarioAnalysisCFI}.

This thesis addresses these issues through a \emph{multi-level perspective} that links cognitive mechanisms to
organizational practice. At the cognitive level, Study~1 uses a controlled reading-and-memory experiment in which
AI assistance is operationalized as pre-generated summaries that vary in \emph{structure} (integrated vs.\ segmented)
and \emph{timing} (pre-reading vs.\ synchronous vs.\ post-reading). At the organizational level, Study~2 examines
how product managers integrate generative AI tools into decision-intensive workflows and how they perceive impacts
on efficiency, judgment, and ethical responsibility \citep{Pinzauti2025ReinventingPM}. The thesis proposes the
\emph{AI Buffer} as a unifying concept: AI-generated external representations can function as persistent buffers
that support cue-based cognition while also introducing source-monitoring and reliance vulnerabilities.

\section{Context and Motivation}
\label{sec:motivation}
Learning from text depends on how readers allocate attention during encoding, how they manage limited working-memory
resources, and which cues are available at retrieval
\citep{Baddeley2012WorkingMemory,Craik1972LevelsOfProcessing,Tulving1973EncodingSpecificity}. In real-world settings,
learners frequently rely on external representations---notes, outlines, highlighted passages, and search
engines---to support these processes. Generative AI introduces a new, scalable form of external representation:
systems can produce structured summaries instantly and present them alongside source material.

At the same time, external representations can shift effort away from internal encoding. The ``Google effect''
shows that when information is easy to re-access, people are more likely to remember where it is than what it is
\citep{Sparrow2011GoogleEffect,Risko2016CognitiveOffloading,Gong2024GoogleEffectsMetaAnalysis}. In generative-AI
settings, this creates a core tension: summaries may support understanding and recognition by providing cues, but
they may also encourage shallow processing, reduce detail-rich encoding, and increase vulnerability to source
confusion when AI content blends with the original material
\citep{Johnson1993SourceMonitoring,Chan2024ConversationalAIFalseMemories}.

Beyond individual learning, the same dynamics play out in professional contexts where decisions depend on accurate,
contextual recall. Knowledge-management research suggests that AI can help organizations retrieve prior decisions
and maintain continuity across projects \citep{Pai2022AIKnowledgeManagement}. However, AI-advised decision-making
also raises risks of epistemic dependence and opaque reasoning
\citep{Pasquale2015BlackBoxSociety,Zhang2024KnowledgeImbalanceAI}. Product management provides a concrete setting in
which these issues are amplified: product managers are expected to synthesize noisy information into decisions and
to communicate rationale clearly across teams, making AI-provided summaries and recommendations both useful and
potentially hazardous \citep{Pinzauti2025ReinventingPM}.

\section{Research Problem and Gap}
\label{sec:research_problem}
Despite rapid adoption, there remains limited integrated evidence on \emph{how} AI-generated external
representations shape cognition and decision-making.

At the cognitive level, evidence on generative AI is still emerging and often treats ``AI assistance'' as a
unitary intervention. Yet design choices matter: \emph{when} a summary is available and \emph{how} it is structured
can change attention allocation, encoding depth, and the balance between internal reconstruction and reliance on
external cues. Controlled studies suggest that conversational AI can amplify false memories under certain
conditions \citep{Chan2024ConversationalAIFalseMemories}, and reviews warn about over-reliance and reduced cognitive
engagement \citep{Zhai2024OverRelianceAIDialogue}, but there is still a need for causal evidence that isolates
interface-relevant dimensions (timing, format) and measures both performance and epistemic risk.

At the organizational level, many accounts focus on productivity or adoption narratives, while fewer connect
observed reliance patterns to cognitive mechanisms such as offloading and source monitoring. For decision-intensive
roles such as product management, the open question is not only whether AI improves efficiency, but whether it
changes how decisions are formed, justified, and governed---including how trust, transparency, and accountability
are maintained when AI outputs shape reasoning \citep{Pasquale2015BlackBoxSociety,Pinzauti2025ReinventingPM}.

The central gap motivating this thesis is therefore the lack of a \emph{multi-level framework} that links (i)
mechanisms of AI-assisted encoding and retrieval to (ii) patterns of reliance and judgment in professional
decision-making contexts.

\section{Research Objectives and Multi-Level Perspective}
\label{sec:objectives}
Following common master-thesis conventions, the work is organized around concrete objectives that span the two
levels of analysis and their integration.
\begin{itemize}
  \item \textbf{O1 --- Cognitive baseline:} quantify memory differences between AI-assisted reading and unaided
  reading under matched constraints (Study~1).
  \item \textbf{O2 --- Cognitive design dimensions:} test how summary \emph{timing} and \emph{structure} affect
  recall quality, recognition accuracy, and behavioral indicators of reliance (Study~1).
  \item \textbf{O3 --- Epistemic risk:} measure susceptibility to false-lure endorsement and source-monitoring
  errors when AI-generated content is present (Study~1).
  \item \textbf{O4 --- Organizational practice:} characterize how product managers deploy generative AI tools in
  decision-intensive workflows, including perceived benefits, risks, and ethical responsibilities (Study~2)
  \citep{Pinzauti2025ReinventingPM}.
  \item \textbf{O5 --- Cross-level integration:} develop an integrated account linking AI-generated external
  representations to both memory mechanisms and professional decision processes, and derive propositions that can
  guide safer tool design and governance.
\end{itemize}

\section{Research Questions}
\label{sec:research_questions}
Building on the research gap identified in the previous section, this thesis investigates how artificial
intelligence reshapes knowledge-intensive decision-making by simultaneously affecting individual cognitive
processes and organizational practices. To address this objective, the study adopts a multi-level perspective and
is guided by the following research questions:

\begin{itemize}
  \item \textbf{RQ1 -- Cognitive Level (Study 1).} \textbf{RQ1:} How do AI-generated external representations
  influence core human memory processes during knowledge-intensive tasks?
  \begin{itemize}
    \item \textit{Memory encoding} (attention allocation, depth of processing).
    \item \textit{Recall and recognition} (quantity and quality of retained information).
    \item \textit{Source monitoring} (discriminating article content from AI-generated content).
    \item \textit{Confidence calibration} (alignment between confidence and accuracy).
    \item \textit{Cognitive load} (perceived effort and split-attention costs).
  \end{itemize}
  Conceptually, RQ1 corresponds to the AI Buffer Model formulated as a research question: it asks how persistent
  AI-generated representations shape encoding, retrieval, and metacognitive monitoring.

  \item \textbf{RQ2 -- Organizational Level (Study 2).} \textbf{RQ2:} How does the adoption of AI tools shape
  decision-making practices, role configuration, and perceived responsibility in product management?
  \begin{itemize}
    \item Role transformation and reallocation of tasks toward more strategic work.
    \item AI-supported decision-making (sensemaking, prioritization, and communication).
    \item Governance and ethics (accountability, transparency, oversight practices).
    \item Cross-context comparison (e.g., differences across organizational and cultural settings such as USA--China).
  \end{itemize}
  Importantly, RQ2 is not treated as a mere ``application'' of Study~1, but as an autonomous analytical level that
  characterizes AI adoption in a decision-intensive managerial domain.

  \item \textbf{RQ3 -- Integrative Level (Cross-study).} \textbf{RQ3:} How can changes in organizational
  decision-making practices enabled by AI be explained through underlying cognitive mechanisms of AI-assisted
  memory?
\end{itemize}

RQ3 is the key integrating question of the thesis: rather than reporting two disconnected sets of findings, the
thesis aims to explain observed organizational patterns (Study~2) using a coherent account of memory mechanisms
identified under controlled conditions (Study~1). This integrative aim motivates the conceptual framework in
Chapter~3 and the dedicated cross-study integration logic developed later in the thesis.

\section{Contribution of the Thesis}
\label{sec:contribution}
The thesis contributes at three levels: cognitive theory, organizational understanding, and integrative framing.

\textbf{Cognitive contribution.} Study~1 provides design-sensitive evidence on AI-assisted reading by measuring both
performance (recall and recognition) and epistemic risk (false-lure endorsement), explicitly manipulating timing
and structure of AI-generated summaries.

\textbf{Organizational contribution.} Study~2 synthesizes patterns of AI use in product management and clarifies how
AI tools are positioned as automation and augmentation within decision-intensive workflows, including perceived
shifts in responsibility and ethical vigilance \citep{Pinzauti2025ReinventingPM}.

\textbf{Integrative contribution.} The thesis advances the \emph{AI Buffer} as a compact bridge concept:
AI-generated representations can function as persistent external buffers that support cue-based cognition while
also altering source monitoring and reliance conditions. This framing supports actionable implications for AI
interface design, training, and governance in knowledge work.

\section{Scope, Definitions, and Assumptions}
\label{sec:scope}
This thesis focuses on AI-generated external representations and their consequences under two complementary
methodological lenses.

\textbf{Key definitions.} \emph{Generative AI} refers to systems that produce novel text based on learned patterns
from data. \emph{External representations} refer to persistent artifacts (summaries, notes, recommendations) that
users can consult during a task. \emph{Cognitive offloading} refers to shifting cognitive work from internal memory
to external aids \citep{Risko2016CognitiveOffloading}. \emph{Source monitoring} refers to processes by which people
attribute information to its origin \citep{Johnson1993SourceMonitoring}. In this thesis, the \emph{AI Buffer} is
defined as an AI-generated representation that persists during a task and can shape encoding, retrieval, and
metacognitive monitoring.

\textbf{Study boundaries.} Study~1 is intentionally scoped to controlled laboratory conditions, pre-generated
summaries (rather than open-ended dialogue), and within-session memory measures. Study~2 is scoped to product
management as a decision-intensive domain and emphasizes professional practice and governance rather than direct
cognitive performance measurement.

\textbf{Assumptions.} The thesis treats AI summaries as fallible representations rather than authoritative ground
truth and assumes that interface properties (timing, structure, provenance cues) can change cognitive and
organizational outcomes by shaping attention, cue availability, and reliance norms.

\section{Structure of the Thesis}
\label{sec:thesis_structure}
The remainder of the thesis follows the structure below:
\begin{itemize}
  \item \textbf{Chapter~2 (Literature Review)} synthesizes human-memory foundations and research on cognitive
  offloading, external representations, and human--AI reliance in knowledge work and managerial contexts.
  \item \textbf{Chapter~3 (Conceptual Framework and Hypotheses)} introduces the AI Buffer model and derives
  cross-level propositions linking cognitive mechanisms to decision processes.
  \item \textbf{Chapter~4 (Methodology)} details Study~1 (controlled experiment) and Study~2 (field study on product
  managers), including instruments and ethical considerations.
  \item \textbf{Chapters~5--7 (Analysis, Results, Integration)} report analytical strategy and findings for each
  study and then integrate evidence across levels.
  \item \textbf{Chapters~8--9 (Discussion and Conclusion)} interpret implications, limitations, and future research
  directions, and summarize the thesis contributions.
  \item \textbf{Appendices} provide study materials, instruments, and supplementary outputs.
\end{itemize}


\chapter{Literature Review}
\label{ch:literature-review}
This chapter reviews prior work that motivates the thesis structure introduced in \Cref{sec:thesis_structure}. It
first establishes foundations of human memory and learning from text, then synthesizes research on cognitive
offloading and external representations. Building on this, it reviews how generative AI functions as a new kind of
external representation in knowledge work, with particular attention to confidence, source monitoring, cognitive
load, and reliance in decision-making. The chapter closes by synthesizing these strands into a multi-level research
gap that motivates the conceptual framework developed in Chapter~3.

\section{Foundations of Human Memory}

\subsection{Theoretical Foundations of Human Memory}
The study of human memory has evolved through several major theoretical frameworks that describe how information is
encoded, stored, and retrieved. The earliest comprehensive account is the \emph{modal model} proposed by Atkinson
and Shiffrin \citep{Atkinson1968HumanMemory}, which conceptualizes memory as a system composed of three structural
stores: sensory memory, short-term memory (STM), and long-term memory (LTM). According to this framework,
information enters through high-capacity but rapidly decaying sensory registers, is filtered into STM via
attentional control, and may be consolidated into LTM through rehearsal and elaborative processing. This model
introduced the influential distinction between temporary and durable memory systems and highlighted the central
role of control processes---particularly attention and rehearsal---in regulating information flow.

A major theoretical shift occurred with Baddeley’s reconceptualization of STM as \emph{working memory}
\citep{Baddeley2012WorkingMemory}. Rather than a passive buffer, working memory is understood as an active
cognitive workspace composed of multiple subsystems (the phonological loop, visuospatial sketchpad, episodic
buffer, and a central executive) that enable the integration of multimodal information. This framework supports
reasoning, decision-making, and sustained mental effort, and it aligns with extensive neuropsychological evidence
for domain-specific maintenance mechanisms and executive-control functions distributed across the cortex.

Building on these models, Tulving introduced the distinction between \emph{episodic} and \emph{semantic} memory
\citep{Tulving1972EpisodicSemantic}. Episodic memory refers to the recollection of personally experienced,
context-rich events, whereas semantic memory stores generalized knowledge about the world. Tulving further proposed
the \emph{encoding specificity} principle \citep{Tulving1973EncodingSpecificity}, which posits that retrieval is
most successful when the cues available at encoding are reinstated at recall. This principle is central for
understanding how external aids (including AI-generated summaries) may facilitate or distort later recall by
changing which cues are available and salient.

Craik and Lockhart’s \emph{Levels of Processing} framework \citep{Craik1972LevelsOfProcessing} emphasized that the
durability of a memory trace depends not on which store it resides in, but on the \emph{depth} and \emph{semantic
elaboration} of processing applied to the information. Deep, meaning-oriented encoding produces more persistent
memory traces than shallow, perceptual processing---a finding repeatedly confirmed by subsequent behavioral and
neuroscientific studies.

Neurophysiological research has extended classical models by illuminating the biological basis of encoding and
retrieval. Single-neuron recording studies show that hippocampal and medial-temporal lobe neurons respond
selectively to abstract concepts, forming building blocks of episodic and semantic representations
\citep{Rutishauser2021ArchitectureHumanMemory}. Electrophysiological evidence further demonstrates that stronger
neural activation during encoding predicts greater likelihood of later retrieval success---the subsequent-memory
effect \citep{Caplan2009EEGAssociativeOrder}.

Memory research has also embraced ecological and contextual perspectives. Studies using immersive virtual
environments indicate that spatially rich, multisensory contexts enhance episodic encoding
\citep{Plancher2018VREpisodicMemory}, while ``real-life'' neuroscience approaches argue that memory should be
studied in dynamic, socially embedded settings rather than in isolation \citep{ShamayTsoory2019RealLifeNeuroscience}.
At a systems level, contemporary accounts emphasize that memory emerges from distributed interactions across
cortical--hippocampal networks \citep{Moscovitch2006CognitiveNeuroscienceRemoteMemory}, and neuroimaging evidence
suggests that retrieving a memory can reactivate cortical patterns present during encoding
\citep{Liu2021TransformativeNeuralRepresentations}. Taken together, these perspectives motivate two principles used
throughout this thesis: (i) encoding depth and attentional control strongly shape memory outcomes, and (ii) memory
is cue-dependent, reconstructive, and sensitive to external context.

A complementary tradition in cognitive psychology emphasizes that remembering is not a verbatim replay but a
\emph{constructive} process shaped by schemas and expectations. In his seminal work, Bartlett argued that recall
reflects reconstruction guided by prior knowledge, which can yield distortions that are coherent but not faithful to
the original experience \citep{Bartlett1932Remembering}. This reconstructive property is central for AI-assisted
cognition: when users encounter fluent AI-generated representations, those representations can become integrated into
the schema that guides later reconstruction, increasing the risk that plausible but incorrect details are remembered
as ``having been in the source''.

From a systems perspective, episodic remembering is distinguished from semantic knowledge because it carries
contextual tags (where, when, and how information was acquired) that enable source attribution and confidence
judgments. Reviews of episodic-memory mechanisms highlight that contextual reinstatement and source cues are key
determinants of retrieval fidelity \citep{Papagno2015EpisodicMemory}. In this thesis, this motivates treating source
monitoring and misinformation susceptibility as first-class outcomes rather than as peripheral errors.

Finally, because the thesis links cognitive constructs (encoding, retrieval, source monitoring) to organizational
practices (decision workflows, governance), it requires a coherent mapping between levels and paradigms. Work on
cognitive ontologies argues that many ``constructs'' are families of measures embedded in specific experimental
paradigms \citep{Turner2012CognitiveParadigmOntology}. This motivates the thesis’s multi-method strategy: rather than
assuming one-to-one equivalence between laboratory tasks and professional practice, the thesis uses Study~1 as
mechanistic evidence and Study~2 as contextual evidence and integrates them via the AI Buffer Model.

\subsection{How Readers Learn from Expository Texts}
Humans do not memorize information in a vacuum; when reading expository texts (such as scientific articles or
technical reports), they actively construct a mental representation of the content that integrates new information
with prior knowledge. According to discourse comprehension theory, readers form a hierarchical \emph{situation
model} of a text, which entails establishing local coherence between consecutive ideas and global coherence across
the entire discourse \citep{vanDijk1983Discourse}. In building this situation model, the reader must identify the
text’s structure, discern relationships between key concepts, and continuously update their understanding as new
information is introduced. This process places substantial demands on attention and working memory.

Two lines of learning research are particularly relevant for interpreting AI summaries in reading contexts. First,
active generation typically improves later memory compared with passive exposure---the generation effect
\citep{SlameckaGraf1978GenerationEffect}. When readers produce their own explanations, outlines, or summaries, they
create retrieval routes that support later reconstruction. Second, retrieval practice improves durable learning
relative to restudying---the testing effect \citep{RoedigerKarpicke2006TestingEffect}. In applied settings, inserting
interpolated tests while studying long materials reduces mind-wandering and improves later performance
\citep{Szpunar2013InterpolatedTestingPNAS,Jing2016InterpolatedTestingApplied}, and the forward testing effect suggests
that retrieval can also enhance learning of subsequent material by improving attention and encoding processes
\citep{Chan2018ForwardTestingEffect}. These findings imply a key boundary condition for AI assistance: if AI summaries
substitute for self-generated processing, they may improve short-term task performance while weakening generative
retrieval and long-term integration.

A related implication concerns measurement: recognition and recall often reflect partially distinct retrieval
processes. Process dissociation approaches highlight that recognition can be supported by familiarity in addition to
recollection, whereas free recall depends more strongly on self-generated retrieval routes
\citep{Jacoby1991ProcessDissociation}. This distinction foreshadows the recognition--recall dissociation observed in
Study~1 and helps interpret why an AI-provided scaffold may boost MCQ performance without producing parallel gains in
free recall.

Because of these cognitive demands, the way an expository text is structured---and the presence of guiding cues or
previews---can significantly influence learning outcomes. Pre-reading aids, often termed \emph{advance organizers}
\citep{Ausubel1960AdvanceOrganizers,Ausubel1968EducationalPsychology}, activate relevant schemas and highlight the
organizational framework of the forthcoming content. Empirical studies find that students who receive a conceptual
outline or summary before reading complex text often achieve better comprehension and organization of recall
\citep{MayerBromage1980AdvanceOrganizersRecall,HartleyDavies1976PreinstructionalStrategies}. Structural cues such as
informative headings guide attention and improve memory for the organization of ideas \citep{LorchLorch1996HeadingsRecall}.

Importantly, these aids often improve \emph{integration} and comprehension even when total free recall of facts is
unchanged. Hartley and Davies \citep{HartleyDavies1976PreinstructionalStrategies} found that overviews do not
always increase verbatim recall but consistently improve learners’ ability to grasp and transfer key concepts.
Similarly, Stull and Mayer \citep{Stull2007GraphicOrganizers} showed that instructor-generated graphic organizers
improve conceptual performance even without an advantage on immediate factual recall. This distinction between
organizational understanding and verbatim detail is relevant for evaluating AI-generated summaries: summaries may
strengthen gist and coherence while potentially weakening detail-rich encoding.

\section{Cognitive Offloading and External Representations}
External representations are a long-standing feature of human cognition: people routinely use notes, outlines, and
digital tools to reduce memory demands and to coordinate complex tasks. In cognitive terms, this is often
described as \emph{cognitive offloading}---the strategic use of external artifacts to store information or to
reduce internal processing requirements \citep{Risko2016CognitiveOffloading}. Offloading can be adaptive in
environments where working-memory resources are limited and tasks are time-constrained
\citep{Baddeley2012WorkingMemory}.

The most visible modern example is internet search. When information is reliably accessible, people encode less of
the content itself and more of the location or access strategy \citep{Sparrow2011GoogleEffect}. Meta-analytic
evidence suggests that frequent search behavior is associated with reduced recall of content alongside improved
memory for where information can be found \citep{Gong2024GoogleEffectsMetaAnalysis}. These findings clarify an
important trade-off: external availability can improve task performance while weakening internal knowledge
structures if it displaces deep processing \citep{Craik1972LevelsOfProcessing}.

This trade-off has motivated broader arguments that pervasive digital scaffolding may reshape cognition over time by
changing the balance between internal memory and external retrieval habits. The ``online brain'' perspective
synthesizes evidence that ubiquitous access technologies can shift attentional strategies and memory reliance
\citep{Firth2019OnlineBrain}. While such shifts can be adaptive in complex environments, they can also create
dependence on external systems for knowledge continuity and reduce opportunities for effortful encoding and
self-generated retrieval.

Generative AI extends offloading beyond retrieval by producing \emph{transformed} representations (summaries,
explanations, and drafts). This transformation can function as a scaffold for comprehension and decision-making,
but it also changes source-monitoring conditions: when AI-generated content is fluent and plausible, users may
later confuse its origin or accept it with insufficient scrutiny
\citep{Johnson1993SourceMonitoring,Zhai2024OverRelianceAIDialogue}. Accordingly, the effects of generative AI cannot
be understood solely as ``more information access''; they depend on how AI representations are timed, structured,
and integrated into the user’s workflow.

\section{Generative AI as External Representation in Knowledge Work}
\subsection{Cognitive Effects of Generative AI Systems}
Generative AI systems can influence cognition through mechanisms such as prompting deeper processing, offloading
cognitive tasks, providing meta-cognitive feedback, and offering context-specific cues. Research indicates that AI
tools can improve learning outcomes when they encourage reflection and elaboration. For example, \citet{Bai2023ChatGPTLearningMemory}
found that students who interact with a large language model by asking questions and explaining answers can
achieve better learning results than those who study without AI. The dialogue can prompt deeper semantic processing,
consistent with the Levels of Processing framework \citep{Craik1972LevelsOfProcessing}. Similarly, \citet{Haider2024AICognitiveFunctions}
report improvements in recall and problem-solving in AI-assisted learning settings.

Beyond encoding, AI can support rehearsal and retrieval by providing external memory scaffolds. The Memoro system,
for instance, offers a real-time dialogue interface that prompts users to restate and verify information during
study \citep{Memoro2024RealTimeMemoryAugmentation}. This resembles retrieval practice: periodic prompting and
feedback can strengthen retention by reactivating information and integrating it with existing knowledge.
Complementing behavioral evidence, preliminary neuroscience work suggests that AI-assisted writing can modulate
neural engagement associated with attention and semantic processing \citep{MITMediaLab2023BrainOnChatGPT}.

AI systems can also support \emph{metacognition}---the monitoring and regulation of one’s own cognitive processes.
\citet{Sun2025GenerativeAICreativity} show that generative AI can improve metacognitive calibration in creative
tasks by prompting evaluation of alternatives and reflection on uncertainty. In learning contexts, similar prompts
may improve confidence judgments by making uncertainty explicit and by encouraging verification behaviors.

\subsection{Benefits and Risks of AI as External Representation}
The benefits of AI assistance are coupled with significant risks. One concern is that AI tools can encourage users
to invest less effort in understanding and remembering, since fluent answers and summaries are available on demand.
This may produce an illusion of competence: a user appears capable with AI support but has not internalized the
information \citep{Oakley2025MemoryParadox}. Such risk is consistent with the offloading literature and with
theories that emphasize deep processing and active engagement for durable memory
\citep{Craik1972LevelsOfProcessing,Baddeley2012WorkingMemory}.

Another risk is \emph{memory distortion} through AI-generated misinformation or suggestion. \citet{Chan2024ConversationalAIFalseMemories}
show that conversational AI can inadvertently increase false memories in tasks analogous to witness interviews,
highlighting how plausible, fluent AI output can become intertwined with a user’s own recollections. Systematic
reviews also caution that heavy dependence on AI tutors can reduce active engagement and weaken long-term skill
development \citep{Zhai2024OverRelianceAIDialogue}.

At a broader level, pervasive AI tools may shift cognitive habits by reallocating effort from memorizing details
toward managing interfaces and interpreting AI outputs \citep{Gerlich2025AIToolsSociety}. AI can also shape the
emotional and contextual aspects of memory: AI-curated experiences may heighten arousal or personalization, which
can strengthen certain memories, but can also narrow exposure and reduce diversity of encoded experiences
\citep{Beyaria2024NeuromarketingAIEmotionMemory}. Understanding when AI functions as a beneficial extension versus a
detrimental replacement of human memory is a central challenge; this thesis approaches the problem by examining
design-sensitive mechanisms, particularly timing and structure of AI-generated summaries.

\section{Confidence, Source Monitoring, and Cognitive Load in Assisted Cognition}
AI-generated summaries can alter not only what is learned, but also the conditions under which confidence is
formed and sources are discriminated. This section synthesizes three theoretical strands that motivate the key
design manipulations in Study~1: (i) cognitive load and split attention during learning, (ii) how timing of access
changes encoding and cue availability, and (iii) how source-monitoring demands shape vulnerability to plausible but
incorrect information.

\subsection{Foundational Theories Motivating Timing and Structure}\label{subsec:theories-timing-structure}
The experimental manipulations in this thesis are grounded in well-established findings in the learning sciences.
The AI-generated summary functions as a compact surrogate for an article’s key propositions. The timing of access
to this summary is manipulated to test whether seeing a summary \emph{before} reading helps to organize and
scaffold encoding of the full text, as opposed to seeing it mid-way or only after reading. The structure of the
summary is manipulated to test whether presenting it as an \emph{integrated paragraph} versus as \emph{segmented
bullet points} affects coherence building and source attributions.

\paragraph{Advance organizers and pre-reading scaffolds.}
Ausubel’s theory of advance organizers predicts that providing a high-level conceptual framework \emph{prior} to
learning enhances integration of new knowledge by activating relevant schemas and guiding attention during encoding
\citep{Ausubel1960AdvanceOrganizers,Ausubel1968EducationalPsychology}. Evidence on pre-instructional strategies
shows that previews (e.g., outlines or summaries) can improve comprehension and organization
\citep{MayerBromage1980AdvanceOrganizersRecall,HartleyDavies1976PreinstructionalStrategies}, and headings can guide
attention and improve recall of structure \citep{LorchLorch1996HeadingsRecall}. This suggests that pre-reading access
to AI summaries may be especially beneficial when the summary acts as an encoding scaffold rather than as a
post-hoc reminder.

\paragraph{Mid-reading interruptions and divided attention.}
Presenting an AI summary synchronously may introduce costs via task-switching and disrupted coherence: readers may
need to pause, reorient, and rebuild a situation model. Task interruption and divided attention can impose extra
working-memory demands, potentially weakening integration of information and increasing reliance on surface-level
cues \citep{Baddeley2012WorkingMemory}.

\paragraph{Cognitive load, split attention, and integrated presentation.}
Cognitive load theory emphasizes limited working-memory resources and predicts that formats that minimize
extraneous processing demands yield better learning \citep{Sweller1988CognitiveLoad}. The split-attention effect
shows that when related information is separated, learners must expend effort to integrate it, which can impair
learning \citep{ChandlerSweller1992SplitAttention}. Applied evidence confirms that integration vs.\ separation can
produce measurable performance differences \citep{PociaskMorrison2008SplitAttentionRedundancy}. In this context, an
integrated paragraph summary may reduce unnecessary scanning and facilitate verification against the source text,
whereas segmented bullet points may encourage piecemeal processing and increase coordination demands.

\subsection{Memory Distortion and Epistemic Risk}\label{subsec:distortion-risk}
The Source Monitoring Framework \citep{Johnson1993SourceMonitoring} emphasizes that remembering involves
determining where information came from (e.g., ``Did I read this in the article, or did it come from the summary?'').
Source attributions rely on contextual and qualitative features of a memory; when different sources overlap and
memories lack distinctive details, misattribution becomes more likely. The misinformation effect illustrates this
risk: misleading post-event information can be incorporated into memory and later misattributed to the original
event \citep{LoftusPalmer1974Misinformation,Loftus2005PlantingMisinformation}. Related work shows that suggestibility
increases when sources share overlapping details and contextual tags are weak \citep{LindsayJohnson1989SuggestibilitySource}.

Beyond individual paradigms, reviews of misinformation and memory distortion emphasize that false memories are not
rare anomalies but systematic outcomes of normal reconstructive processes under conditions of suggestion,
source-confusability, and fluency \citep{Frenda2011MisinformationAdvances}. For AI-assisted cognition, this is a
critical framing: when AI outputs are plausible and presented in ways that blur provenance cues, they can function
as post-event information that competes with or overwrites source material in memory.

False-memory research further underscores how gist-based representations can generate confident errors. In the DRM
paradigm, people falsely recall theme-consistent lure words \citep{RoedigerMcDermott1995FalseMemoriesDRM}. Fuzzy
Trace Theory explains this by positing that both gist and verbatim traces are encoded, but reliance on gist can
dominate when verbatim traces are weak \citep{BrainerdReyna2005ScienceFalseMemory}. Summaries emphasize gist and can
thus increase vulnerability to gist-consistent false recognitions, especially if AI output is plausible and fluent.
This motivates measuring epistemic risk (e.g., false-lure endorsement) in Study~1 and motivates design choices that
support provenance discrimination.

\section{Human--AI Decision-Making (Trust, Reliance, Bias, Accountability)}
\label{sec:human-ai-decision-making}
When AI outputs support decisions, the central behavioral question becomes not only \emph{accuracy} but also
\emph{calibrated reliance}: users must decide when to accept an AI representation, when to verify it, and when to
disregard it. In educational contexts, over-reliance can reduce active engagement and long-term skill development
\citep{Zhai2024OverRelianceAIDialogue}. In professional contexts, similar dynamics can create a dependence loop in
which decision-makers defer to AI recommendations even when they could reason independently, producing a knowledge
imbalance and increasing epistemic risk \citep{Zhang2024KnowledgeImbalanceAI}. These dynamics are more likely when
AI outputs are fluent, time pressure is high, and verification costs are non-trivial.

Work on AI assistants and user interpretations further suggests that interface framing can shape expectations about
agency and competence, influencing both trust and perceived responsibility---a dynamic that becomes more salient as
AI outputs are embedded into workflows and presented with conversational fluency \citep{KaplanHaenlein2018SiriSiriAI}.

Importantly, ``trust'' and ``reliance'' should be distinguished. Users may express general trust in AI systems while
still choosing not to rely on them in a given task, or they may rely heavily because the workflow makes
verification costly even when trust is low. In assisted cognition, reliance becomes a form of \emph{resource
allocation}: deciding whether to invest effort in independent reasoning, to cross-check sources, or to offload to an
external representation \citep{Risko2016CognitiveOffloading}. This implies that improving outcomes is not only about
increasing AI accuracy, but also about shaping the conditions of calibrated reliance (feedback, provenance cues, and
low-friction verification).

Reliance is also mediated by skill and literacy. Because generative AI outputs are probabilistic and can contain
plausible errors, effective use requires AI literacy: the ability to interpret outputs, recognize uncertainty, and
choose appropriate validation strategies \citep{WangPham2022AILiteracyScale}. Without such literacy, users may adopt
uncritical patterns of use (e.g., accepting fluent summaries as ground truth) or may underuse AI due to uncertainty
about when it is appropriate. This thesis therefore treats reliance not as a stable attitude, but as a dynamic
interaction between task constraints, user competencies, and representational design.

Reliance is also shaped by how AI is embedded in team settings. In organizational decision-making, the relevant
unit is often not the individual user but the \emph{human--AI--team} system: AI outputs are interpreted, debated,
and propagated through artifacts that multiple people consume. Conceptual and empirical work on combining human and
artificial intelligence for strategic decisions highlights both complementarities (speed of synthesis, scenario
exploration) and coordination risks when AI recommendations are treated as authoritative without scrutiny
\citep{TrunkBreitenecker2020HumanAIStrategicDecisionMaking,KaggwaFonsah2024AIDecisionMakingBusinessStrategies}. In
collaborative work, the benefits of AI assistance can also depend on experience and role configuration: evidence on
human--AI teaming suggests that workers with different levels of experience may collaborate with AI differently,
which can shape whether AI augments judgement or induces over-reliance \citep{WangGino2023FriendOrFoeAITeaming}.

Reliance is also shaped by accountability and transparency constraints. If an AI system influences decisions but
its rationale is opaque, organizations may struggle to audit errors, detect bias, or assign responsibility. The
``black-box'' problem is therefore not only a technical issue but a governance issue: opaque systems can encourage
uncritical acceptance while making failures hard to diagnose \citep{Pasquale2015BlackBoxSociety}. For roles that
mediate between stakeholders and technical systems---such as product management---this elevates the importance of
provenance cues, documentation practices, and norms of verification when AI-generated summaries and
recommendations enter decision workflows \citep{Pinzauti2025ReinventingPM}.

Finally, human--AI decision-making has an explicit ethical dimension. When AI representations influence judgments,
errors can be difficult to detect and responsibility can be difficult to assign, especially when AI content is
reused and propagated through documents, dashboards, and presentations. In such contexts, accountability is not only
about explaining model outputs; it is also about maintaining a decision trace that links claims to sources and
indicates which parts of a decision were AI-mediated \citep{Pasquale2015BlackBoxSociety}. This is directly relevant
for roles like product management, where decisions must be justified to stakeholders and later revisited.

\section{AI in Professional and Managerial Contexts}
\subsection{AI--Augmented Memory in Organizations and Applied Systems}
Beyond individual cognition, the integration of AI into memory processes has significant implications for
organizations and knowledge-intensive work. Enterprises increasingly rely on digital ecosystems in which AI
systems support information recall, contextual understanding, and knowledge retrieval, effectively functioning as
external, collective memory infrastructures.

AI tools can act as cognitive extensions in professional environments by organizing personal and collective
knowledge into searchable, semantically structured archives. By surfacing prior meeting notes, decision histories,
or relevant research exactly when needed, such systems externalize elements of working memory and help maintain
continuity in fast-paced workflows \citep{Memoro2024RealTimeMemoryAugmentation,Gerlich2025AIToolsSociety}. This
extends cognitive offloading \citep{Sparrow2011GoogleEffect,Risko2016CognitiveOffloading} into organizational
settings where information fragmentation is a constant challenge.

This perspective aligns with work on AI-assisted knowledge management, which emphasizes complementarity between AI
systems and human expertise \citep{Pai2022AIKnowledgeManagement}. AI can rapidly organize and filter information,
but it is most effective when guided by humans who provide context and interpretive insight. Together, they can
sustain an organizational memory that is more comprehensive and accessible than unaided human memory, yet more
flexible and meaningful than data left to accumulate without human interpretation.

\subsection{AI Adoption, Digital Transformation, and Role Reconfiguration}
At the organizational level, AI is increasingly framed as a general-purpose technology that reshapes business
processes, role boundaries, and strategic decision-making rather than as a point solution. Strategy and management
research discusses how AI can be combined with human judgement in organizational decisions, potentially improving
speed and breadth of analysis while creating new coordination and accountability challenges
\citep{TrunkBreitenecker2020HumanAIStrategicDecisionMaking,KamariotouKitsios2021AIBusinessStrategyDigitalTransformation,LiZhao2025TransformingOrganisationsThroughAI}. Industry
reports similarly emphasize that value creation depends on how organizations ``rewire'' processes and capabilities
around AI deployment rather than on model access alone \citep{McKinsey2025StateOfAIRewiring}.

This transformation is visible across business functions, where AI is adopted for customer-facing and back-office
work. Practitioner overviews document AI use in customer support and engagement \citep{Echegu2024AIinCustomerService},
operations management \citep{McGrath2024AITransformingOperationsManagementIBM}, financial services \citep{ChlouverakisKarapanos2024AIReshapingFinancialServices},
scenario analysis in corporate finance \citep{Bailey2025AITransformsScenarioAnalysisCFI}, and human-resource
management practices \citep{NawazAhmad2024AIAdoptionHRMPractices}. In many cases, adoption is paired with automation
technologies such as RPA and workflow tooling, which shift the division of labor by externalizing routine decision
steps into software artifacts \citep{Mangal2023RPAandAIAutomatingBusinessProcesses,EdlichIqbal2018BotsAlgorithmsCorporateSupport}.

At the same time, AI is increasingly treated as a strategic capability for competitive positioning and business
model innovation. Reviews and agendas highlight AI’s role in strategic decision-making and the emergence of new
business-model configurations enabled by algorithmic capabilities \citep{KaggwaFonsah2024AIDecisionMakingBusinessStrategies,Jorzik2024AIDrivenBusinessModelInnovationReview,Atsmon2023AIInStrategyMcKinsey,CorboFountaine2021ReinforcementLearningBusinessCourse,SoniKrishnan2019ImpactAIOnBusinesses}.
Such changes create new demands for organizational learning and training, including ``hands-on'' AI skill
development in business education and professional contexts \citep{Freire2024BusinessEducationHandsOnAI,WangPham2022AILiteracyScale}.

Applied AI--memory systems illustrate how algorithmic tools can function as external supports for encoding,
retrieval, and metacognitive regulation in real-world settings. In healthcare and neurotechnology, AI-guided
stimulation approaches have been shown to enhance episodic recall in clinical contexts
\citep{Kucewicz2023BrainStimulationMemory}. Complementing these intervention-oriented approaches, applied frameworks
also examine how AI can support memory impairments and caregiver decision-making (e.g., dementia care) through
monitoring, prompting, and personalized assistance \citep{Guzzi2023DementiaAIFramework}. In education, intelligent
tutoring systems improve learning outcomes by tailoring practice and feedback
\citep{Ma2014ITSMetaanalysis,Pane2014CognitiveTutorAlgebra,Alshaikh2021ITS}, and active learning improves durable
retention relative to passive reading \citep{Freeman2014ActiveLearning}. Generative AI tutors can leverage these
principles by sustaining reflective dialogue and prompting retrieval and self-explanation \citep{Abrar2025Cognitive}.
In everyday settings, augmented reality interfaces are emerging as a platform for on-demand contextual recall; the
``AR Secretary Agent'' combines wearable AR glasses with an LLM-based assistant to provide real-time memory
augmentation \citep{ElHaddad2025ARSecretaryAgent}.

\section{Product Management as a Decision-Intensive Domain}
\label{sec:pm-decision-domain}
Product management is a decision-intensive role that integrates heterogeneous evidence (user feedback, market
signals, analytics, technical feasibility, and stakeholder constraints) into product choices under uncertainty and
time pressure. In agile settings, product managers coordinate discovery and prioritization work while aligning
cross-functional teams around goals and trade-offs \citep{TkaliRomanova2022AgilePM}. These responsibilities make the
role highly dependent on \emph{external representations}: roadmaps, requirement documents, research summaries, and
internal knowledge bases stabilize collective understanding over time.

From a decision-process perspective, product management can be described as a recurring cycle of (i) problem
framing (defining user needs and business objectives), (ii) option generation (alternative solutions and
feature sets), (iii) prioritization under constraints (engineering capacity, time-to-market, risk), and (iv)
evaluation and iteration after release. Each step relies on artifacts that function as shared memory objects: PRDs,
user stories, sprint backlogs, dashboards, meeting notes, and stakeholder narratives. These artifacts do not merely
store information; they coordinate attention, align mental models across functions, and preserve rationale so that
decisions remain revisitable when assumptions change.

This artifact-centric nature makes product management a sensitive domain for AI-generated external representations.
If AI becomes the first producer of the artifacts that others rely on (summaries of research, synthesized user
feedback themes, competitive analyses, and executive-ready narratives), then AI outputs can shape the initial frame
of the decision problem and thereby influence downstream choices even when humans remain nominally in control. In
this sense, AI adoption in product management is not a narrow productivity add-on; it is a change in the cognitive
infrastructure through which teams remember, justify, and coordinate decisions.

Generative AI is increasingly positioned as an additional representational layer in product-management workflows,
supporting tasks such as summarizing customer feedback, drafting product narratives, and accelerating synthesis
across fragmented information sources \citep{Pinzauti2025ReinventingPM}. These uses are best understood as a mix of
automation (reducing repetitive synthesis work) and augmentation (supporting judgment with condensed
representations). However, they also shift part of the role toward \emph{evaluation and governance} of AI outputs:
product managers may need to detect when AI-generated summaries are incomplete or biased, maintain provenance cues,
and ensure that accountability for decisions remains clear \citep{Pasquale2015BlackBoxSociety}.

Practitioner and industry sources make this shift concrete by describing how AI is inserted into the everyday
artifact pipeline of product work. Guides aimed at product managers emphasize automation of documentation and
coordination tasks (e.g., drafting PRDs, writing user stories, generating meeting summaries, and preparing
stakeholder updates), as well as use cases in discovery and synthesis (e.g., clustering feedback themes, proposing
hypotheses, and generating experiment variants) \citep{Huryn2024AutomateWorkProductManager,Gupta2024AIUseCasesProductManagers,Nest2024AIProductManagementWorkflows}. In software-heavy settings, analyses of generative AI in product
development also highlight potential acceleration of time-to-market and changes in cross-functional collaboration,
as AI-generated artifacts flow between product, design, and engineering teams \citep{GnanasambandamHall2024GenAIAccelerateTimeToMarket}.

These accounts align with a broader business-operations framing in which AI supports both operational efficiency
and strategic exploration \citep{TarafdarMeijer2019AIEnhanceBusinessOperations,Atsmon2023AIInStrategyMcKinsey}. For
product managers, the implication is that AI becomes part of the decision infrastructure: it can influence which
options are generated, which evidence is highlighted, and how trade-offs are communicated, with downstream
implications for responsibility allocation and governance when AI-generated content is reused across the
organization.

The literature often distinguishes between \emph{automation} and \emph{augmentation} as two modes of AI value in
professional work. In product management, automation is typically associated with removing routine workload
(drafting documentation, reporting, ticket triage, and basic synthesis), whereas augmentation is associated with
improving higher-level cognitive work (sensemaking, scenario exploration, strategy articulation). While automation
benefits can be captured through time saved, augmentation benefits depend on whether the representations preserve
key assumptions, uncertainties, and trade-offs that support judgement. This distinction motivates Study~2’s focus on
both perceived efficiency gains and perceived decision-making changes (\Cref{sec:results-study2}).

At the same time, because product decisions are accountable to multiple stakeholders, AI-generated representations
can create organizational risks that mirror cognitive risks. If AI outputs are reused across documents without clear
provenance, errors can propagate and responsibility can diffuse. This makes governance mechanisms (documentation,
traceability, and explicit validation steps) part of the representational system itself rather than an external
compliance layer.

Because product management combines heavy information processing with high-stakes communication and justification,
it provides a natural applied setting for studying how AI-generated representations shape both cognition and
reliance. Competence in this setting plausibly includes \emph{AI literacy}---the ability to interpret, evaluate,
and appropriately use AI outputs rather than treating them as authoritative \citep{WangPham2022AILiteracyScale}.

\section{Synthesis and Research Gap}
Across cognitive and organizational perspectives, prior work converges on a common pattern: external
representations can enhance performance by providing cues and reducing load, but they can also induce cognitive
offloading and source confusion when they displace deep processing
\citep{Craik1972LevelsOfProcessing,Risko2016CognitiveOffloading,Sparrow2011GoogleEffect,Johnson1993SourceMonitoring}.
Generative AI intensifies this trade-off because it produces fluent, transformed representations whose provenance
may be ambiguous and whose errors can be incorporated into memory and judgment
\citep{Chan2024ConversationalAIFalseMemories,Zhai2024OverRelianceAIDialogue}. In organizations, AI-augmented
knowledge management promises continuity and efficiency \citep{Pai2022AIKnowledgeManagement}, yet can create
epistemic dependence and accountability challenges when AI outputs shape professional decisions
\citep{Zhang2024KnowledgeImbalanceAI,Pasquale2015BlackBoxSociety}.

Despite the rapid diffusion of generative AI, two limitations remain common in the current evidence base. First,
many cognitive studies operationalize ``AI assistance'' as a coarse treatment (AI vs no AI) without testing how
representation design features---timing, structure, and provenance cues---shape outcomes. This makes it difficult to
derive actionable design guidance: if outcomes depend on \emph{how} AI is embedded into the task, then binary
comparisons can obscure mechanisms and lead to inconsistent conclusions. Second, performance benefits are often
reported without parallel measurement of epistemic risk (e.g., misinformation acceptance, source confusions, or
miscalibrated reliance), even though these risks may be more consequential in decision settings than small changes in
accuracy.

On the organizational side, the emerging literature often documents adoption, perceived productivity gains, and
general attitudes, but less frequently connects observed practices to cognitive mechanisms. This is a meaningful gap
because organizations do not adopt ``AI'' in the abstract; they adopt representational artifacts (summaries, drafts,
analyses) that change how evidence is encoded, shared, and retrieved in collective work. Without a mechanism-based
link, it remains unclear why some adoption patterns improve decision quality while others introduce fragility,
accountability diffusion, or over-reliance.

A key gap is that these strands are often studied in isolation. Cognitive studies rarely connect interface-level
design choices (timing, structure, provenance cues) to professional reliance patterns, while organizational
accounts rarely link observed AI use to mechanisms such as cue-dependent retrieval and source monitoring. This
thesis addresses that gap with a multi-level design: Study~1 provides causal evidence on how timing and structure
of AI-generated summaries affect memory and epistemic risk in a controlled reading task, while Study~2
characterizes how product managers deploy AI-generated representations in decision-intensive work
\citep{Pinzauti2025ReinventingPM}. The next chapter introduces the \emph{AI Buffer} model as an integrative
framework to connect these levels and to derive propositions for safer, more effective AI-assisted cognition and
decision-making.


\chapter{Conceptual Framework}
\label{ch:conceptual-framework}

This chapter develops the conceptual framework that links the cognitive and organizational levels of analysis in
this thesis. Building on the literature review, it positions AI-generated external representations as a form of
cognitive infrastructure, introduces the AI Buffer Model proposed in this thesis, and then bridges cognitive
mechanisms to decision processes in product management. The chapter concludes with an integrated multi-level
framework and a set of research propositions/hypotheses aligned with RQ1--RQ3.

\section{Positioning: AI-Assisted External Representations as Cognitive Infrastructure}
\label{sec:positioning-cognitive-infrastructure}
Across domains, humans use external representations to stabilize information, reduce memory demands, and coordinate
complex tasks. In cognitive terms, this practice is often described as \emph{cognitive offloading}---the strategic
use of external artifacts to store information or reduce internal processing requirements
\citep{Risko2016CognitiveOffloading}. Digital search technologies provide an emblematic case: when information is
reliably accessible, people may encode less of the content itself and more of the access strategy
\citep{Sparrow2011GoogleEffect}. This does not imply that offloading is inherently harmful; rather, it highlights
that cognition adapts to the structure of the environment and the availability of external resources.

Generative AI extends external representation beyond retrieval by producing \emph{transformed} representations:
summaries, explanations, and recommendations that compress, reorganize, and reframe source material. Compared with
notes or static documents, these representations are often fluent, context-rich, and immediately reusable. As a
result, they can become part of the \emph{cognitive infrastructure} of a task---not merely an output to be read, but
an always-available representational layer that shapes what users attend to, what they encode, and which cues are
available during retrieval. From the perspective of encoding specificity \citep{Tulving1973EncodingSpecificity},
the introduction of a persistent AI representation can change both encoding conditions (what cues are present while
learning) and retrieval conditions (what cues are present when remembering), with direct implications for memory
performance and error.

AI-generated representations also introduce distinctive epistemic risks. When an external representation is
plausible and blends seamlessly with source content, users may later confuse its origin. The Source Monitoring
Framework emphasizes that remembering involves attributing content to its source; when contextual tags are weak,
source confusions and misattributions become more likely \citep{Johnson1993SourceMonitoring}. The risk is amplified
when AI content includes subtle inaccuracies or leading suggestions: generative systems can introduce details that
are later incorporated into a user’s recollection or judgment \citep{Chan2024ConversationalAIFalseMemories}.
Accordingly, AI-assisted external representations should be conceptualized not only as aids, but also as potential
drivers of distortion, over-reliance, and miscalibrated confidence \citep{Zhai2024OverRelianceAIDialogue}.

Finally, the notion of cognitive infrastructure has an organizational dimension. Knowledge-intensive work is
mediated by shared artifacts---documents, roadmaps, dashboards, and decision logs. When AI-generated summaries and
recommendations enter these artifact ecosystems, they can reshape collective memory and decision processes.
Organizations may benefit from improved knowledge continuity and retrieval \citep{Pai2022AIKnowledgeManagement},
while simultaneously facing new challenges of accountability, transparency, and epistemic dependence
\citep{Pasquale2015BlackBoxSociety,Zhang2024KnowledgeImbalanceAI}. This thesis therefore treats AI-assisted external
representations as a multi-level phenomenon: they influence individual cognition (RQ1), organizational practice in
product management (RQ2), and the link between these levels (RQ3).

\section{The AI Buffer Model (Proposed in this Thesis)}
\label{sec:ai-buffer-model}
Classical models of memory conceptualize encoding, storage, and retrieval as primarily internal processes,
regulated by attention, rehearsal, and executive coordination
\citep{Atkinson1968HumanMemory,Baddeley2012WorkingMemory,Tulving1972EpisodicSemantic}. However, contemporary AI
applications increasingly provide persistent, content-based representations that accompany users during cognitive
tasks, introducing an external layer of informational support.

To interpret the cognitive effects of such support, this thesis introduces the \emph{AI Buffer Model} as a
parsimonious conceptual lens. The \textbf{AI Buffer} refers to AI-generated external representations that persist
during a task and can influence memory encoding, retrieval, and metacognitive monitoring. The model does not posit
a new neurocognitive module, nor does it claim to be a universal theory of AI-augmented work. Instead, it
characterizes a \emph{functional role} that AI-generated representations may play in shaping memory-related
behavior under controlled conditions (Study~1) and in decision-intensive work settings (Study~2).

\subsection{Model Components and Design Levers}
The AI Buffer Model distinguishes three elements:
\begin{itemize}
  \item \textbf{Human memory system.} A capacity-limited working-memory workspace and long-term memory stores that
  support encoding, consolidation, and retrieval \citep{Baddeley2012WorkingMemory}.
  \item \textbf{AI-generated representation (the buffer).} A persistent external representation (e.g., a summary)
  that can be consulted during task execution and that provides semantic content and retrieval cues.
  \item \textbf{Task and interface context.} The constraints and interaction structure that govern how the buffer
  is accessed (e.g., timing of availability, integrated vs.\ segmented formatting) and how the user allocates
  attention between sources.
\end{itemize}

In Study~1, two interface-level levers operationalize properties of the AI Buffer:
\begin{itemize}
  \item \textbf{Timing of access:} whether the buffer is available before reading (pre-reading), during reading
  (synchronous), or after reading (post-reading). Timing changes whether the buffer functions as an advance
  organizer, an interruption, or a review aid \citep{Ausubel1960AdvanceOrganizers,Ausubel1968EducationalPsychology}.
  \item \textbf{Structure of presentation:} whether the buffer is presented as an integrated paragraph or as
  segmented bullet points. Structure changes coherence cues and coordination demands, and is expected to affect
  split-attention costs and verification behavior \citep{Sweller1988CognitiveLoad,ChandlerSweller1992SplitAttention}.
\end{itemize}

Other properties (e.g., factual accuracy, provenance cues, and the degree of interactivity) are treated as
important contextual variables, but are held constant or discussed as boundary conditions in the present work.

\subsection{Mechanisms Aligned with RQ1}
RQ1 asks how AI-generated external representations influence core human memory processes. Within the AI Buffer
Model, these influences are expressed through five tightly linked constructs: encoding, recall/recognition, source
monitoring, confidence calibration, and cognitive load.

\paragraph{Memory encoding.}
The buffer can shape encoding by changing what users attend to and how deeply they process the source material.
When the buffer functions as an advance organizer, it may activate schemas and provide a high-level framework that
guides attention toward central propositions \citep{Ausubel1960AdvanceOrganizers,Ausubel1968EducationalPsychology}.
This can promote semantic elaboration and more organized encoding, consistent with Levels of Processing theory
\citep{Craik1972LevelsOfProcessing}. Conversely, when a buffer is perceived as a reliable substitute for internal
encoding, it may encourage shallow processing and cognitive offloading: users may allocate less working-memory
effort to constructing a detailed internal representation because the external representation remains available
\citep{Risko2016CognitiveOffloading,Sparrow2011GoogleEffect}.

\paragraph{Recall and recognition.}
At retrieval, a buffer can serve as a cue source that reinstates aspects of the encoding context. Encoding
specificity predicts better retrieval when cues present at encoding are available at recall
\citep{Tulving1973EncodingSpecificity}. This cueing logic suggests that buffer access can improve recognition of
summary-aligned propositions even if free recall remains unchanged. At the same time, reliance on external cues may
reduce the need to reconstruct information from internal traces, which could weaken detail-rich recall when the
buffer is unavailable or when retrieval requires reconstruction beyond the buffer’s coverage.

\paragraph{Source monitoring.}
Because AI-generated content is fluent and semantically plausible, it can blur boundaries between what was read in
the original text and what was supplied by the buffer. The Source Monitoring Framework predicts greater
misattribution when sources share overlapping content and when contextual cues are weak
\citep{Johnson1993SourceMonitoring}. In AI-assisted settings, this risk extends to both inadvertent inaccuracies and
to structurally plausible but incorrect details. The potential for suggestion-driven errors is supported by
evidence that conversational AI can increase false memories under certain conditions
\citep{Chan2024ConversationalAIFalseMemories}. In the AI Buffer Model, source monitoring is therefore a central
epistemic-risk channel, not a peripheral outcome.

\paragraph{Confidence calibration.}
Confidence is a metacognitive judgment about the reliability of one’s own memory or decision. AI buffers can
increase confidence by providing fluent explanations and by reducing uncertainty during retrieval. However, fluency
is not equivalent to correctness: if the buffer introduces inaccuracies or if the user misattributes buffered
content to the source, confidence can become miscalibrated. This is especially problematic when high confidence
drives uncritical reliance on AI-generated representations \citep{Zhai2024OverRelianceAIDialogue}.

\paragraph{Cognitive load.}
The buffer can reduce load by compressing information and by offering structured cues, but it can also add load by
creating split attention and task-switching demands. Cognitive load theory predicts that learning is constrained
by working-memory capacity and that formats imposing extraneous coordination demands reduce learning efficiency
\citep{Sweller1988CognitiveLoad}. The split-attention effect predicts that separated information sources require
extra integration effort, whereas integrated formats reduce this burden
\citep{ChandlerSweller1992SplitAttention}. Accordingly, a segmented summary may increase coordination costs relative
to an integrated summary, while synchronous access may introduce interruption costs during reading.

\subsection{Boundary Conditions and Individual Differences}
The AI Buffer Model also highlights boundary conditions that shape whether external representations augment or
replace cognition. Prior knowledge can reduce reliance on the buffer because users can evaluate and integrate
information more independently. Conversely, high time pressure or low familiarity can increase reliance because
verification costs are higher. At the organizational level, AI literacy---the ability to interpret, evaluate, and
appropriately use AI outputs---is expected to moderate reliance behaviors and perceived benefits
\citep{WangPham2022AILiteracyScale}. These factors are relevant for both Study~1 (as individual differences) and
Study~2 (as professional competencies and organizational practices).

\section{From Cognitive Mechanisms to Decision Processes (Bridge)}
\label{sec:bridge-cognition-decisions}
While RQ1 focuses on memory mechanisms, knowledge-intensive work is ultimately evaluated through decisions and
actions. Many professional decisions require (i) retrieving relevant evidence from memory and artifacts, (ii)
integrating that evidence into a coherent situation model, and (iii) communicating a rationale that is defensible
to stakeholders. AI-generated external representations can influence each step by changing what information is
available, how it is trusted, and how it is justified.

The AI Buffer Model bridges cognition to decisions through three pathways.

\textbf{(1) Evidence selection and salience.} By compressing and reordering information, the buffer makes certain
propositions more salient than others. This can improve efficiency (faster access to core points), but also
introduce representational bias: what is summarized and how it is framed can shape which evidence is considered.
When users offload heavily, the buffer becomes a primary evidence source rather than a secondary aid
\citep{Risko2016CognitiveOffloading,Sparrow2011GoogleEffect}.

\textbf{(2) Epistemic control and verification.} Decisions in organizations often require traceability: the ability
to justify a choice by pointing to sources. Source monitoring failures can therefore become governance failures.
If users misattribute AI content to authoritative sources, decision rationales may be built on unverified claims.
This risk is magnified when AI systems are opaque or when their reasoning is difficult to audit
\citep{Pasquale2015BlackBoxSociety}.

\textbf{(3) Confidence-driven reliance.} Confidence affects whether people seek additional information, challenge a
recommendation, or defer to an automated output. If AI buffers increase confidence without improving accuracy,
they can encourage over-reliance. At scale, this can produce a ``knowledge imbalance'' in AI-advised decision-making
where decision-makers become dependent on representations they cannot fully evaluate
\citep{Zhang2024KnowledgeImbalanceAI}.

These pathways suggest that cognitive outcomes (encoding depth, source monitoring, confidence calibration, and
load) should be interpreted not only as individual-level effects, but also as mechanisms that can explain changes
in professional decision processes.

\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.15}
\begin{tabularx}{\textwidth}{l X X}
\hline
\textbf{Cognitive mechanism (RQ1)} & \textbf{Decision-process implication} & \textbf{Organizational risk / design need} \\
\hline
Offloading and cue-dependent retrieval & Faster access to ``good enough'' evidence; less internal reconstruction & Over-reliance under time pressure; need for verification norms \\
Source monitoring and misinformation & Misattribution of AI claims as authoritative sources & Accountability failures; need for provenance cues and audit trails \\
Confidence calibration & Greater decisiveness; reduced information-seeking when confidence is high & Overconfidence in flawed outputs; need for calibration feedback \\
Cognitive load and split attention & Reduced effort when summaries compress information; increased effort when switching sources & Interface design matters (integration, timing); need to minimize extraneous load \\
\hline
\end{tabularx}
\caption{How AI-assisted memory mechanisms can translate into decision-process dynamics.}
\label{tab:cognition-to-decisions}
\end{table}

\section{Organizational Layer: AI Adoption in Product Management}
\label{sec:org-layer-pm}
RQ2 focuses on product management as a decision-intensive domain in which AI adoption is expected to reshape both
work practices and responsibility structures. Product managers routinely integrate heterogeneous evidence---user
research, market signals, analytics, technical feasibility, and stakeholder constraints---into prioritization and
strategy under uncertainty and time pressure. In agile settings, they coordinate discovery and delivery while
aligning cross-functional teams around goals and trade-offs \citep{TkaliRomanova2022AgilePM}. These responsibilities
depend heavily on external representations: roadmaps, PRDs, experiment results, meeting notes, and decision logs
stabilize shared understanding and enable coordination across time.

Generative AI tools are increasingly positioned as an additional representational layer in these workflows
\citep{Pinzauti2025ReinventingPM}. Typical uses include summarizing customer feedback, drafting product narratives,
accelerating synthesis across fragmented sources, and supporting decision preparation (e.g., ``pros/cons'' lists,
alternatives, and risk summaries). These uses combine \emph{automation} (reducing repetitive synthesis work) and
\emph{augmentation} (supporting judgment with condensed representations). However, they also shift part of the PM
role toward evaluation and governance of AI outputs: product managers must detect when AI-generated summaries are
incomplete or biased, maintain provenance cues, and ensure that accountability for decisions remains clear
\citep{Pasquale2015BlackBoxSociety}.

The organizational layer therefore foregrounds three constructs that parallel the cognitive layer:
\begin{itemize}
  \item \textbf{Workflow reconfiguration:} which tasks are delegated to AI representations and which remain
  human-led.
  \item \textbf{Role configuration and skill shift:} movement toward strategic, interpretive, and oversight
  responsibilities; increasing importance of AI literacy \citep{WangPham2022AILiteracyScale}.
  \item \textbf{Governance and perceived responsibility:} who is accountable for AI-influenced decisions, and what
  oversight practices are adopted to prevent uncritical acceptance and mitigate bias \citep{Zhang2024KnowledgeImbalanceAI}.
\end{itemize}

Finally, adoption is expected to be context-sensitive. Organizational maturity, regulatory expectations, and
cultural norms can shape how much AI is used, which tasks are automated versus augmented, and how governance is
implemented. This motivates explicit cross-context comparison within Study~2 (e.g., USA--China) as part of RQ2’s
scope.

\section{Integrated Multi-Level Framework (Cognitive $\rightarrow$ Organizational)}
\label{sec:integrated-framework}
The core contribution of this thesis is an integrated framework that connects cognitive mechanisms of AI-assisted
memory (RQ1) to organizational decision-making practices in product management (RQ2), and uses the former to
explain the latter (RQ3).

At the \textbf{cognitive level}, the AI Buffer Model specifies how AI-generated external representations shape
encoding, retrieval, source monitoring, confidence calibration, and cognitive load. These mechanisms determine not
only whether people remember more or less, but also what \emph{kind} of knowledge is retained (gist vs.\ detail),
how confidently it is held, and whether its source is correctly attributed.

At the \textbf{decision-process level}, these cognitive outcomes translate into observable behaviors: how evidence
is selected, how options are generated and evaluated, and how rationales are communicated. For example, cue-driven
recognition may speed up decision preparation, whereas source confusions can undermine traceability and learning
from past decisions.

At the \textbf{organizational level}, repeated use of AI-generated representations can stabilize into norms of
reliance, documentation, and accountability. Over time, teams may increasingly treat AI outputs as default inputs
to decisions, potentially creating epistemic dependence and shifting expertise toward those who can evaluate and
govern AI systems \citep{Zhang2024KnowledgeImbalanceAI}. In parallel, organizations may invest in knowledge
management practices that embed AI into collective memory systems \citep{Pai2022AIKnowledgeManagement}.

Crucially, the framework includes \textbf{feedback loops}. Organizational norms and training influence individual
reliance and verification behavior (e.g., AI literacy and governance practices), while individual experiences of
accuracy, effort reduction, and error influence organizational adoption decisions. These loops imply that AI’s
impact is not static: the same tool can produce different outcomes depending on governance maturity, interface
design, and task context.

\section{Research Propositions / Hypotheses}
\label{sec:propositions-hypotheses}
This section formalizes testable expectations derived from the conceptual framework. Hypotheses are stated for the
controlled experiment (Study~1; RQ1), while propositions are stated for the organizational analysis (Study~2; RQ2)
and the cross-study integration (RQ3).

\subsection{Study 1 (RQ1): Cognitive-Level Hypotheses}
\label{sec:hypotheses-study1}
\begin{enumerate}
  \item \textbf{H1 (AI Buffer vs.\ No-AI; recall/recognition).} Relative to unaided reading, access to an AI Buffer
  (operationalized as an AI-generated summary) is expected to improve recognition performance for core
  propositions, with weaker or no improvement expected for free recall quality.

  \item \textbf{H2 (Timing; encoding scaffold vs.\ interruption).} Pre-reading access to the AI Buffer is expected
  to yield stronger encoding and higher recognition accuracy than synchronous or post-reading access, because it
  can function as an advance organizer rather than as an interruption or a post-hoc reminder
  \citep{Ausubel1960AdvanceOrganizers,Ausubel1968EducationalPsychology}.

  \item \textbf{H3 (Structure; split-attention and coherence).} Integrated-paragraph buffers are expected to reduce
  split-attention costs and support more coherent mental-model construction than segmented bullet-point buffers,
  resulting in higher recall quality and/or recognition accuracy \citep{Sweller1988CognitiveLoad,ChandlerSweller1992SplitAttention}.

  \item \textbf{H4 (Epistemic risk; source monitoring).} The presence of an AI Buffer is expected to increase
  vulnerability to source-monitoring errors and false recognitions for plausible but non-present details. This
  risk is expected to be higher under segmented structure than integrated structure, because segmentation can
  promote decontextualized processing and weaker provenance cues \citep{Johnson1993SourceMonitoring}.

  \item \textbf{H5 (Confidence calibration).} AI Buffer access is expected to increase subjective confidence in
  memory judgments. However, confidence may be less well calibrated to accuracy under high reliance conditions,
  especially for buffer-consistent false lures \citep{Zhai2024OverRelianceAIDialogue}.
\end{enumerate}

\subsection{Study 2 (RQ2): Organizational-Level Propositions}
\label{sec:propositions-study2}
\begin{enumerate}
  \item \textbf{P1 (Role transformation).} AI adoption in product management is expected to shift effort away from
  routine synthesis and coordination toward more strategic, interpretive, and oversight activities
  \citep{Pinzauti2025ReinventingPM}.

  \item \textbf{P2 (Decision-support centrality).} AI-generated external representations are expected to become
  central inputs to decision preparation and communication (e.g., summarizing evidence, framing options, and
  drafting rationale), increasing the speed of decision cycles while changing how evidence is selected and
  narrated \citep{Pinzauti2025ReinventingPM}.

  \item \textbf{P3 (Governance and accountability).} As AI outputs influence product decisions, perceived
  responsibility for verification, transparency, and ethical oversight is expected to increase, and governance
  mechanisms are expected to become more salient in PM practice \citep{Pasquale2015BlackBoxSociety}.

  \item \textbf{P4 (AI literacy as a capability).} Higher AI literacy is expected to be associated with more
  calibrated reliance and more effective integration of AI into PM workflows \citep{WangPham2022AILiteracyScale}.

  \item \textbf{P5 (Contextual moderation).} Adoption patterns, perceived responsibility, and governance emphasis
  are expected to vary across organizational and cultural contexts (e.g., USA--China), reflecting differences in
  norms, constraints, and maturity of AI deployment \citep{Pinzauti2025ReinventingPM}.
\end{enumerate}

\subsection{Cross-Study Integration (RQ3): Integrative Propositions}
\label{sec:propositions-rq3}
\begin{enumerate}
  \item \textbf{I1 (Offloading $\rightarrow$ reliance norms).} When AI Buffers reliably reduce cognitive effort,
  users and teams are expected to offload more frequently, which can scale into organizational norms of reliance
  under time pressure \citep{Risko2016CognitiveOffloading,Zhang2024KnowledgeImbalanceAI}.

  \item \textbf{I2 (Source monitoring $\rightarrow$ accountability).} Source-monitoring difficulties at the
  individual level are expected to manifest as accountability and traceability challenges at the organizational
  level, increasing the need for provenance cues, documentation, and verification practices
  \citep{Johnson1993SourceMonitoring,Pasquale2015BlackBoxSociety}.

  \item \textbf{I3 (Confidence calibration $\rightarrow$ trust and escalation).} Miscalibrated confidence in AI
  representations is expected to contribute to over-trust and insufficient escalation or verification, motivating
  organizational interventions that improve calibration (training, feedback, and auditability)
  \citep{Zhai2024OverRelianceAIDialogue}.
\end{enumerate}

\chapter{Methodology and Research Design}
\label{ch:methodology}

This chapter describes the research design and methods used to answer the three research questions introduced in
\Cref{sec:research_questions}. The thesis adopts a multi-level approach: Study~1 provides controlled evidence on
how AI-generated external representations affect memory mechanisms (RQ1), Study~2 provides field evidence on how
AI adoption reshapes decision practices and perceived responsibility in product management (RQ2), and the two are
integrated conceptually and analytically to explain organizational patterns through cognitive mechanisms (RQ3).

\section{Research Design Overview}
\label{sec:research-design-overview}
The central methodological challenge of this thesis is that the phenomenon of interest is inherently
multi-level. At the individual level, AI-generated representations can change attention allocation, encoding
depth, cue availability, and source monitoring, with downstream effects on recall, recognition, confidence, and
perceived cognitive load. At the organizational level, the same representational layer can become embedded in
workflows, shaping how evidence is synthesized, how decisions are justified, and how responsibility is distributed
in practice. No single method is sufficient to capture both levels with high validity.

Accordingly, the thesis combines two complementary designs:
\begin{itemize}
  \item \textbf{Internal validity (Study~1).} A controlled experiment isolates the causal effect of AI-generated
  summaries as an external representation by manipulating key design properties of the AI Buffer (timing and
  structure) under matched task constraints.
  \item \textbf{Ecological validity (Study~2).} A field study captures how AI tools are adopted and governed in a
  real managerial role (product management), including role configuration, decision support practices, and
  perceived accountability.
\end{itemize}

The two studies are linked by a shared conceptual object: \emph{AI-assisted external representations}. In Study~1,
this object is operationalized as pre-generated summaries that persist during a reading-and-memory task. In
Study~2, the object is operationalized as generative AI tools that produce summaries, analyses, and drafts within
product-management workflows. The conceptual bridge is provided by the AI Buffer Model introduced in
\Cref{sec:ai-buffer-model}.

\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.15}
\begin{tabularx}{\textwidth}{l l X X}
\hline
\textbf{Component} & \textbf{RQ} & \textbf{Design and unit of analysis} & \textbf{Primary measurements} \\
\hline
Study 1 & RQ1 & Controlled experiment; individuals performing a timed reading-and-memory task & Free recall, recognition, false-lure endorsement/source monitoring, confidence ratings, cognitive-load ratings, behavioral logs \\
Study 2 & RQ2 & Field study; product managers reporting AI adoption and decision practices in context & AI tool use, workflow shifts, decision-support patterns, governance/ethics and perceived responsibility, USA--China comparison, open-ended responses \\
Integration & RQ3 & Cross-study mapping (mechanisms $\rightarrow$ practices) & Mechanism-based explanation of observed organizational patterns and boundary conditions \\
\hline
\end{tabularx}
\caption{Multi-level research design linking RQ1--RQ3.}
\label{tab:research-design-overview}
\end{table}

\section{Study 1: Controlled Experiment (Cognitive Level -- RQ1)}
\label{sec:study1-method}
Study~1 answers RQ1: \emph{How do AI-generated external representations influence core human memory processes during
knowledge-intensive tasks?} The study uses a controlled reading paradigm in which an AI-generated summary
constitutes a persistent external representation (an AI Buffer) that participants can consult before, during, or
after reading. The design targets five operational dimensions aligned with RQ1: memory encoding, recall and
recognition, source monitoring, confidence calibration, and cognitive load.

\subsection{Participants and Setting}
Thirty-six adults were retained for analysis (\(N_{\mathrm{AI}}=24\), \(N_{\mathrm{NoAI}}=12\)), including university
students and early-career professionals. Participants were 18--35 years old (\(M=24.33\), \(SD=3.83\)) and were
balanced overall by gender (18 women, 18 men). All participants reported Chinese as their native language.

The study was delivered in a browser-based environment. Participant-facing materials were presented in Simplified
Chinese to match participants’ native language, while English descriptions are used in this thesis for exposition.

\begin{table}[t]
    \centering
    \small
    \caption{Participant demographics by group (Study~1).}
    \label{tab:participant-demographics}
    \begin{tabular}{lccc}
        \toprule
        & AI (\(n=24\)) & No-AI (\(n=12\)) & Total (\(N=36\)) \\
        \midrule
        Age, \(M\) (\(SD\)) & 24.50 (4.31) & 24.00 (2.76) & 24.33 (3.83) \\
        Age range & 18--35 & 20--30 & 18--35 \\
        Gender (F/M) & 13/11 & 5/7 & 18/18 \\
        Native language (Chinese), \(n\) (\%) & 24 (100\%) & 12 (100\%) & 36 (100\%) \\
        \bottomrule
    \end{tabular}
\end{table}

Participants received a base compensation of 60 RMB plus a performance-based bonus of up to 60 RMB linked to
recognition-test accuracy.

\subsection{Experimental Design and Conditions}
AI assistance was operationalized as an AI-generated summary of each article. Within the AI-assisted sample, the
core design forms a \(2 \times 3\) matrix:
\begin{itemize}
  \item \textbf{Summary structure (between-subjects):} integrated paragraph vs.\ segmented bullet points.
  \item \textbf{Summary timing (within-subjects):} pre-reading vs.\ synchronous during reading vs.\ post-reading.
\end{itemize}
A separate No-AI baseline group provides an external comparison but does not include the timing manipulation,
yielding an asymmetric mixed design that balances experimental control and feasibility.

\paragraph{Structure manipulation.}
The integrated condition presents the buffer as a coherent paragraph-style summary. The segmented condition
presents the same informational content as a set of bullet points. The manipulation targets coherence cues and
coordination demands: integrated presentation should reduce split-attention and verification costs relative to
segmented presentation \citep{Sweller1988CognitiveLoad,ChandlerSweller1992SplitAttention}.

\paragraph{Timing manipulation.}
Timing determines whether the buffer functions primarily as an encoding scaffold (pre-reading), as an interruption
and on-demand aid during comprehension (synchronous), or as a review cue after initial encoding (post-reading).
This manipulation is theoretically grounded in advance-organizer logic and interruption/switching-cost accounts
\citep{Ausubel1960AdvanceOrganizers,Ausubel1968EducationalPsychology}.

\subsection{Materials and Task Procedure}
\subsubsection{Materials}
\paragraph{Texts.}
Participants read three expository articles of approximately 1,300 words each (topics included urban heat islands,
CRISPR gene editing, and semiconductor supply chains). Articles were selected to be comparable in difficulty and
conceptual density and were administered in Simplified Chinese using fixed, pre-generated stimulus content. English
descriptions are used throughout the thesis for exposition, and study materials are provided in the Appendices.

\paragraph{AI summaries (structure and incompleteness).}
Summaries were generated and refined \emph{in advance} and were approximately 250 words. They were identical in
informational content across the two AI structure conditions, differing only in formatting (integrated paragraph vs.\
segmented bullets). Summaries were intentionally incomplete (approximately 15--20\% of article information omitted)
to discourage reliance on summaries alone and to ensure that some recognition items required article-based encoding.

\paragraph{False lures.}
To assess susceptibility to misinformation and source-monitoring errors, each article’s summary contained two
plausible but incorrect statements (\emph{false lures}). These lure concepts also appeared as distractor options in
the recognition test; selecting them was counted as false-lure acceptance.

\subsubsection{Platform and implementation}
The experiment was delivered via a custom browser-based application with a Python (Flask) backend and HTML/CSS/JavaScript
front-end pages. The platform was designed to support (i) strict enforcement of timing constraints across
conditions, (ii) automated counterbalancing and randomization, and (iii) fine-grained behavioral logging of reading
and summary exposure. All stimuli (articles, summaries, and MCQs) were preloaded, ensuring identical content across
participants and removing runtime variability due to on-the-fly AI generation.

\subsubsection{Procedure and timing constraints}
Participants completed three article blocks. In each block, participants encountered the summary according to their
assigned timing condition, read the article under a fixed time window, and then completed memory assessments and
post-block ratings. Article order was randomized. For AI participants, the mapping between article and timing
condition was counterbalanced through permutations so that each participant experienced each timing condition exactly
once.

\begin{table}[t]
    \centering
    \small
    \setlength{\tabcolsep}{5pt}
    \begin{tabularx}{\textwidth}{l l X l}
        \toprule
        \textbf{Condition} & \textbf{Summary timing} & \textbf{Summary access} & \textbf{Reading cap} \\
        \midrule
        No-AI & -- & None & 15 min \\
        Pre-reading & Before reading & Separate page, $\leq$ 3 min (continue early allowed) & 12 min \\
        Synchronous & During reading & On-demand open/close during reading & 15 min \\
        Post-reading & After reading & Separate page, $\leq$ 3 min (continue early allowed) & 12 min \\
        \bottomrule
    \end{tabularx}
    \caption{Implementation of timing conditions and exposure windows (Study~1).}
    \label{tab:timing-implementation}
\end{table}

After each reading phase, a short enforced break preceded testing to reduce immediate carryover and to standardize
the transition to retrieval. Free recall lasted up to 5 minutes and the MCQ block lasted up to 7 minutes, with
automatic submission at timeout. Back-navigation was disabled to prevent revisiting prior pages or materials.

\subsection{Measures and Operationalization of RQ1}
Study~1 operationalizes RQ1 through complementary behavioral, performance, and self-report measures aligned with the
AI Buffer Model (\Cref{sec:ai-buffer-model}).

\paragraph{Free recall (generative retrieval).}
After reading each article, participants produced a written free recall. Free recall was scored as an index of
episodic--semantic reconstruction of expository text rather than verbatim reproduction. In line with constructive
memory theory \citep{Bartlett1932Remembering}, responses were evaluated for fidelity to the original conceptual
structure (and penalized for plausible distortions), not for length or fluency.

Each recall received a single ordinal score from 0 to 10 based on five rubric dimensions: (i) factual accuracy,
(ii) mechanistic reconstruction, (iii) structural alignment, (iv) specificity vs.\ gist, and (v) source-monitoring
integrity (penalizing false-lure intrusions). To improve scoring consistency, anchor bands were used (9--10 high
fidelity; 7--8 mostly accurate; 5--6 gist-level; 3--4 headline-level; 1--2 fragmentary).

To reduce variability associated with qualitative rating, free-recall responses were initially scored using an LLM
rater applying the pre-specified rubric, and all scores were subsequently reviewed by a human rater (100\% of
responses) to confirm rubric adherence and correct questionable cases.

\paragraph{Recognition (cue-supported retrieval).}
Recognition was measured as proportion correct on 14 multiple-choice items per article. Items were categorized by
information source (summary-sourced vs.\ article-only) to separate recognition of summary-covered information from
comprehension of the underlying text. Misinformation susceptibility was indexed by the number of false-lure options
selected (0--2 per article; 0--6 overall).

\paragraph{Source monitoring and epistemic risk.}
Source monitoring is assessed primarily through false-lure endorsement and related misattributions, motivated by the
Source Monitoring Framework \citep{Johnson1993SourceMonitoring} and evidence that fluent AI interaction can amplify
false memories under some conditions \citep{Chan2024ConversationalAIFalseMemories}.

\paragraph{Confidence and cognitive load.}
Participants provided recall confidence and post-block ratings of mental effort and perceived difficulty using 7-point
scales. Confidence calibration is analyzed by relating confidence to accuracy, testing whether AI assistance
increases confidence without commensurate improvement in correctness.

\paragraph{Trust, dependence, and behavioral logs (process measures).}
AI-group participants provided baseline (general) measures of AI trust and AI dependence and post-block (state)
ratings of trust and dependence. Behavioral measures were computed from server logs, including reading time, summary
view time, number of summary openings, and interaction timestamps. These process indicators are used to triangulate
how timing and structure shape reliance and attention allocation.

\subsection{Data Quality and Exclusion Logic}
To preserve internal validity, sessions were excluded prior to analysis if participants failed to complete the full
protocol or showed clear non-compliance (e.g., implausibly short reading times indicating rushing or non-substantive
responses). The resulting dataset is structured at the block level (participant \(\times\) article), yielding 108
block-level observations (36 participants $\times$ 3 articles). This structure supports within-participant
comparisons across timing conditions in the AI group while retaining a No-AI baseline for reference.

\section{Study 2: Field Study on Product Managers (Organizational Level -- RQ2)}
\label{sec:study2-method}
Study~2 answers RQ2: \emph{How does the adoption of AI tools shape decision-making practices, role configuration,
and perceived responsibility in product management?} The study uses a cross-national field survey to capture how
product managers incorporate AI tools into their workflow, how they perceive role transformation and skill shifts,
and how they understand governance and ethical responsibility when AI influences decisions.

\subsection{Design and Sample}
The field study is based on a structured survey administered in two languages (English and Chinese) to support
participation across contexts. The final dataset comprises 74 validated responses, evenly split across the United
States (\(n=37\)) and China (\(n=37\)). This balanced structure enables both (i) estimation of overall patterns of
AI adoption in product management and (ii) a comparative lens on how adoption and responsibility perceptions vary
across contexts (USA--China), directly addressing the cross-context component of RQ2.

The focus on the United States and China is motivated by their central roles in the global AI ecosystem and by
differences in organizational environments that may shape adoption (e.g., governance norms, tool ecosystems, and
data-driven practices). The comparative design is therefore not treated as a mere ``case split'', but as a way to
identify which adoption patterns are robust across contexts and which patterns are context-contingent. This logic
directly supports RQ2 and also provides a foundation for the integrative explanation in RQ3.

Responses were screened for completeness and coherence (e.g., removing incomplete submissions and invalid patterns).
The thesis treats the resulting dataset as an analytic snapshot of AI adoption in a decision-intensive managerial
role rather than as a representative census of all product managers.

\subsection{Survey Structure and Measurement Strategy}
The survey combines Likert-scale items, multiple-choice items, and open-ended questions. This combination is
intentional: structured items quantify adoption patterns and perceived impacts, while open-ended responses capture
contextual nuance (e.g., examples of AI-supported decisions, perceived risks, and organizational constraints).

At a high level, the instrument is organized around four measurement domains aligned with RQ2:
\begin{itemize}
  \item \textbf{AI tool usage and task coverage:} frequency and type of AI applications used in daily PM work,
  including automation tools, analytics, and generative AI for drafting and synthesis.
  \item \textbf{Role configuration and workflow reallocation:} perceived changes in responsibilities, time saved
  through AI support, and whether time is reallocated toward more strategic work.
  \item \textbf{Skill shift and AI literacy:} perceived need for new competencies (e.g., interpreting AI outputs,
  prompt practices, data literacy) and organizational support for upskilling.
  \item \textbf{Governance, ethics, and responsibility:} perceptions of accountability, transparency, bias
  management, and customer-trust implications when AI informs decisions or product features.
\end{itemize}

Operationally, the survey is structured in four sections (A--D). Section A measures AI tool usage and organizational
adoption context (e.g., which tools are used, who drives adoption, and what goals motivate integration). Section B
measures workload reduction, tasks that become more complex due to AI, and how time savings are reinvested (e.g.,
strategic planning, innovation). Section C focuses on skill development, confidence, and training resources. Section
D focuses on governance and ethics (e.g., responsibility for AI-mediated decisions, guideline structures, and ethical
concerns). This structure is aligned with the conceptual framework’s organizational layer and supports both
descriptive analysis and cross-context comparisons.

In terms of measurement design, Section A combines (i) frequency scales for distinct AI application categories
(e.g., automation and workflow tools, analytics, NLP-based feedback analysis, generative AI drafting tools, and
chatbots) with (ii) adoption-maturity items (e.g., duration of use) and (iii) open-ended prompts for tool names and
examples. Sections B--D use a mix of Likert-scale judgments and categorical items to capture both intensity (how
strongly an impact is perceived) and structure (who owns responsibility, whether guidelines exist, what forms of
training are available). This combination supports the thesis’s interpretation that AI adoption is not only a level
of usage, but a configuration of practices and governance structures \citep{Pinzauti2025ReinventingPM}.

\section{Data Collection Instruments and Materials}
\label{sec:instruments-materials}
Across both studies, the thesis uses instruments designed to capture behavior, judgment, and subjective
experience around AI-generated external representations.

\paragraph{Study 1 instruments (experiment).}
Study~1 uses (i) expository reading materials, (ii) AI-generated summaries in two presentation formats (integrated
vs.\ segmented), (iii) a free-recall prompt, (iv) a recognition test per article including false-lure distractors,
and (v) post-block rating scales for perceived cognitive load and confidence. The experimental platform records
fine-grained interaction logs (e.g., time spent reading and time spent consulting the summary) to provide
behavioral indicators of reliance.

\paragraph{Study 2 instruments (field survey).}
Study~2 uses a bilingual structured survey comprising Likert-scale items, multiple-choice items, and open-ended
questions. The instrument measures frequency of AI use across task categories, perceived role transformation,
time savings and reallocation, skill development and training, and governance/ethics perceptions. The bilingual
format is intended to reduce language barriers and support valid cross-context comparison.

\paragraph{Integration materials.}
For RQ3, the key ``instrument'' is the mapping logic specified by the conceptual framework in
\Cref{ch:conceptual-framework}. Cognitive constructs from Study~1 (offloading, cue-dependent retrieval, source
monitoring, confidence calibration, cognitive load) are used as explanatory lenses for interpreting adoption and
governance patterns observed in Study~2.

\section{Ethical Considerations}
\label{sec:ethics}
Both studies involve minimal-risk procedures and follow standard research ethics principles: voluntary
participation, informed consent, the right to withdraw, and privacy-preserving data handling.

\paragraph{Study 1 (experiment).}
Participants were informed about the study procedures, data collection, and compensation, and provided consent
before participation. Personal identifiers were not stored with behavioral data. Because the experiment includes
plausible but incorrect statements in AI summaries to test source monitoring and misinformation susceptibility,
the study requires careful debriefing: participants should be informed after participation that some summary
content was intentionally inaccurate for research purposes, to avoid leaving lasting misconceptions.

\paragraph{Study 2 (field survey).}
Survey participation was voluntary and responses were collected and analyzed in aggregated form. The survey was
designed to avoid collection of sensitive corporate information (e.g., proprietary product details). Any
open-ended responses are treated as confidential and are reported only in anonymized, non-identifying form.

\paragraph{Data management.}
Across studies, datasets are stored in access-controlled locations and are used only for research purposes.
Reporting focuses on patterns rather than on identifying individuals or organizations, consistent with the thesis’s
goal of developing generalizable insights about AI-assisted cognition and decision-making.

\chapter{Data Analysis}
\label{ch:data-analysis}

This chapter specifies how data from Study~1 (controlled experiment; RQ1) and Study~2 (field study on product
managers; RQ2) are transformed into analytic constructs, prepared for analysis, and analyzed. It also defines the
cross-study integration logic used to address RQ3 and summarizes how validity, reliability, and robustness are
handled across the full thesis.

\section{Constructs and Operationalization}
\label{sec:constructs-operationalization}
The guiding principle for operationalization is alignment with the three research questions. RQ1 requires
measuring memory mechanisms under AI-assisted external representations (the AI Buffer). RQ2 requires measuring how
AI adoption reshapes decision practices and perceived responsibility in product management. RQ3 requires a mapping
between the constructs: organizational patterns are interpreted through cognitive mechanisms.

\subsection{Study 1 Constructs (RQ1)}
Study~1 operationalizes the AI Buffer as an AI-generated summary and defines experimental conditions via two design
factors:
\begin{itemize}
  \item \textbf{AI Buffer presence (between):} No-AI baseline vs.\ AI-assisted.
  \item \textbf{AI Buffer properties (AI participants):} \emph{timing} (pre-reading, synchronous, post-reading) and
  \emph{structure} (integrated paragraph, segmented bullet points).
\end{itemize}

Study~1 outcomes map directly to the RQ1 components:
\begin{itemize}
  \item \textbf{Recall and recognition.} Free-recall responses capture reconstruction from internal traces, while
  recognition items capture cue-supported memory performance.
  \item \textbf{Source monitoring.} False-lure endorsement in recognition provides an index of epistemic risk and
  misattribution under AI assistance, motivated by source-monitoring theory \citep{Johnson1993SourceMonitoring}.
  \item \textbf{Confidence calibration.} Confidence ratings are used to assess whether AI assistance increases
  confidence and whether confidence aligns with accuracy (calibration).
  \item \textbf{Cognitive load.} Post-block ratings of mental effort and perceived difficulty operationalize
  subjective cognitive load under each condition.
  \item \textbf{Behavioral reliance (supporting indicator).} Platform logs (e.g., time spent viewing the summary,
  number of openings, and reading time) operationalize reliance behavior and exposure to the AI Buffer.
\end{itemize}

\subsection{Study 2 Constructs (RQ2)}
Study~2 operationalizes product-management adoption of AI tools through survey measures that cover four domains:
\begin{itemize}
  \item \textbf{AI usage and task coverage.} Frequency and types of AI tools used in daily PM work (automation,
  analytics, generative drafting/synthesis, decision support).
  \item \textbf{Workflow and role configuration.} Perceived task reallocation and role transformation (e.g., shift
  toward strategic work; changes in coordination, analysis, and leadership responsibilities).
  \item \textbf{Perceived responsibility and governance.} Accountability, transparency, oversight practices,
  fairness/bias concerns, and customer-trust implications when AI informs product decisions
  \citep{Pasquale2015BlackBoxSociety}.
  \item \textbf{Context and moderation.} Region (USA vs.\ China) as a key grouping variable, allowing cross-context
  comparison aligned with RQ2’s comparative component \citep{Pinzauti2025ReinventingPM}.
\end{itemize}

When multiple items measure a shared latent concept (e.g., perceived responsibility or AI literacy), items are
combined into composite indices by averaging standardized item scores, subject to internal-consistency checks (see
\Cref{sec:validity-reliability-robustness}).

\subsection{Cross-Study Constructs (RQ3)}
To answer RQ3, Study~2 patterns are interpreted through the cognitive mechanisms measured in Study~1. The mapping
uses the AI Buffer Model constructs from Chapter~3:
\begin{itemize}
  \item \textbf{Offloading and cue-dependent retrieval} (effort substitution and reliance norms).
  \item \textbf{Source monitoring and epistemic risk} (traceability and accountability).
  \item \textbf{Confidence calibration} (trust and escalation behavior).
  \item \textbf{Cognitive load and split attention} (interface costs and workflow friction).
\end{itemize}

\section{Data Preparation and Quality Checks}
\label{sec:data-prep}
Data preparation follows two goals: (i) ensure the dataset represents compliant task execution and valid responses,
and (ii) ensure constructs reflect the intended operational definitions.

\subsection{Study 1 Data Preparation}
Study~1 data are structured at the \textbf{block level} (participant \(\times\) article), enabling within-person
comparisons across timing conditions for AI participants while retaining a No-AI baseline group.

Preparation includes:
\begin{itemize}
  \item \textbf{Session completeness.} Exclude incomplete sessions and blocks with missing core outcomes (recall or
  recognition).
  \item \textbf{Compliance screening.} Flag and exclude clearly non-compliant behavior (e.g., implausibly short
  reading time suggesting rushing, non-substantive recall text).
  \item \textbf{Derived variables.} Compute block-level indices: recall score, recognition accuracy, false-lure
  endorsement count, confidence measures, cognitive-load ratings, and behavioral reliance metrics from logs.
  \item \textbf{Randomization checks.} Verify balance in article assignment and timing order (AI group), and confirm
  that key participant characteristics do not differ systematically across between-subject structure conditions.
\end{itemize}

\subsection{Study 2 Data Preparation}
Study~2 data are structured at the \textbf{respondent level} (one row per product manager). Preparation includes:
\begin{itemize}
  \item \textbf{Validity screening.} Retain only complete and internally consistent responses; remove duplicate or
  corrupted entries when identifiable.
  \item \textbf{Missing data handling.} For item non-response, apply listwise deletion for analyses requiring the
  item, and report the effective sample size for each test. If missingness is non-trivial for a composite
  construct, compute the index only when a minimum proportion of items is present.
  \item \textbf{Coding and harmonization.} Harmonize categorical response labels across languages; code open-ended
  responses into qualitative themes for interpretive analysis.
  \item \textbf{Cross-context consistency.} Ensure that the USA and China cohorts are treated symmetrically in
  variable coding, scaling, and aggregation to support valid comparison.
\end{itemize}

\section{Analysis Strategy for Study 1 (Experiment)}
\label{sec:analysis-study1}
The Study~1 analysis strategy mirrors the experimental design and the RQ1 decomposition into memory mechanisms.
Because AI participants contribute repeated measures across three timing conditions, analyses account for
within-participant dependence.

Analyses were conducted in \textbf{R} using a combination of mixed-design ANOVA and mixed-effects modelling. For
ANOVA-style tests, sum-to-zero contrasts and Type-III tests were used to align inference with the factorial design.
For mixed-effects models, random intercepts were specified for participants and (where applicable) for articles to
account for repeated measurement and stimulus heterogeneity. Post-hoc contrasts were estimated using marginal-means
frameworks with Holm correction for multiple comparisons.

\subsection{Primary comparisons}
The primary comparisons are:
\begin{itemize}
  \item \textbf{AI vs.\ No-AI (baseline effect).} Compare AI-assisted blocks vs.\ No-AI blocks on recognition
  accuracy, recall quality, cognitive load, confidence, and epistemic risk.
  \item \textbf{Timing effects (AI only).} Compare pre-reading vs.\ synchronous vs.\ post-reading timing within the
  AI group.
  \item \textbf{Structure effects (AI only).} Compare integrated vs.\ segmented structures between AI participants.
  \item \textbf{Timing $\times$ structure (AI only).} Test whether timing effects differ by summary structure.
\end{itemize}

\subsection{Models and outcome-specific approaches}
Analyses are performed using regression models appropriate to outcome type:
\begin{itemize}
  \item \textbf{Recall quality.} Modeled as a continuous or ordinal outcome at the block level (depending on score
  distribution), with predictors for timing and structure and participant-level random effects.
  \item \textbf{Recognition accuracy.} Modeled either at the item level (binary correct/incorrect) using logistic
  mixed-effects models, or at the block level using binomial or proportion models, with predictors for timing and
  structure and participant-level random effects. Where feasible, recognition items are partitioned into
  summary-aligned vs.\ article-only subsets to distinguish buffer-supported performance from article-dependent
  performance.
  \item \textbf{Source monitoring / false-lure endorsement.} Modeled as a count (e.g., 0--2 per article) or as a
  binary item-level event using count models or logistic mixed-effects models. The main inferential focus is on
  whether segmented structure increases false-lure endorsement relative to integrated structure.
  \item \textbf{Confidence and calibration.} Confidence is modeled as an outcome and also used as a predictor of
  accuracy to test calibration. Calibration is assessed by relating confidence to correctness (e.g., via
  regression or correlation-based indices) and by comparing calibration across conditions.
  \item \textbf{Cognitive load.} Mental-effort and difficulty ratings are modeled as continuous outcomes; timing is
  expected to increase load when it introduces interruptions and split attention, while integrated structure is
  expected to reduce extraneous load.
  \item \textbf{Behavioral reliance.} Summary view time and opening counts are analyzed descriptively and via
  mixed-effects models to test whether timing and structure change reliance behavior. These metrics are also used
  as covariates or sensitivity controls to assess whether performance effects persist beyond differential exposure.
\end{itemize}

\subsection{Inference and reporting}
Unless otherwise stated, tests are two-sided with \(\alpha = 0.05\). Results are reported with effect sizes and
confidence intervals where appropriate. Because multiple outcomes are analyzed, robustness checks include assessing
whether key conclusions persist under reasonable alternative specifications (e.g., alternative outcome
transformations and exclusion thresholds).

Reporting follows two principles. First, because outcome types differ (continuous, proportions, and binary events),
models are chosen to respect the measurement scale and to avoid artefactual conclusions. Second, because the thesis
aims to explain mechanisms rather than to maximize the number of significant results, interpretation emphasizes
pattern coherence across measures (e.g., timing effects on recognition supported by process measures), not only
p-values.

\section{Analysis Strategy for Study 2 (Field Study)}
\label{sec:analysis-study2}
The Study~2 analysis strategy is designed to characterize adoption patterns, role transformation, and perceived
responsibility in product management (RQ2) and to support cross-context comparison (USA--China).

Following the logic of \Cref{sec:study2-method}, each survey item is interpreted through two linked lenses: an
\emph{overall} lens (what pattern is present across the full sample) and a \emph{comparative} lens (whether the USA
and China cohorts differ). This dual emphasis ensures that the study can report both (i) general adoption patterns
that plausibly reflect role-level dynamics in product management and (ii) context-contingent differences that may
reflect organizational environments and governance norms \citep{Pinzauti2025ReinventingPM}.

\subsection{Quantitative analysis of structured items}
For Likert-scale items, the analysis proceeds in three steps:
\begin{itemize}
  \item \textbf{Descriptive statistics.} Compute means, standard deviations, and distribution plots for each item
  and for composite indices (where applicable).
  \item \textbf{Overall effects.} Test whether the overall sample indicates above-neutral endorsement of key
  statements (e.g., increased strategic focus, increased governance responsibility) using one-sample tests against
  the neutral midpoint (typically 3 on a 1--5 scale).
  \item \textbf{Cross-context comparison.} Compare the USA and China cohorts using independent-samples tests for
  continuous-like outcomes and contingency analyses for categorical outcomes.
\end{itemize}

For categorical or multiple-choice items (e.g., tool selection, governance ownership, training modes), chi-square
tests are used to evaluate (i) whether observed distributions differ from uniform or expected baselines (goodness of
fit) and (ii) whether response distributions differ by region (tests of independence). When distributions are
sparse, categories are consolidated to preserve interpretability.

\subsection{Qualitative analysis of open-ended responses}
Open-ended responses are analyzed using thematic coding to identify recurring patterns relevant to RQ2, including:
examples of AI-supported decisions, perceived risks (bias, opacity, over-reliance), governance practices, and
skill-development narratives. Themes are compared across contexts (USA--China) to identify convergences and
divergences, and qualitative insights are used to contextualize quantitative patterns.

To improve transparency and replicability, coding follows a staged approach: (i) open coding to identify candidate
themes, (ii) consolidation into a codebook aligned with the measurement domains in \Cref{sec:study2-method}, and
(iii) axial coding to connect themes to role transformation and governance narratives. Where themes differ by
region, interpretation is tied back to the comparative logic of RQ2 (context-contingent adoption patterns).

\section{Cross-Study Integration Logic (RQ3)}
\label{sec:cross-study-integration}
RQ3 asks how changes in organizational decision-making practices enabled by AI can be explained through underlying
cognitive mechanisms of AI-assisted memory. The integration logic follows a mechanism-to-practice mapping approach:

\begin{enumerate}
  \item \textbf{Extract mechanism signatures from Study~1.} Identify which AI Buffer properties (timing, structure)
  change (i) performance (recall/recognition), (ii) epistemic risk (false-lure endorsement/source monitoring), and
  (iii) metacognitive and effort signals (confidence and cognitive load).

  \item \textbf{Extract practice patterns from Study~2.} Identify how AI tools are used in product management
  (automation vs.\ augmentation vs.\ decision support), what role shifts are reported, and how responsibility and
  governance are perceived and implemented.

  \item \textbf{Map patterns to mechanisms.} Interpret practice patterns through the AI Buffer mechanisms:
  \begin{itemize}
    \item Workflow acceleration and increased strategic focus are mapped to \emph{offloading and cue-supported
    cognition} (effort substitution and faster evidence access).
    \item Governance emphasis and accountability concerns are mapped to \emph{source monitoring} and \emph{epistemic
    risk} (traceability failures under misattribution).
    \item Trust and reliance narratives are mapped to \emph{confidence calibration} (when confidence is increased
    by fluency rather than by accuracy).
    \item Adoption friction and interface concerns are mapped to \emph{cognitive load} and \emph{split attention}
    (coordination costs and interruptions).
  \end{itemize}

  \item \textbf{Specify boundary conditions.} Use Study~2 context differences (USA--China) and reported skill
  differences (AI literacy and training) as candidate moderators of the mechanism-to-practice link, consistent with
  the framework in \Cref{sec:integrated-framework}.
\end{enumerate}

The goal of integration is explanatory coherence: the thesis does not treat Study~1 and Study~2 as two separate
projects, but as complementary evidentiary sources that jointly support the AI Buffer Model as an account of how
AI-generated external representations reshape knowledge work.

\section{Validity, Reliability, and Robustness}
\label{sec:validity-reliability-robustness}
Because the thesis combines experimental and field designs, validity and robustness are addressed at multiple
levels.

\subsection{Study 1}
\textbf{Internal validity.} Randomization and controlled timing constraints support causal interpretation of timing
and structure effects. Compliance screening reduces noise from non-compliance.

\textbf{Construct validity.} RQ1 constructs are measured with complementary outcomes (recall, recognition, false
lures, confidence, cognitive load, logs), reducing reliance on any single metric. False-lure endorsement provides a
direct operationalization of epistemic risk rooted in source-monitoring theory \citep{Johnson1993SourceMonitoring}.

\textbf{Reliability.} When qualitative scoring is required (e.g., recall quality), reliability is supported by a
pre-specified rubric and by systematic review procedures. Sensitivity checks include verifying that results do not
hinge on a small number of ambiguous scoring cases.

\textbf{Robustness.} Robustness checks include alternative model specifications (e.g., item-level vs.\ block-level
recognition models), inclusion/exclusion of covariates (e.g., reading time and summary exposure), and alternative
exclusion thresholds for compliance.

\subsection{Study 2}
\textbf{External validity.} The field study measures AI adoption in a real decision-intensive role. The cross-national
design increases generalizability by comparing two influential contexts (USA and China).

\textbf{Measurement reliability.} For multi-item constructs, internal consistency is assessed (e.g., Cronbach’s
alpha) and items are combined only when coherence is acceptable. For qualitative themes, coding consistency is
supported through structured codebooks and review.

\textbf{Cross-context comparability.} The bilingual instrument design is intended to reduce language barriers and
support semantic equivalence across cohorts. Analyses check that observed differences are not driven solely by
systematic missingness or coding artifacts.

\subsection{Cross-study robustness}
Cross-study claims are treated as \emph{explanatory} rather than purely statistical. Robustness is evaluated by
triangulation: an integrative claim is considered stronger when (i) it is consistent with Study~1 mechanisms, (ii)
it aligns with Study~2 reported practices, and (iii) it remains plausible under alternative boundary conditions
(e.g., differences in AI literacy, governance maturity, or time pressure).

\chapter{Results}
\label{ch:results}

\section{Study 1 Results (RQ1)}
\label{sec:results-study1}

This section reports the outcomes of the controlled experiment and addresses \textbf{RQ1}: \emph{How do AI-generated
external representations influence core human memory processes during knowledge-intensive tasks?} Results are
organized around the mechanisms specified in the AI Buffer Model (encoding scaffolding, cue-dependent retrieval,
source monitoring, confidence calibration, and cognitive load; \Cref{sec:ai-buffer-model}).

\subsection{Sample overview and outcome structure}
The experimental dataset includes $N=36$ participants (AI: $n=24$; No-AI: $n=12$), each completing three reading
blocks with comprehension and memory assessments, yielding $108$ block-level observations. Within the AI group, the
experimental manipulation followed a $2$ (summary structure: integrated vs segmented) $\times$ $3$ (summary timing:
pre-reading vs synchronous vs post-reading) mixed design (details in \Cref{sec:study1-method}).

\subsection{Reporting conventions}
For each outcome, we report descriptive statistics (mean and standard deviation) followed by inferential tests. For
tests within the AI group, timing is treated as a within-participant factor and structure as a between-participant
factor. Where post-hoc timing contrasts are reported, p-values are Holm-corrected within the relevant family of
comparisons. Unless stated otherwise, analyses use the complete-case dataset described in
\Cref{sec:study1-method,sec:analysis-study1}.

\subsection{Recognition and learning outcomes (MCQ accuracy)}
\label{sec:study1-results-mcq}

\paragraph{AI vs No-AI baseline.}
Across all multiple-choice items, the AI condition outperformed the No-AI condition (AI: $M=0.598$, $SD=0.085$;
No-AI: $M=0.510$, $SD=0.098$), $F(1,34)=7.86$, $p=.008$, $\eta^2_p=0.188$. This establishes a global recognition
benefit associated with AI summaries. In absolute terms, this corresponds to an 8.8 percentage-point gain in
recognition accuracy (Cohen’s $d \approx 0.98$).

\begin{table}[H]
    \centering
    \small
    \begin{tabular}{lcc}
        \toprule
        \textbf{Group} & \textbf{Overall MCQ accuracy (Mean $\pm$ SD)} & \textbf{$N$} \\
        \midrule
        AI-assisted & $0.598 \pm 0.085$ & 24 \\
        No-AI & $0.510 \pm 0.098$ & 12 \\
        \bottomrule
    \end{tabular}
    \caption{Overall MCQ accuracy by AI availability (Study~1).}
    \label{tab:study1-baseline-ai-noai}
\end{table}

\paragraph{Timing effects within the AI group.}
When recognition is indexed specifically to information covered by the AI summary (\emph{AI-summary-sourced}
questions), pre-reading access yields the highest accuracy. AI-summary-sourced accuracy by timing is:
pre-reading $M=0.833$ ($SD=0.141$), synchronous $M=0.568$ ($SD=0.239$), and post-reading $M=0.641$ ($SD=0.196$).
A mixed ANOVA (structure $\times$ timing; AI group) shows a strong timing main effect, $F(2,44)=14.00$,
$p<.001$, $\eta^2_p=0.389$, with no structure main effect and no interaction. Holm-corrected comparisons indicate
pre-reading outperforms synchronous ($p=.00052$, $d_z=0.91$) and post-reading ($p=.00057$, $d_z=0.87$); the
synchronous vs post-reading contrast is not significant ($p=.148$).

\paragraph{Overall MCQ accuracy (all questions).}
The same timing ordering appears in overall MCQ accuracy (pre-reading $M=0.699$, $SD=0.125$; synchronous
$M=0.533$, $SD=0.147$; post-reading $M=0.562$, $SD=0.127$), with a significant timing effect,
$F(1.77,38.87)=11.77$, $p<.001$.

\begin{figure}[H]
    \centering
    \subfloat[AI-summary-sourced accuracy by timing (AI group).]{
        \includegraphics[width=0.49\textwidth]{slide1_main_finding_1_ai_summary_accuracy.png}
        \label{fig:study1-ai-summary-accuracy}
    }
    \hfill
    \subfloat[Overall MCQ accuracy by structure and timing (AI group).]{
        \includegraphics[width=0.49\textwidth]{A1_plot_mcq_accuracy.png}
        \label{fig:study1-mcq-accuracy}
    }
    \caption{Recognition outcomes in Study~1. Error bars indicate $\pm$SE.}
    \label{fig:study1-recognition}
\end{figure}

\paragraph{Boundary condition: article-only comprehension.}
Timing does not meaningfully affect accuracy on questions that are not covered by the AI summary (article-only
accuracy), indicating that the timing manipulation primarily changes learning for summary-covered information rather
than broadly improving comprehension of the underlying article.

\begin{figure}[H]
    \centering
    \subfloat[Article-only accuracy by timing (AI group).]{
        \includegraphics[width=0.49\textwidth]{A1_plot_article_accuracy.png}
        \label{fig:study1-article-accuracy}
    }
    \hfill
    \subfloat[Decomposition of timing effects into summary- vs article-sourced accuracy (AI group).]{
        \includegraphics[width=0.49\textwidth]{timing_decomposition.png}
        \label{fig:study1-timing-decomposition}
    }
    \caption{Timing diagnostics for Study~1: timing primarily affects learning of summary-covered content.}
    \label{fig:study1-timing-diagnostics}
\end{figure}

\subsubsection{Mechanism-oriented robustness checks for the timing advantage}
To test whether the timing advantage is explained by summary-aligned encoding and/or by increased exposure to the
summary, two complementary model-based checks were conducted.

\paragraph{Summary-alignment mechanism.}
AI-summary-sourced accuracy strongly predicts overall MCQ accuracy in a mixed-effects model controlling for timing
and structure ($\beta = 0.472$, $p < .001$). When AI-summary-sourced accuracy is included as a predictor, the
pre-reading advantage on overall MCQ accuracy shrinks substantially (pre--synchronous: $\Delta = 0.172 \rightarrow
0.043$; pre--post: $\Delta = 0.143 \rightarrow 0.044$) and is no longer significant after Holm correction
($p = .352$). Model fit improves markedly ($\chi^2(1) = 46.38$, $p < .001$), supporting the interpretation that
timing primarily increases learning of summary-covered content rather than broadly raising comprehension.

\paragraph{Exposure control (summary time).}
Controlling for log-transformed summary exposure reduces the timing estimates only modestly, and the timing
contrasts remain significant, suggesting that pre-reading advantages are not explained solely by ``more time'' on
the summary. Summary time itself is a positive predictor of AI-summary-sourced accuracy ($\beta = 0.062$, $p = .031$).

\begin{table}[H]
    \centering
    \small
    \begin{tabular}{lcc}
        \toprule
        \textbf{Model} & \textbf{Pre--Synchronous estimate} & \textbf{Pre--Post estimate} \\
        \midrule
        Base (timing only) & 0.271*** & 0.214*** \\
        $+$ log(summary\_time) & 0.258*** & 0.164** \\
        \textbf{Reduction} & \textbf{5\%} & \textbf{23\%} \\
        \bottomrule
    \end{tabular}
    \caption{Timing contrasts on AI-summary-sourced accuracy with and without controlling for summary exposure.}
    \label{tab:study1-timing-contrast-summary-time}
\end{table}

\noindent{\small \textit{Note:} *** $p<.001$, ** $p<.01$.}

\subsection{Generative retrieval: free recall is unaffected by timing}
\label{sec:study1-results-recall}

In contrast to recognition performance, free recall does not vary by timing in the AI group: pre-reading $M=5.50$
($SD=1.92$), synchronous $M=5.54$ ($SD=1.99$), and post-reading $M=5.56$ ($SD=2.17$). A mixed ANOVA shows no
timing effect, $F(1.86,40.88)=0.03$, $p=.969$. Additionally, an AI vs No-AI comparison indicates no meaningful
recall difference (AI: $M=5.535$; No-AI: $M=5.403$; $p>.50$). Together, these findings indicate that AI assistance
primarily affects cue-supported recognition rather than internally generated retrieval.

\begin{figure}[H]
    \centering
    \subfloat[Recall total score by timing (AI group).]{
        \includegraphics[width=0.49\textwidth]{A1_plot_recall_total_score.png}
        \label{fig:study1-recall}
    }
    \hfill
    \subfloat[Mental effort by condition (AI and No-AI).]{
        \includegraphics[width=0.49\textwidth]{A1_plot_mental_effort.png}
        \label{fig:study1-mental-effort}
    }
    \caption{Secondary outcomes in Study~1: recall and cognitive load. Error bars indicate $\pm$SE.}
    \label{fig:study1-secondary-recall-effort}
\end{figure}

\subsection{Source monitoring and misinformation: structure shapes false-lure endorsement}
\label{sec:study1-results-source-monitoring}

Susceptibility to false AI-generated claims is primarily driven by summary structure. In the AI group, segmented
summaries increase false-lure endorsement relative to integrated summaries. Participants exposed to segmented
summaries select more false lures (integrated: $M=0.58$, $SD=0.69$; segmented: $M=1.06$, $SD=0.79$), and show lower
false-lure accuracy (integrated: $M=0.556$, $SD=0.354$; segmented: $M=0.375$, $SD=0.302$). A binomial GLMM
predicting false-lure endorsement yields an odds ratio of $5.93$ for segmented structure (95\% CI $[1.63,21.5]$;
$p=.007$).

\begin{figure}[H]
    \centering
    \subfloat[False-lure accuracy by structure and timing (AI group).]{
        \includegraphics[width=0.49\textwidth]{A1_plot_false_lure_accuracy.png}
        \label{fig:study1-false-lure-accuracy}
    }
    \hfill
    \subfloat[False-lure endorsement probability by structure (AI group).]{
        \includegraphics[width=0.49\textwidth]{ORD_plot2_lure_prob_by_structure.png}
        \label{fig:study1-false-lure-prob}
    }
    \caption{Source-monitoring outcomes in Study~1. Error bars indicate $\pm$SE.}
    \label{fig:study1-source-monitoring}
\end{figure}

\subsection{Confidence calibration}
\label{sec:study1-results-calibration}

Metacognitive calibration (confidence relative to actual performance) is imperfect overall, but AI does not worsen
overconfidence. Recall confidence is similar across AI availability (AI: $M=4.25$; No-AI: $M=4.08$; scale 1--7),
and an independent-samples t-test on overconfidence indicates no difference between groups, $t(31.7)=0.16$,
$p=.873$.

\begin{figure}[H]
    \centering
    \subfloat[Overconfidence by group (AI vs No-AI).]{
        \includegraphics[width=0.49\textwidth]{F2_plot_overconfidence.png}
        \label{fig:study1-overconfidence}
    }
    \hfill
    \subfloat[Recall calibration by group.]{
        \includegraphics[width=0.49\textwidth]{H3_plot_recall_calibration_by_group.png}
        \label{fig:study1-recall-calibration}
    }
    \caption{Confidence calibration in Study~1. Error bars indicate $\pm$SE.}
    \label{fig:study1-calibration}
\end{figure}

\subsection{Process evidence: timing redistributes attention without increasing total time}
\label{sec:study1-results-process}

The timing manipulation changes how attention is allocated between summary and article. In the AI group, pre-reading
produces the highest summary viewing time (mean $132.5$ seconds) and summary share ($24.9$\%), while post-reading is
lowest (mean $69.5$ seconds; $13.7$\%). Importantly, total time-on-task remains stable across timing conditions
(approximately $531$--$536$ seconds), indicating a redistribution of attention rather than more time spent overall.

\begin{table}[H]
    \centering
    \small
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        \textbf{Timing (AI group)} & \textbf{Summary time (s)} & \textbf{Reading time (min)} & \textbf{Total time (s)} & \textbf{Summary share (\%)} \\
        \hline
        Pre-reading & 132.5 & 6.72 & 535.8 & 24.9 \\
        Synchronous & 100.3 & 7.19 & 531.6 & 19.5 \\
        Post-reading & 69.5 & 7.69 & 530.8 & 13.7 \\
        \hline
    \end{tabular}
    \caption{Time allocation by summary timing (Study~1, AI group; means).}
    \label{tab:study1-time-allocation}
\end{table}

\begin{figure}[H]
    \centering
    \subfloat[Summary time (seconds).]{
        \includegraphics[width=0.24\textwidth]{A1_plot_summary_time_sec.png}
        \label{fig:study1-summary-time}
    }
    \hfill
    \subfloat[Summary share of total time.]{
        \includegraphics[width=0.24\textwidth]{A1_plot_summary_prop.png}
        \label{fig:study1-summary-share}
    }
    \hfill
    \subfloat[Reading time (minutes).]{
        \includegraphics[width=0.24\textwidth]{A1_plot_reading_time_min.png}
        \label{fig:study1-reading-time}
    }
    \hfill
    \subfloat[Total time (seconds).]{
        \includegraphics[width=0.24\textwidth]{A1_plot_total_time_sec.png}
        \label{fig:study1-total-time}
    }
    \caption{Process measures in Study~1 (AI group). Error bars indicate $\pm$SE.}
    \label{fig:study1-process}
\end{figure}

\paragraph{Trust and dependence.}
Post-block ratings indicate that perceived reliance tracks timing and, for dependence, also differs by structure.
Mixed-design analyses show a timing effect for trust ($F(2,43)=7.90$, $p=.001$) and both structure and timing
effects for dependence (structure: $F(1,22)=6.21$, $p=.021$; timing: $F(2,43)=7.74$, $p=.001$). These subjective
patterns converge with the process evidence that pre-reading increases engagement with the external representation at
the point where it can shape encoding.

\begin{figure}[H]
    \centering
    \subfloat[Trust by structure and timing (AI group).]{
        \includegraphics[width=0.49\textwidth]{A1_plot_ai_trust.png}
        \label{fig:study1-trust}
    }
    \hfill
    \subfloat[Dependence by structure and timing (AI group).]{
        \includegraphics[width=0.49\textwidth]{A1_plot_ai_dependence.png}
        \label{fig:study1-dependence}
    }
    \caption{Subjective reliance in Study~1 (AI group). Error bars indicate $\pm$SE.}
    \label{fig:study1-reliance}
\end{figure}

\subsection{Robustness checks and supplementary diagnostics}
\label{sec:study1-results-robustness}

Robustness checks support the stability of the main conclusions. Counterbalancing diagnostics indicate that timing
conditions were distributed across articles as intended; leave-one-article-out analyses show that the timing
advantage for AI-summary-sourced accuracy persists when any single article is removed. Supplementary plots also
characterize potential stimulus heterogeneity (e.g., article difficulty) and the decomposition of timing effects.

\begin{figure}[H]
    \centering
    \subfloat[Counterbalancing check: timing by article.]{
        \includegraphics[width=0.32\textwidth]{EXP_fig_counterbalancing_timing_by_article.png}
        \label{fig:study1-counterbalancing}
    }
    \hfill
    \subfloat[Leave-one-article-out robustness (timing effect).]{
        \includegraphics[width=0.32\textwidth]{slide4_robustness_leave_one_article_out.png}
        \label{fig:study1-loo}
    }
    \hfill
    \subfloat[Article difficulty diagnostic.]{
        \includegraphics[width=0.32\textwidth]{ORD_plot3_article_difficulty.png}
        \label{fig:study1-article-difficulty}
    }
    \caption{Robustness and diagnostic checks for Study~1.}
    \label{fig:study1-robustness}
\end{figure}

\section{Study 2 Results (RQ2)}
\label{sec:results-study2}

This section reports results from the field study and addresses \textbf{RQ2}: \emph{How does the adoption of AI tools
shape decision-making practices, role configuration, and perceived responsibility in product management?} The final
dataset comprises $N=74$ validated responses, evenly split between the United States ($n=37$) and China ($n=37$)
(\Cref{sec:study2-method}).

\subsection{Adoption patterns and tool portfolio}
\label{sec:study2-results-adoption}

Respondents report moderate-to-high use of AI tools across several categories, with generative AI tools showing the
highest average usage intensity. Expectations are generally met or exceeded (mean $3.45$ on a 1--5 scale), and
respondents anticipate continued growth in AI usage over the next 12 months (mean $3.66$).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{study2_plot_tool_usage.png}
    \caption{Self-reported AI tool usage intensity in product management (Study~2; error bars show $\pm$SD).}
    \label{fig:study2-tool-usage}
\end{figure}

\subsection{Workload shifts and role reconfiguration}
\label{sec:study2-results-workload}

AI adoption is associated with differentiated workload reduction across task categories. The strongest perceived
reductions occur for routine tasks and documentation/content creation, while routine communications show the weakest
reduction. Consistent with the role-reconfiguration claims in \Cref{sec:pm-decision-domain}, this pattern suggests
that AI is primarily used to automate or accelerate operational work, freeing capacity for higher-level activities.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{study2_plot_workload_reduction.png}
    \caption{Perceived workload reduction by product-management task (Study~2; error bars show $\pm$SD).}
    \label{fig:study2-workload-reduction}
\end{figure}

\subsection{Decision-making support and strategic impact}
\label{sec:study2-results-decision}

Respondents report positive perceived impacts on productivity and decision-making. Perceived productivity gain is
above the neutral benchmark ($M=3.47$, $SD=1.10$; $t(73)=4.77$, $p<.001$), and decision-making improvement is also
positive ($M=3.45$, $SD\approx 1.05$; $t(73)=3.34$, $p=.0013$). Reported strategic time gained through automation is
modest but significant (mean $0.38$ hours per week; $t(73)=5.33$, $p<.001$). Cross-national comparisons indicate a
significant difference in perceived decision-making improvement, with Chinese respondents reporting higher perceived
benefits ($t(72)=-2.63$, $p=.0104$).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{study2_plot_strategic_impacts.png}
    \caption{Strategic and governance impacts of AI adoption (Study~2; error bars show $\pm$SD).}
    \label{fig:study2-strategic-impacts}
\end{figure}

\subsection{Responsibility, governance, and ethics}
\label{sec:study2-results-governance}

Ethical issues are reported as occurring less than ``occasionally'' on average ($M=2.65$, $SD\approx 1.09$;
$t(73)=-2.54$, $p=.013$). However, governance and accountability remain unresolved for many respondents. The most
frequent response for ethical responsibility indicates no clear ownership (33.8\%), and the most frequent response
for the existence of ethical AI guidelines indicates no clear guidelines (36.5\%). Distributional tests further show
non-uniformity in ethical guideline structures (overall $\chi^2(3)=15.56$, $p<.001$) and ethical responsibility
distribution (overall $\chi^2(4)=10.26$, $p=.037$), with cross-national differences for both responsibility
distribution ($p=.003$) and guideline structures ($p=.0017$). These findings align with the thesis claim that AI
adoption changes not only task execution but also accountability boundaries in decision-intensive roles.

\begin{table}[H]
    \centering
    \small
    \begin{tabularx}{\textwidth}{|X|X|c|}
        \hline
        \textbf{Survey item} & \textbf{Most frequent response} & \textbf{\%} \\
        \hline
        AI tools used by product managers & ChatGPT & 78.4\% \\
        Comparison of AI integration goals in product management & Reducing operational costs & 52.7\% \\
        Top factors selecting AI tools & Cost-effectiveness & 58.1\% \\
        Tasks that became more complex due to AI & Human-in-the-loop checks & 75.7\% \\
        Tasks prioritized with time saved by AI & Product innovation & 85.1\% \\
        Primary responsibility for ethical AI decisions & No clear ownership/responsibility defined & 33.8\% \\
        Existence of ethical AI guidelines in organizations & No clear guidelines exist & 36.5\% \\
        \hline
    \end{tabularx}
    \caption{Selected categorical highlights from Study~2 (most frequent response by item).}
    \label{tab:study2-categorical-highlights}
\end{table}

\subsubsection{Cross-national governance patterns}
To contextualize the ``no clear ownership'' and ``no clear guidelines'' patterns, Tables~\ref{tab:study2-ethics-responsibility}
and \ref{tab:study2-ethics-guidelines} report the full response distributions by region. Two patterns are salient.
First, the U.S.\ cohort reports substantially higher ambiguity in responsibility ownership (18 vs.\ 7 reporting ``no
clear ownership''), while the China cohort more frequently attributes responsibility to product managers
individually. Second, the U.S.\ cohort reports more formal internal guideline structures (18 vs.\ 4), while the
China cohort reports higher reliance on informal guidance.

\begin{table}[H]
    \centering
    \small
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Responsibility structure} & \textbf{China} & \textbf{USA} & \textbf{Overall} & \textbf{\% Overall} \\
        \midrule
        Formal organizational guidelines/policies & 7 & 6 & 13 & 17.6\% \\
        Product managers individually & 11 & 0 & 11 & 14.9\% \\
        Dedicated AI/Ethics committee & 4 & 6 & 10 & 13.5\% \\
        Cross-functional team/collaboration & 7 & 6 & 13 & 17.6\% \\
        No clear ownership/responsibility defined & 7 & 18 & 25 & 33.8\% \\
        \bottomrule
    \end{tabular}
    \caption{Responsibility for ethical AI decisions by region (Study~2; counts and overall percentages).}
    \label{tab:study2-ethics-responsibility}
\end{table}

\begin{table}[H]
    \centering
    \small
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Guideline structure} & \textbf{China} & \textbf{USA} & \textbf{Overall} & \textbf{\% Overall} \\
        \midrule
        Formal internal AI ethics guidelines exist & 4 & 18 & 22 & 29.7\% \\
        No clear guidelines exist & 16 & 11 & 27 & 36.5\% \\
        Informal ethical guidance only & 12 & 7 & 19 & 25.7\% \\
        External standards followed (e.g., GDPR, EU AI Act) & 4 & 0 & 4 & 5.4\% \\
        \bottomrule
    \end{tabular}
    \caption{Existence of ethical AI guidelines in product management by region (Study~2).}
    \label{tab:study2-ethics-guidelines}
\end{table}

\section{Cross-Study Integration (RQ3)}
\label{sec:results-cross-study}

This section addresses \textbf{RQ3}: \emph{How can changes in organizational decision-making practices enabled by AI
be explained through underlying cognitive mechanisms of AI-assisted memory?} The cross-study logic treats Study~1 as
mechanistic evidence about \emph{how} external representations shape memory and metacognition, and Study~2 as field
evidence about \emph{where} those mechanisms become operational in decision-intensive work settings.

\subsection{Integrated interpretation across levels}
The combined findings support three integration claims consistent with the AI Buffer Model and the multi-level
framework introduced in \Cref{sec:integrated-framework}.

\paragraph{Claim 1: ``AI-first framing'' is effective when it shapes encoding, not when it replaces it.}
Study~1 shows a strong timing advantage for pre-reading summaries on recognition outcomes, supported by process
evidence that timing redistributes attention toward the summary without increasing total time. Study~2 indicates that
PMs use AI to reduce routine workload and report improved decision-making. Together, these results suggest that
organizational benefits are most plausible when AI is used early to frame problems and structure attention, while
humans still perform critical integration and judgement.

\paragraph{Claim 2: Fragmentation increases misattribution risk, motivating governance mechanisms.}
Study~1 demonstrates that segmented presentations substantially increase false-lure endorsement (odds ratio $5.93$),
pointing to a source-monitoring vulnerability when AI-provided information is fragmented. Study~2 shows that
accountability and ethical governance are often unclear and that guidelines are frequently absent. The integration
implication is that organizations should treat the \emph{provenance} and \emph{structure} of AI outputs as part of
decision governance: integrated, traceable artifacts should reduce misattribution and support responsibility
assignment.

\paragraph{Claim 3: Reliance is not purely ``overconfidence''; it is a shift in cognitive infrastructure.}
Study~1 finds that AI does not worsen overconfidence but does change reliance and attention allocation (trust and
dependence track timing). Study~2 complements this by showing that AI adoption reallocates work toward higher-level
activities while introducing new responsibilities (e.g., human-in-the-loop checks). The integrated interpretation is
that reliance operates through infrastructural shifts in the external representation: who curates it, when it is
consulted, and how it is audited.

\begin{table}[H]
    \centering
    \small
    \begin{tabularx}{\textwidth}{|X|X|X|}
        \hline
        \textbf{Cognitive mechanism (Study~1)} & \textbf{Empirical pattern (Study~1)} & \textbf{Organizational implication (Study~2)} \\
        \hline
        Encoding scaffold (advance organizer) & Pre-reading yields highest AI-summary-sourced accuracy; timing is a strong driver of recognition & AI is most valuable when used early to frame options and guide subsequent work, rather than as a late-stage ``answer engine'' \\
        \hline
        Cue-dependent retrieval and offloading & MCQ improves while free recall is unchanged (recognition--recall dissociation) & Decision practices may become artifact-driven (easy retrieval of AI-framed options) without equivalent deep internalization; verification practices become essential \\
        \hline
        Source monitoring & Segmented structure increases false-lure endorsement (OR $5.93$) & Fragmented AI usage across tools/channels can increase misattribution and accountability diffusion; integrated documentation and provenance controls are risk mitigations \\
        \hline
        Confidence calibration & AI does not increase overconfidence relative to No-AI & Governance should not rely on self-reported confidence; instead it should require explicit checks (e.g., references, validation, sign-off) \\
        \hline
        Load allocation and reliance & Pre-reading increases summary engagement without increasing total time; reliance measures track timing & Adoption shifts time from operational to strategic work, but also shifts responsibility to monitoring, auditing, and escalation of AI outputs \\
        \hline
    \end{tabularx}
    \caption{Cross-study integration linking cognitive mechanisms (Study~1) to product-management practices (Study~2).}
    \label{tab:cross-study-integration}
\end{table}

\chapter{Discussion}
\label{ch:discussion}

\section{Answering the Research Questions}
\label{sec:discussion-answering-rqs}

\subsection{RQ1: How do AI-generated external representations influence core human memory processes?}
\label{sec:discussion-rq1}

Study~1 provides converging evidence that AI-generated summaries function as \emph{external representations} that
selectively reshape memory performance depending on \emph{when} and \emph{how} they are presented
(\Cref{sec:results-study1}). Three implications are central for RQ1.

\paragraph{First, AI assistance improves cue-supported recognition, but does not strengthen generative retrieval.}
The AI condition outperforms No-AI on MCQ accuracy (\Cref{tab:study1-baseline-ai-noai}), yet free recall remains
largely unchanged across timing conditions and does not meaningfully differ between AI and No-AI
(\Cref{sec:study1-results-recall}). This pattern is consistent with the AI Buffer Model’s prediction that external
representations primarily support \emph{cue-dependent retrieval} and may not translate into stronger internally
generated retrieval traces (\Cref{sec:ai-buffer-model}). In other words, AI changes the \emph{availability} of
retrieval cues more than the \emph{durability} of self-generated memory.

\paragraph{Second, timing shapes encoding by changing attention allocation, not time-on-task.}
Pre-reading access yields the highest recognition performance on summary-covered information
(\Cref{sec:study1-results-mcq,fig:study1-recognition}), and process measures indicate that pre-reading increases
summary engagement without increasing total time (\Cref{sec:study1-results-process,tab:study1-time-allocation}).
Together, these findings support an \emph{encoding scaffold} interpretation: the external representation is most
effective when encountered early enough to structure subsequent processing, consistent with advance organizer and
cognitive load accounts \citep{Ausubel1960AdvanceOrganizers,Sweller1988CognitiveLoad}.

\paragraph{Third, structure shapes epistemic risk through source monitoring.}
Integrated summaries reduce false-lure endorsement relative to segmented summaries
(\Cref{sec:study1-results-source-monitoring,fig:study1-source-monitoring}). This supports the claim that the
structure of an external representation can either preserve or degrade cues that enable source attribution and
cross-checking \citep{Johnson1993SourceMonitoring,ChandlerSweller1992SplitAttention}. Importantly, these risks are
not well explained by a simple ``overconfidence'' mechanism: calibration is imperfect overall, but AI does not
increase overconfidence (\Cref{sec:study1-results-calibration,fig:study1-calibration}).

Overall, RQ1 is answered by a mechanism-specific conclusion: AI-generated representations can improve recognition
outcomes when they function as early scaffolds, but they can also increase misattribution risks when fragmented, and
they do not automatically improve generative recall.

\subsection{RQ2: How does AI adoption reshape decision-making practices, roles, and responsibility in product management?}
\label{sec:discussion-rq2}

Study~2 suggests that AI adoption in product management is best understood as a dual shift: a \emph{task shift} toward
automation of operational work and a \emph{governance shift} toward monitoring, accountability, and escalation
(\Cref{sec:results-study2}).

\paragraph{Task shift and role reconfiguration.}
Respondents report the strongest workload reductions for routine work and document drafting/content creation
(\Cref{fig:study2-workload-reduction}). In parallel, respondents report positive productivity and decision-making
impacts (\Cref{fig:study2-strategic-impacts}). This aligns with the thesis positioning of AI as an external
representational layer that accelerates synthesis and documentation, enabling more time for prioritization and
strategic work (\Cref{sec:pm-decision-domain}).

\paragraph{Decision support with cross-context variation.}
Perceived decision-making improvement is positive overall and differs across contexts, with Chinese respondents
reporting higher perceived gains than U.S.\ respondents (\Cref{sec:study2-results-decision}). This suggests that the
organizational value of AI may depend on local adoption maturity, tool ecosystems, and governance constraints, even
when overall usage patterns converge.

\paragraph{Responsibility and governance are frequently unresolved.}
Despite widespread tool usage (e.g., ChatGPT as the most frequently reported tool),
survey highlights show that many respondents report unclear ethical ownership and missing guidelines
(\Cref{tab:study2-categorical-highlights}). In practical terms, adoption changes not only \emph{what} PMs do, but
\emph{who} is accountable for failures and for the verification of AI-mediated evidence---a core organizational
dimension of assisted decision-making.

In sum, RQ2 is answered by a practice-level conclusion: AI adoption is associated with operational efficiency and
perceived decision support, but also with a redistribution of responsibility toward evaluation and governance
activities that many organizations have not yet institutionalized.

\subsection{RQ3: How can organizational changes be explained through cognitive mechanisms of AI-assisted memory?}
\label{sec:discussion-rq3}

RQ3 is addressed by integrating the mechanisms observed in Study~1 with the adoption patterns observed in Study~2
(\Cref{sec:results-cross-study,tab:cross-study-integration}). Three bridges are particularly salient.

\paragraph{Bridge 1: Early framing translates into organizational value.}
The pre-reading advantage in Study~1 indicates that external representations deliver the highest benefits when they
shape encoding and attention allocation. In product management, this maps onto practices in which AI is used early to
frame decision spaces (e.g., synthesizing signals into initial hypotheses), after which humans perform integration and
judgement. The implication is not that AI should ``replace'' decision-making, but that its organizational value is
highest when it functions as cognitive infrastructure for problem framing.

\paragraph{Bridge 2: Artifact-driven work emerges from cue-dependent retrieval.}
Study~1 shows a recognition--recall dissociation, consistent with AI supporting cue-based retrieval more than deep
internalization. Study~2 shows that AI reduces documentation and routine workload and supports decision-making.
Together, these findings imply that organizations may increasingly rely on AI-produced artifacts as shared memory
objects. This strengthens the case for systematic documentation practices (what was generated, why it was trusted,
and how it was validated), because the artifact itself becomes part of the decision trace.

\paragraph{Bridge 3: Fragmentation produces misattribution risk and accountability diffusion.}
Structure-driven false memory effects in Study~1 show that fragmented AI representations increase misattribution risk.
Study~2 indicates that accountability is often unclear and governance policies are frequently missing. The integrated
interpretation is that fragmentation at the cognitive level (weak source cues) can scale into fragmentation at the
organizational level (diffuse ownership). This links the AI Buffer Model’s source-monitoring pathway to governance
requirements for traceability and responsibility assignment \citep{Pasquale2015BlackBoxSociety}.

Taken together, the thesis answers RQ3 by showing how micro-level cognitive mechanisms (scaffolding, cue dependence,
source monitoring, and reliance) provide a coherent explanation for macro-level changes in decision practices and
accountability boundaries.

\section{Theoretical Implications}
\label{sec:discussion-theoretical}

The thesis contributes to theory at three levels: cognition, human--AI interaction, and organizational decision-making.

\paragraph{External representations as designable cognitive infrastructure.}
Study~1 shows that timing and structure are not superficial interface features: they are parameters that modulate
memory outcomes. The pre-reading advantage is consistent with the idea that an AI-generated representation can act as
an advance organizer that structures encoding \citep{Ausubel1960AdvanceOrganizers}, while the segmented-structure risk
connects to split-attention and source-monitoring accounts \citep{ChandlerSweller1992SplitAttention,Johnson1993SourceMonitoring}.
This supports a design-oriented view of AI assistance: the cognitive consequences of ``using AI'' depend on how the
representation is integrated into the task pipeline.

\paragraph{AI-assisted memory is cue-dependent and may not generalize to durable internalization.}
The recognition--recall dissociation refines cognitive offloading arguments by distinguishing between (i) enhanced
performance under cue-rich retrieval conditions and (ii) durable generative retrieval without cues. This aligns with
the broader perspective that external memory aids can change what is encoded and how it is retrieved, rather than
uniformly strengthening memory \citep{Tulving1973EncodingSpecificity,Risko2016CognitiveOffloading,Sparrow2011GoogleEffect}.

\paragraph{From individual reliance to organizational responsibility.}
Study~2 extends the cognitive story into organizational contexts by showing that AI adoption reorganizes work toward
artifact-mediated decision processes while simultaneously creating governance gaps. The cross-study integration
provides theoretical support for the thesis claim that reliance is not reducible to subjective trust alone; it is a
structural shift in how cognitive work is distributed across people, tools, and documentation (\Cref{sec:integrated-framework}).

\section{Practical Implications}
\label{sec:discussion-practical}

The results suggest actionable implications for AI tool design, for product managers, and for organizations.

\paragraph{Implications for AI tool design.}
Design choices should explicitly support (i) early-stage scaffolding and (ii) strong provenance cues. Concretely,
tools can provide ``preview'' modes (pre-reading outlines), integrated views that reduce split attention, and
traceability features (links to sources, persistent citations, and structured decision logs). Because fragmented
representations increase misattribution risk, interface patterns that scatter outputs across channels without
provenance should be treated as a safety concern rather than a convenience feature.

\paragraph{Implications for product managers.}
PMs can treat AI as a framing and synthesis tool rather than a substitute for judgement. When using AI-generated
summaries, the findings support a workflow in which AI outputs are used \emph{before} deeper analysis to structure
attention (analogous to pre-reading), followed by explicit verification steps for high-stakes claims. The results
also motivate maintaining integrated artifacts (one consolidated brief, one decision record) rather than multiple
uncoordinated outputs, to reduce source confusion during later decision reviews.

\paragraph{Implications for organizations and governance.}
Organizations should formalize accountability for AI-mediated decisions: who validates AI-generated evidence, what
counts as sufficient verification, and how exceptions are escalated. Training programs can focus on source
monitoring, prompt hygiene, and documentation standards. Finally, because responsibility and guideline structures
vary across contexts in Study~2, governance frameworks should be adapted to local regulatory and cultural settings
while preserving a common core: provenance, auditability, and clear ownership.

\section{Limitations}
\label{sec:discussion-limitations}

Several limitations constrain the interpretation and generalization of the findings.

\paragraph{Study 1 limitations.}
The experiment uses a controlled task environment with a limited sample and a finite set of stimuli. The AI
assistance is operationalized as summaries rather than fully interactive dialogue, and the outcomes focus on
short-term performance. These choices strengthen internal validity but limit conclusions about long-term retention,
expert populations, and interactive AI workflows.

\paragraph{Study 2 limitations.}
The field study relies on self-reported perceptions in a cross-sectional survey. Causal claims about productivity or
decision quality cannot be inferred, and results may reflect selection bias (e.g., respondents who adopt AI more
actively may be more likely to participate) and measurement limitations in cross-cultural survey design.

\paragraph{Cross-study limitations.}
The multi-level integration is explanatory rather than a statistical mediation test: the studies involve different
tasks and populations, and cognitive variables are not directly measured in the field context. The integration
therefore provides a coherent account of plausible mechanisms, but it should be treated as a framework for future
multi-level validation.

\section{Future Research Directions}
\label{sec:discussion-future}

The thesis motivates several directions for future work.

\begin{itemize}
    \item \textbf{Longitudinal cognitive outcomes:} test whether timing and structure effects persist over longer
    retention intervals and in repeated-use settings where offloading may accumulate.
    \item \textbf{Interactive AI and provenance interventions:} evaluate whether inline citations, uncertainty cues,
    and structured verification prompts reduce false-lure endorsement without reducing productivity.
    \item \textbf{Decision-quality measures in the field:} complement self-reports with behavioral logs, decision
    audit outcomes, and objective performance metrics in product teams.
    \item \textbf{Multi-level causal designs:} run field experiments that manipulate AI representation design in
    real workflows (e.g., pre-reading vs post-reading briefs; integrated vs fragmented outputs) while measuring both
    cognitive and organizational outcomes.
    \item \textbf{Boundary conditions:} examine moderators such as domain expertise, time pressure, governance
    maturity, and regulatory context to specify when AI functions as beneficial cognitive infrastructure versus a
    source of epistemic risk.
\end{itemize}

\chapter{Conclusion}
\label{ch:conclusion}

\section{Summary of Findings}
This thesis investigated how generative AI reshapes knowledge-intensive decision-making by simultaneously affecting
individual cognitive processes and organizational practices. The multi-level design combined a controlled experiment
on AI-assisted memory (Study~1) with a cross-national field study of product managers (Study~2), integrated through
the AI Buffer Model (\Cref{sec:ai-buffer-model}).

At the cognitive level, AI-generated summaries improve recognition performance but do not automatically strengthen
generative retrieval: MCQ accuracy increases with AI assistance, while free recall remains largely unchanged
(\Cref{sec:results-study1}). Timing is a key design parameter: pre-reading access yields the strongest learning
effects for summary-covered information and changes attention allocation toward the summary without increasing total
time on task. Structure is a key safety parameter: segmented presentations increase false-lure endorsement and thus
raise epistemic risk through source-monitoring vulnerabilities.

At the organizational level, product managers report moderate-to-high adoption of AI tools and substantial perceived
workload reduction for routine work and documentation (\Cref{sec:results-study2}). AI use is also associated with
perceived improvements in productivity and decision-making, alongside persistent governance and accountability gaps:
many respondents report unclear ownership for ethical AI decisions and missing guidelines, with cross-national
differences in responsibility and guideline structures (\Cref{sec:study2-results-governance}).

\section{Contributions of the Thesis}
The thesis contributes in three ways. First, it provides an integrative conceptual contribution by framing AI
assistance as \emph{AI-assisted external representation} and by proposing the AI Buffer Model as a mechanism-based
account of how timing and structure shape both performance and epistemic risk. Second, it offers empirical
contributions across levels: controlled evidence that design parameters (timing and structure) drive distinct memory
outcomes, and field evidence that adoption reshapes product-management work and governance perceptions in practice.
Third, it provides actionable implications: designing AI representations to support early-stage scaffolding and
provenance cues, and institutionalizing organizational practices for verification, traceability, and clear
accountability.

\section{Final Answers to RQ1--RQ3}
\textbf{RQ1.} AI-generated external representations influence memory through design-sensitive mechanisms: they improve
cue-supported recognition when encountered early as encoding scaffolds, but they do not necessarily improve free
recall and can increase misattribution risk when fragmented.

\textbf{RQ2.} In product management, AI adoption is associated with a shift from operational workload toward more
strategic activities and perceived decision support, alongside a governance shift in which responsibility and
guideline structures are frequently unclear and vary across contexts.

\textbf{RQ3.} Organizational changes can be explained through cognitive mechanisms: AI-driven shifts toward
artifact-mediated decision-making map onto cue-dependent retrieval and offloading, while governance and
accountability challenges map onto source-monitoring vulnerabilities and the propagation of AI-generated artifacts
through organizational memory systems.

\section{Final Remarks}
Taken together, the findings support a central thesis claim: the impact of generative AI on knowledge work is not
determined solely by whether AI is used, but by how AI-generated representations are embedded into cognitive and
organizational processes. Designing for early scaffolding, integrated presentation, and provenance transparency---and
matching these design choices with organizational verification and accountability practices---is essential for
realizing the benefits of AI assistance while reducing epistemic and governance risks.

\cleardoublepage
\bibliography{Thesis_bibliography} % The references information are stored in the file named "Thesis_bibliography.bib"
\cleardoublepage

\appendix
\chapter{Study 1 Materials (Stimuli, Tasks, Questionnaires)}
\label{app:materials}

This appendix documents the participant-facing study materials (article stimuli, AI summaries, and the MCQ item bank).
Internal annotation markers (e.g., ``FALSE LURE'') present in the source code were stripped in the experimental interface before display; the summaries reproduced here match the displayed versions.
\par\smallskip
\textbf{Language note.} All participant-facing materials were administered in Simplified Chinese. For transparency and accessibility, this appendix reproduces each stimulus in its original Chinese form, followed by an English translation (for reference only). The English versions were translated from the Chinese materials used in the experiment.

\section{Stimuli overview}

\begin{table}[H]
\centering
\caption[Stimuli overview]{Overview of reading stimuli and summaries (word counts based on whitespace tokenization).}
\label{tab:stimuli_overview}
\small
\begingroup
\setlength{\tabcolsep}{4pt}
\begin{tabularx}{\linewidth}{@{}>{\raggedright\arraybackslash}X >{\centering\arraybackslash}p{1.6cm} >{\centering\arraybackslash}p{2.2cm} >{\centering\arraybackslash}p{2.2cm} >{\centering\arraybackslash}p{1.4cm}@{}}
\toprule
Article title & Article\\words & Integrated\\summary\\words & Segmented\\summary\\words & MCQ\\items \\
\midrule
Urban Heat Islands: Causes, Consequences, and What Works & 1223 & 270 & 152 & 14 \\
CRISPR Gene Editing: Promise, Constraints, and Responsible Use & 1249 & 271 & 149 & 14 \\
Semiconductor Supply Chains: Why Shortages Happen and How to Build Resilience & 1293 & 264 & 162 & 14 \\
\bottomrule
\end{tabularx}
\endgroup
\end{table}

\section{MCQ source mapping and false-lure items}

Each article contained 14 multiple-choice items. Items were categorized as (i) AI-summary-covered, (ii) article-only, or (iii) \emph{false-lure} items (where a specific incorrect option corresponded to an AI hallucination/distractor used to quantify misinformation endorsement).

\begin{table}[H]
\centering
\caption{MCQ source mapping and false-lure designation (0-based indices in parentheses; question numbers in brackets).}
\label{tab:false_lure_mapping}
\begin{tabular}{p{2.8cm}p{4.1cm}p{4.1cm}p{3.9cm}}
\toprule
Article & AI summary items & Article-only items & False-lure items \\
\midrule
CRISPR & 0,1,3,4,5,6,7,9 & 8,10,11,12 & 2 (Q3), 13 (Q14) \\
Semiconductors & 0,1,2,3,4,5,6,9 & 7,11,12,13 & 8 (Q9), 10 (Q11) \\
Urban heat islands (UHI) & 0,1,2,4,5,6,7,8 & 9,11,12,13 & 3 (Q4), 10 (Q11) \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{False-lure option mapping (0-based option indices: A=0, B=1, C=2, D=3).}
\label{tab:false_lure_options}
\begin{tabular}{llcl}
\toprule
Article & Item & False-lure option & Description \\
\midrule
CRISPR & Q3 (idx 2) & B (1) & DNA repair activity \\
CRISPR & Q14 (idx 13) & A (0) & Restore \\
Semiconductors & Q9 (idx 8) & A (0) & Quantum processors \\
Semiconductors & Q11 (idx 10) & B (1) & 46 silicon atoms wide \\
UHI & Q4 (idx 3) & C (2) & Photocatalytic roof tiles \\
UHI & Q11 (idx 10) & C (2) & Aged asphalt albedo 0.22 \\
\bottomrule
\end{tabular}
\end{table}

\section{Full stimuli (articles and summaries)}

\subsection{Urban Heat Islands: Causes, Consequences, and What Works}

\textbf{Original (Chinese; administered in the experiment).}\par
\begin{ChineseBlock}
\textbf{标题。} 城市热岛：原因、后果和有效方法\par
\textbf{自由回忆提示。} 请在 5 分钟内写出您从文章中记住的所有内容。尝试用自己的话描述主要想法和关系——原因、后果和解决方案。\par

\textbf{文章正文。}
\begin{quote}\small
城市作为复杂的热系统运作——建筑物、道路与大气共同作用，形成持续的温度差异。每条街道、每个屋顶、每段道路都像热存储单元：白天，沥青路面、砖砌建筑和混凝土结构吸收阳光，并将这股能量转化为热量。与通过水分蒸发和反射降温的绿地不同，城市表面在整个白昼都不断储热。夜晚太阳落山后，这些储存的热量以红外辐射的形式向下层大气释放。城市峡谷几何结构——建筑高度与街道宽度的比率——限制了这种热释放：热辐射在多个表面反射之间被"困"住，才得以逃逸至大气。由此，市中心区域夜间温度比周边郊区高出三至七摄氏度，形成科学家所称的"城市热岛"（Urban Heat Island, UHI）效应。热浪期间，这种温度升高在基础变暖之上叠加，导致空调能耗升高、雾霾生成加剧、弱势人群的热应激风险增加。

这些温差的规模源自物理、材料与气流等因素共同决定的城市能量平衡。表面反射率——以反照率（albedo）衡量，从 0（完全吸收）至 1（完全反射）——在决定吸收太阳能量方面起到关键作用。新铺的沥青表面反照率约为 0.05，仅反射约 5 \% 的入射阳光，而吸收约 95 \%。氧化后的旧沥青略提高至约 0.12，但仍远低于植被覆盖地面（反照率约 0.20 — 0.25）。由此，低反照率表面成为高效的太阳能"收集器"，将辐射转化为可储存的热量。另外，材料特有的热容量（thermal mass）——即储热能力——决定加热和冷却的速度。密集的建筑材料包括混凝土、砖石、石材等具有高热容量，使储热延长、夜间降温延迟。这个储热‑释放循环持续运作：建筑在白天吸收辐射，日落后逐渐释放储热，以便使夜间温度长期维持在较高水平。

城市几何结构进一步增强了热量滞留——所谓"城市峡谷"效应。高楼夹在狭窄街道两侧，形成受限空间，不仅阻挡正午太阳直射，也限制夜间热辐射外逸。在这些峡谷内，太阳辐射在表面之间多次反弹然后逃逸，每次反弹增加被吸收的概率。三维结构由此像一个热量陷阱，最大化能量捕获而最小化冷却路径。同时，建筑物墙面之间受限的气流阻碍了对流冷却——空气流动搬运热量的机制——以便阻断了大气冷却的一个主要途径。最后，来自车辆发动机、建筑供暖与制冷系统、工业流程等的人为废热直接加入城市大气，补充了太阳加热。在极端高温事件中，电力公司响应高用电需求启动效率较低的备用发电机（常为燃煤厂），这些发电机既排放温室气体又使用水进行冷却塔操作，形成一个反反馈循环：降热措施反而产生额外排放与资源使用。

热应激的地理分布直接映射到社会经济格局，产生环境正义问题并带来可测量的健康影响。集中贫困、绿地覆盖较少、建筑密度高、更多的不透水面（如水泥、沥青）社区，所经历的热暴露明显更严重。在严重高温期间——定义为持续超过正常温度范围的时期——死亡风险随热强度显著上升，尤其影响老年人、有心肺疾病者、户外工作者和没有空调的居民。应急医疗系统也由此面临激增的热相关护理需求，给医疗资源带来压力。由此，城市热岛既是一种具有可测温差的物理气候现象，也是反映城市人口之间基础设施、资源分配和恢复能力不平等的社会问题。

要抵消这些热积累过程，需要跨尺度、多层次的综合策略。其核心原理是通过四种相互关联的机制来调节地表能量收支：提升反照率、提供遮荫、增强蒸发冷却，以及调控热容量。例如，使用高反照率涂层的"冷屋顶系统"可将表面反射率从常规值（约 0.10–0.20）提升至 0.70–0.85，以便减少 60–75\% 的太阳能吸收。同样，"冷铺装"——采用浅色材料或可渗水设计的路面，可使高温时段的地表温度比传统沥青低 10–20 摄氏度。但这些方案需要精心设计：如果仅提升可见光反射，而忽略红外辐射，会导致眩光问题，造成视觉不适，甚至因反射辐射加热周围空气。由此，最优的冷表面技术采用波长选择性涂层，最大化反射近红外光（太阳能量峰值所在），同时调控可见亮度。

植被则通过多种生物过程提供辅助降温。树冠在阳光到达地面前先行拦截，形成叶片下方的阴凉区。更重要的是，植物通过叶孔控制释放水蒸气（蒸腾作用），将热能转化为水分蒸发。这种蒸散冷却过程相当于一个分布式大气冷却系统，既降低温度，也提高局部湿度。由此，城市绿化工程具有双重降温效果：一是直接遮荫，二是通过蒸发冷却。小尺度绿色基础设施——如绿化屋顶、垂直绿墙、用于雨水管理的植被浅沟（bioswales）——也可将此类功能扩展至建筑表面与街道设施中。但植被方案亦面临实施挑战，例如干旱地区水资源不足、维护成本高、地下管线冲突及长时间生长周期才见效等问题。

在个体干预之外，要建立系统性的复原力，必须通过综合城市规划将热环境纳入土地利用、建筑规范与基础设施政策中。例如，规划法规可以强制最低树木覆盖率、限制不透水地表比例，或通过开发奖励机制鼓励使用冷材料。建筑能效标准日益涵盖热性能指标，如屋顶太阳反射指数、墙体热阻值等，可降低冷却需求并提升室内舒适度。优先发展行人区、自行车道及公共交通导向开发（TOD）的交通规划，不仅减少汽车热排放，也创造出适合植树与设置透水地面的空间。

最关键的是，实现公平的气候适应需将降温干预优先导向热风险最严重的社区，通过"按需分配资源"的方式，而不是仅由市场机制决定谁能获得降温基础设施。换言之，应把社会脆弱性纳入城市热岛应对策略中，以便避免让高温治理本身加剧社会不平等

最新研究正探索多项先进技术，包括辐射冷却材料，这类材料被设计用于在大气透明窗口波段（8–13 微米红外）主动发射热量，使即便在白天也能将热量直接以辐射形式释放到太空中，实现被动降温。相变材料（Phase-change materials, PCMs）被嵌入建筑墙体中，可在升温期间吸收热能、降温期间释放热量，以便缓冲室内温度变化。在更大尺度上，区域能源系统通过回收废热进行再利用（如区域集中供热网络或工业系统整合），可显著降低整体热排放。但这些技术仍远比传统材料昂贵，在没有政策激励或财政补贴的情况下难以被广泛采用。

归根结底，应对城市热岛效应是一项社会-技术复合型挑战，需跨越不同治理层级、专业领域与社区主体之间进行协同合作。对热传导物理、材料科学与大气动力学的科学理解为机制提供理论基础；工程专业能力则将这些原理转化为实际可用的冷表面技术、绿色基础设施与建筑创新解决方案。城市规划则将这些技术能力整合进空间框架中，考量土地利用格局、交通网络与社会公平维度。社区参与确保干预措施贴合地方需求，融合传统知识，并增强居民的适应能力。

最终，成功不仅取决于技术部署本身，更依赖于制度安排——是否能实现长期维护、公平可及、以及在城市形态与气候变化演变中持续调整适应。城市热岛由此不仅是一个需要工程应对的物理问题，更是一个检视城市如何在效率与韧性、经济增长与环境质量、整体繁荣与公平分配之间取得平衡的透视窗口。
\end{quote}

\textbf{AI 摘要（整合版）。}
\begin{quote}\small
城市热岛效应的形成源于城市区域比周边乡村地带更强地吸收并储存太阳辐射。沥青、砖块、混凝土等材料通常具有 0.05–0.20 的极低反照率，意味着它们几乎不反射阳光，而是吸收大部分入射能量。新铺沥青仅反射约 5\% 的阳光，行业评估普遍认为老化沥青的反照率可升至接近 0.22，显示其随时间推移反射率有所提高。高热容量材料（如石材、混凝土、砖块）在日落后仍会持续释放白天存储的热量，使夜间温度比周边郊区高出 3–7 °C。城市峡谷结构（高楼夹窄街）进一步困住长波辐射，减缓夜间冷却并降低通风效率。

在热浪期间，这些热效应被显著放大，提高健康风险和用电需求。热暴露呈不均衡分布：树荫稀少、建筑密集、不透水地面比例高的低收入社区往往更炎热。弱势人群（如老年人、心血管病患者、户外工作者以及没有空调的住户）承受更大健康风险。与此同时，近期试点项目推广光催化屋顶瓦片，声称其可通过热电效应将吸收的热量转化为电能，被视为一种兼具降温与发电潜力的复合型城市降温技术。

缓解策略包括提升表面反照率、增加遮荫、利用植被增强蒸散冷却、优化城市材料的热容量等。"冷屋顶"反照率可达到 0.70–0.85，可减少 60–75\% 的热量吸收。城市林业通过遮荫与蒸腾共同降低温度。长期韧性需要结合技术方案与公平的资源分配，优先支持缺乏绿地与降温基础设施的弱势社区。
\end{quote}

\textbf{AI 摘要（分段版）。}
\begin{quote}\small
1. 城市热岛效应源于城市表面对太阳能的吸收与储存远超周边乡村地区。

2. 沥青（约 0.05）与砖块等低反照材料可吸收 90–95\% 的阳光。

3. 高热容量材料在白天储热，夜间缓慢释放，使高温持续。

4. 城市峡谷结构阻碍长波辐射散逸，削弱夜间冷却并抑制空气流动。

5. 行业评估称老化沥青反照率可升至约 0.22，随老化而更具反射性。

6. 热浪期间健康风险加剧，尤其影响老年人、心血管病患、户外劳工与无空调住户。

7. 低收入社区因树木稀少、建筑密集、不透水地面多而承受更高热负担。

8. 高反照"冷屋顶"（0.70–0.85）可减少 60–75\% 的热吸收。

9. 城市植被通过遮荫和叶片蒸腾实现双重降温。

10. 试点项目称光催化屋顶瓦片可借由热电效应把热量直接转化为电能
\end{quote}
\end{ChineseBlock}

\textbf{English translation (for reference only).}\par
\textbf{Free-recall prompt.} Please write everything you remember from the article within 5 minutes. Try to describe main ideas and relationships --- causes, consequences, and solutions --- in your own words.

\textbf{Article text.}
\begin{quote}\small
Cities function as complex heat systems where buildings, roads, and the atmosphere interact to create persistent temperature differences. Every street, rooftop, and road acts as a heat storage unit: during the day, asphalt roads, brick buildings, and concrete structures absorb sunlight and convert this energy into heat. Unlike green spaces that cool through water evaporation and reflection, city surfaces continuously store heat throughout daylight hours. When night falls and the sun disappears, these stored heat sources release infrared radiation back into the lower atmosphere. This heat release is restricted by urban canyon geometry---the ratio of building height to street width---which traps outgoing radiation through multiple reflections between surfaces before it can escape to the atmosphere. As a result, central city areas show nighttime temperatures three to seven degrees Celsius higher than surrounding suburban areas, creating what scientists call the urban heat island (UHI) effect. During heat waves, this temperature increase adds to baseline warming, raising energy use for air conditioning, worsening air quality through smog formation, and increasing heat stress among vulnerable populations.

The size of these temperature differences comes from combined physical, material, and airflow factors that together determine the urban energy balance. Surface reflectivity---measured through the albedo coefficient ranging from zero (complete absorption) to one (perfect reflection)---plays a crucial role in determining absorbed solar energy. Fresh asphalt surfaces have albedo values around 0.05, reflecting only five percent of incoming sunlight while absorbing ninety-five percent. Aged asphalt oxidizes to slightly higher reflectance (\ensuremath{\sim}0.12), but remains much darker than vegetated ground cover (albedo \ensuremath{\sim}0.20--0.25). Low-albedo surfaces therefore work as efficient solar collectors that convert radiation into stored heat. Additionally, the material-specific thermal mass---defined as heat storage capacity---controls the speed of heating and cooling. Dense construction materials including concrete, brick, and stone have high thermal mass, enabling prolonged energy storage that delays nighttime cooling. This storage-release cycle operates continuously: buildings absorb radiation throughout the day, then gradually release stored heat after sunset, maintaining elevated nighttime temperatures for extended periods.

Urban geometry further increases heat retention through urban canyon effects. Tall buildings along narrow streets create confined spaces that restrict both incoming sunlight during midday and outgoing radiation at night. Within these canyons, solar radiation bounces between surfaces multiple times before escaping to the atmosphere, increasing the chance of absorption with each bounce. The three-dimensional structure therefore functions as a heat trap, maximizing energy capture while minimizing cooling pathways. At the same time, restricted airflow between building walls prevents convective heat removal---the mechanical transport of heat through air movement---thus blocking one of the atmosphere's main cooling mechanisms. Finally, human-generated waste heat from vehicle engines, building heating and cooling systems, and industrial processes adds extra heat directly into the urban atmosphere, supplementing solar heating. During extreme heat events, power companies respond to increased electricity demand by activating less efficient backup generators, often coal-burning plants that emit greenhouse gases while using water for cooling towers, creating a feedback loop where heat reduction efforts paradoxically generate additional emissions and resource use.

The geographic distribution of heat stress maps directly onto socioeconomic patterns, creating environmental justice issues with measurable health impacts. Neighborhoods with concentrated poverty, reduced tree coverage, higher building density, and more impervious surfaces experience disproportionately higher heat exposure. During severe heat episodes---defined as sustained periods exceeding normal temperature ranges---mortality risk increases dramatically with heat intensity, affecting elderly people, individuals with heart or breathing problems, outdoor workers, and residents without air conditioning. Emergency medical systems face surging demand for heat-related care, straining healthcare resources. Thus, the urban heat island is both a physical weather phenomenon with measurable temperature differences and a social inequality issue that reflects unequal resource distribution, infrastructure investment, and resilience capacity across urban populations.

Counteracting these heat processes requires integrated strategies across multiple scales. The basic principle involves modifying surface energy budgets through four connected mechanisms: increasing reflectance, providing shade, amplifying evaporative cooling, and manipulating thermal mass. Cool roofing systems coated with high-albedo materials can raise surface reflectance from typical values (\ensuremath{\sim}0.10--0.20) to enhanced levels approaching 0.70--0.85, thereby reducing absorbed solar energy by sixty to seventy-five percent. Similarly, "cool pavements" using lighter-colored materials or permeable designs that allow subsurface moisture show surface temperature reductions of ten to twenty degrees Celsius compared to conventional asphalt during peak sun exposure. However, these solutions require careful design: increased visible light reflection without corresponding infrared reduction can increase glare, creating visual discomfort and potentially raising nearby air temperatures through redirected radiation. Optimal cool surface technologies therefore use wavelength-selective coatings that maximize near-infrared reflection---where solar energy peaks---while moderating visible brightness.

Vegetation provides complementary temperature control through multiple biological processes. Tree canopies intercept sunlight before it reaches the ground, creating shaded areas beneath leaves. More importantly, leaf transpiration---the controlled release of water vapor through plant pores---converts heat into water evaporation energy. This evapotranspiration process effectively works as a distributed atmospheric cooling system that moderates temperatures while simultaneously increasing local humidity. Urban forestry programs thus serve dual heat reduction functions: direct shade plus evaporative cooling. Green infrastructure at smaller scales---including vegetated rooftops, vertical gardens, and bioswales for stormwater management---extends these principles across building surfaces and street features. Nevertheless, vegetation faces implementation challenges including water availability in dry regions, maintenance costs, conflicts with underground utilities, and long growth periods before benefits fully develop.

Beyond individual interventions, systemic resilience requires comprehensive urban planning that integrates heat considerations into land-use decisions, building codes, and infrastructure priorities. Zoning regulations can mandate minimum tree coverage ratios, restrict impervious surface percentages, or incentivize cool material use through development bonuses. Building energy standards increasingly include thermal performance metrics---such as roof solar reflectance indices and wall thermal resistance---that reduce cooling needs while improving indoor comfort. Transportation planning that prioritizes pedestrian areas, cycling networks, and transit-oriented development reduces vehicle heat emissions while creating opportunities for shade trees and permeable surfaces. Critically, equitable climate adaptation requires targeting interventions toward thermally vulnerable neighborhoods through needs-based resource allocation rather than allowing market forces alone to determine cooling infrastructure distribution.

Emerging research explores advanced technologies including radiative cooling materials engineered to emit heat at atmospheric transparency wavelengths (8--13 micrometer infrared band), enabling passive heat rejection directly to space even during daylight. Phase-change materials within building walls can buffer indoor temperature changes by absorbing heat during warming periods and releasing it during cooling cycles. District-scale energy systems that recover waste heat for beneficial uses---such as district heating networks or industrial integration---reduce overall heat emissions. However, these innovations remain expensive compared to conventional materials, limiting adoption without regulations or subsidies.

Ultimately, urban heat mitigation represents a sociotechnical challenge requiring coordinated action across governance levels, professional fields, and community stakeholders. Scientific understanding of heat transfer physics, materials science, and atmospheric dynamics provides the mechanistic foundation. Engineering expertise translates theoretical principles into practical cool surface technologies, green infrastructure systems, and building innovations. Urban planning synthesizes these technical capabilities within spatial frameworks that account for land-use patterns, transportation networks, and social equity considerations. Community engagement ensures that interventions address local needs, incorporate traditional knowledge, and build adaptive capacity among residents. Success depends not merely on technology deployment but on institutional arrangements that sustain long-term maintenance, equitable access, and continuous adaptation as climate and urban form evolve. The heat island thus becomes not only a physical problem with engineering solutions but a lens revealing how cities balance efficiency with resilience, economic growth with environmental quality, and overall prosperity with distributional justice.
\end{quote}

\textbf{AI summary (integrated/paragraph format).}
\begin{quote}\small
Urban heat islands develop when cities absorb and retain solar radiation far more effectively than nearby rural landscapes. Surfaces such as asphalt, brick, and concrete have very low albedo values---typically between 0.05 and 0.20---meaning they reflect little sunlight and absorb most incoming energy. While fresh asphalt reflects only around five percent of sunlight, some assessments suggest that aged asphalt can reach albedo values near 0.22, though measured values generally remain much lower in practice. High thermal-mass materials including stone, brick, and concrete continue releasing stored heat well after sunset, keeping nighttime temperatures three to seven degrees Celsius warmer than surrounding areas. Urban canyon geometry---tall buildings along narrow streets---further traps outgoing longwave radiation, slowing atmospheric cooling and reducing ventilation.

These thermal effects intensify during heat waves, elevating health risks and increasing electricity demand. Heat exposure is distributed unevenly: low-income neighborhoods with limited tree canopy, dense construction, and extensive impervious surfaces experience far higher temperatures. Vulnerable groups such as elderly residents, people with cardiovascular conditions, outdoor workers, and those without air conditioning face disproportionate risks. At the same time, recent pilot programs have promoted photocatalytic roof tiles that supposedly convert absorbed heat into electrical energy through thermoelectric effects, though such claims remain unverified and lack large-scale evidence.

Mitigation strategies focus on increasing surface reflectance, boosting shading, enhancing evaporative cooling through vegetation, and optimizing thermal mass. High-albedo "cool roofs," with reflectance values of 0.70--0.85, can reduce absorbed heat by 60--75\%. Urban forestry provides dual benefits through shading and evapotranspiration. Long-term resilience requires integrated planning that aligns technical solutions with equitable resource distribution, prioritizing vulnerable communities lacking access to cooling infrastructure and green space.
\end{quote}

\textbf{AI summary (segmented/bullet format).}
\begin{itemize}
  \item Urban heat islands form when city surfaces absorb and retain far more solar energy than nearby rural areas.
  \item Low-albedo materials such as asphalt (\ensuremath{\sim}0.05) and brick absorb 90--95\% of incoming sunlight.
  \item High thermal-mass materials store heat during the day and release it slowly overnight, sustaining elevated temperatures.
  \item Urban canyon geometry traps outgoing longwave radiation, reducing nighttime cooling and impeding airflow.
  \item Some assessments claim aged asphalt can reach albedo values near 0.22, increasing reflectance with age.
  \item Heat exposure intensifies health risks for elderly individuals, people with cardiovascular conditions, and those lacking air-conditioning.
  \item Low-income neighborhoods face higher heat burdens due to fewer trees, denser buildings, and more impervious surfaces.
  \item High-albedo cool roofs (0.70--0.85 reflectance) reduce heat absorption by 60--75\% compared with conventional materials.
  \item Urban forestry cools cities through shading and evaporative cooling generated by leaf transpiration.
  \item Pilot programs investigating photocatalytic roof tiles claim they convert absorbed heat into electrical energy, though evidence is limited.
\end{itemize}

\subsection{CRISPR Gene Editing: Promise, Constraints, and Responsible Use}

\textbf{Original (Chinese; administered in the experiment).}\par
\begin{ChineseBlock}
\textbf{标题。} CRISPR 基因编辑：承诺、限制和负责任的使用\par
\textbf{自由回忆提示。} 请在 5 分钟内回忆起文章中的所有内容，描述 CRISPR 的工作原理、其医学和农业应用、主要局限性以及伦理或治理挑战。\par

\textbf{文章正文。}
\begin{quote}\small
CRISPR–Cas 系统 最初起源于微生物的防御机制——一种细菌用来识别并消灭入侵病毒的分子形式免疫记忆。每次感染都会在细菌基因组中留下病毒 DNA 的一小段碎片，以便创建了一份永久的生物入侵攻击记录。当同一种病毒再次出现时，细菌会将这些碎片转录为 RNA 指导链，引导 Cas 酶朝向匹配的序列，将病毒 DNA 切割开来。这个识别 + 切割的优雅过程激发了科学家们将该系统改造成他们自己的用途。通过设计与任意选定 DNA 序列匹配的合成指导 RNA，研究人员就可以精确引导 Cas 酶到达该位点，切裂双螺旋，然后让细胞的修复机制对其进行重写。这一原理——"指导、切割、修复"——已经把一种细菌为生存所用的技巧变成了现代生物学中最强大的工具之一。

CRISPR 的可获得性具有革命性意义。以往需要数月努力、使用复杂工具（如锌指核酸酶或 TALENs）才能完成的任务，现在在基础实验室中、借助廉价试剂，在数日内即可完成。这种基因编辑的民主化加速了医学、农业和环境修复方面的发现。不过，CRISPR 的简易性背后隐藏着层层复杂。基因组修饰的精确性是统计性的，而非绝对的：即便设计良好的指导 RNA 也可能结合到意外的 DNA 区域，以便造成"脱靶"编辑，破坏其他基因。挑战不仅在于准确地切割，更在于确保切割仅发生在预期位置。

为了降低这些风险，科学家不断改进系统。他们调整指导 RNA 的长度和化学结构，开发预测算法，并设计具有更高保真的酶。像 SpCas9‑HF1 或 eSpCas9 由此的高精度变体，通过修改 DNA 结合表面来最小化不想要的相互作用。更新的工具——碱基编辑器和始端编辑器（prime editors）——更进一步，通过避免完全双链断裂来实现编辑。它们不是切断两条 DNA 链，而是替换单个字母或复制短序列，以便允许以更少副作用进行细微校正。从粗略切割转向分子级微调，这扩展了可治疗的基因变异范围。

尽管有这些改进，递送仍然是最难的一步。编辑组件必须进入正确的细胞、到达细胞核，并在不引发免疫排斥的情况下发挥作用。像 adeno‑associated virus (AAV) 由此的病毒载体效率高，但其载货空间有限，且可能引发抗体反应，以便阻碍重复给药。脂质纳米颗粒——在 mRNA 疫苗中使用的那种——可以携带更大分子，但会集中在肝脏，有时导致炎症反应。研究者正在测试聚合物载体、细胞外囊泡、以及组织定向肽，亦或物理方法如电穿孔或超声递送。每种方法都必须在效率、安全性和成本之间取得平衡。

另一个关键维度是"时间"。即使 CRISPR 成功到达目标细胞，其活跃持续时间也决定了成效与风险之间的平衡。Cas 酶若持续活动过久，脱靶效应的风险就会增加；反之，如果暴露时间过短，可能导致编辑不完整。为了控制时机，科学家设计出"自限系统"，其中信使 RNA 或蛋白质会在数小时内降解，以便形成一个短暂而精准的"编辑脉冲"。也有研究者开发出可诱导的开关，只有在特定的化学物质或温度信号下，Cas 酶才会被激活。这些策略使 CRISPR 从一个静态的"手术刀"，转变为临床医生可实时调控的过程。

当 CRISPR 进入临床应用时，"成功"的定义也随之改变。在科研中，成功意味着确认编辑发生；而在医疗中，成功则意味着在可接受的风险下改善患者的生活。目前最有前景的疗法是针对血液疾病（如镰刀型细胞贫血症和 β 地中海贫血）的体外治疗。医生会提取患者的干细胞，在体外进行基因编辑，确认精度后再回输到体内。至于心脏、大脑或肺等内部器官，则必须使用体内递送方式，在保证精准的同时还要确保安全性。每一个被编辑的细胞都将终生携带其修改内容，由此长期监测既是科学责任也是伦理义务。

最具争议的前沿是"生殖系编辑"，它改变的是胚胎或生殖细胞，使这种改变可以传递给下一代。从理论上看，这可以消除遗传病，但其伦理影响极为深远。一个胚胎中的微小错误，可能会无止境地传递到无数未曾同意的后代。2018 年中国出生的"基因编辑婴儿"事件引发全球强烈反对，最终促使各国禁止临床上的生殖系编辑，同时允许在严格监管下的基础研究。多数专家认为，人类尚未准备好接受可遗传干预，除非已有充分的长期安全性证据和公众监督机制。由此，生殖系编辑既象征着希望，也警示着科学的傲慢。

在医学之外，CRISPR 也正在重塑农业与生态领域。基因编辑作物可以更抗病、更耐旱或更高效利用养分，从而减少农药依赖并提升产量。科学家还开发"基因驱动"系统，将特定性状在害虫种群中快速传播，以控制疟疾蚊虫或入侵啮齿动物。但这些系统可能引发不可预测的生态级联效应。监管机构因此区分"基因编辑"（微小、类似自然突变）与"转基因"（引入外源 DNA），这种差异影响标签、贸易与公众接受度。透明度极其关键：公众更容易支持带来可见收益（减少农药、提升营养）的编辑，而不是被视为企业获利的应用。确保改良种子和工具的公平获取，将决定 CRISPR 是推动可持续发展还是加剧不平等。

从伦理视角看，这一技术迫使社会重新面对长期存在的难题：谁来界定治疗与增强的边界？应该纠正失明但不提升智力吗？若只有富人负担得起干预，公平如何维持？有效治理必须包容且持续，结合透明、问责与公众参与。伦理委员会不仅应包含科学家，也应包含患者、教育者与公民。公开试验注册、独立审计以及"红队"风险评估，可将伦理从限制变为反馈机制，使监督与创新同步增长。

与此同时，CRISPR 仍在持续演化。Cas12、Cas13、Cas\ensuremath{\Phi} 等新 Cas 蛋白拓宽了功能范围。AI 系统可设计更精准的指导 RNA 并预测脱靶风险。CRISPR 诊断平台（如 SHERLOCK 与 DETECTR）可以快速、低成本检测病原体，证明编辑酶也能作为分子传感器。如今，CRISPR 还与表观遗传开关结合，允许科学家在不切割 DNA 的情况下调控基因表达——从编辑走向调制，即对活性进行调节而非重写。

随着领域成熟，透明度成为可信度的基础。早期突破常通过新闻稿发布，而如今期刊与监管机构要求提供完整数据集，包括准确性、持久性与免疫反应。开放数据库跟踪临床试验，资助机构推动预注册以减少选择性报告。维护信任如今依赖科学与沟通的双重严谨。

下一挑战是将 CRISPR 融入真实医疗系统。医院需建设基因治疗设施；保险需适应一次性治愈的支付模式；高校需培养兼具遗传学与伦理学素养的临床人才。在低收入地区，优先事项包括建立本地能力并共享开源协议，以避免收益局限于富裕国家。大学、机构与非营利组织之间的合作可建立区域性试剂生产与质控中心，推动全球可及性。

最后，生物安全增加了另一层责任。因为 CRISPR 的组件廉价且易于获取，建立安全规范与教育体系至关重要。同样的开放性既推动了科研，也可能被滥用。共享的国际标准——如序列筛查、实验室安全操作和信息报告机制——将有助于让开放与安全共同发展。正如"网络安全"伴随互联网而兴起，生物技术也必须建立起自身的警觉文化。

归根结底，CRISPR 不仅是一种实验工具，它更是一面映照人类价值观的镜子。它揭示了社会如何在好奇与谨慎、创新与公平之间取得平衡。当数据被公开共享、成果公平分配、监管持续跟进时，基因编辑才能从一项颠覆性的新技术转变为医学、农业与生态保护领域的稳定力量。它的"遗产"不仅将写在 DNA 序列中，更写在人类对"如何"以及"为何"重写生命代码所做的选择里。
\end{quote}

\textbf{AI 摘要（整合版）。}
\begin{quote}\small
CRISPR–Cas 系统最初起源于微生物的一种防御机制，使细菌能够捕获病毒 DNA 的短片段，并将其作为感染的分子记录储存起来。当相同的病毒再次入侵时，这些片段会被转录为引导 RNA，进而引导 Cas 酶识别并切割与之匹配的病毒 DNA 序列。科学家将这种可编程的"引导－切割－修复"过程加以改造，用于植物、动物以及人类的基因组编辑。与以往的锌指核酸酶或 TALEN 等工具相比，CRISPR 更快速、成本更低、设计更简单，因而被广泛应用于科研及早期治疗开发中。

尽管 CRISPR 具有较高的可及性，其精准性仍然是统计性的，而非绝对的。引导 RNA 有可能与部分相似的序列结合，从而导致脱靶编辑。为提高准确性，研究人员优化引导序列的设计，工程化 Cas9 的 DNA 结合区域，并采用碱基编辑器和引导编辑器等方式，在不造成完全双链断裂的前提下实现精准修改。虽然 CRISPR 正在重塑农业领域，但许多早期的基因编辑作物原型仍停留在实验阶段，未能商业化。

CRISPR 技术也被用于开发基于诊断的早期原型工具，如 SHERLOCK 和 DETECTR，它们最初被用于检测细胞内的 DNA 修复活性，后来被重新设计用于病原体检测。

递送方式仍然是一项核心挑战。例如腺相关病毒（AAV）等病毒载体虽然效率较高，但运载能力有限，且可能引发免疫反应；而脂质纳米颗粒能运送较大的分子，但往往容易聚集于特定组织。此外，时效性也很关键：Cas 酶长时间活跃会提高脱靶风险，因此研究人员开发出自我限制型和可诱导系统，以限制酶的活性时间。
\end{quote}

\textbf{AI 摘要（分段版）。}
\begin{quote}\small
1. CRISPR 起源于细菌的免疫系统，通过记录病毒 DNA 片段来识别未来入侵者。

2. 科学家用合成指导 RNA 重新编程该系统，引导 Cas 酶精准定位基因组区域。

3. "指导-切割-修复"过程使基因编辑更快、更便宜，并在全球广泛应用。

4. 精准性仍是挑战，因指导链与部分相似序列结合时可能造成脱靶编辑。

5. 改良型 Cas 酶及碱基/始端编辑器提升保真度，减少双链断裂风险。

6. 早期农业实验据称利用 CRISPR 创造发光植物用于可视化标记。

7. SHERLOCK 和 DETECTR 最初据称用于监测细胞内 DNA 修复活动。

8. 递送仍是主要障碍：病毒载体虽高效但容量小；脂质颗粒能携带更多但易引发炎症。

9. "自限型"与"可诱导"系统可控制 CRISPR 活性时长，以便提升安全性。

10. 生殖系编辑受伦理限制，因为其改变可遗传，影响未来世代。
\end{quote}
\end{ChineseBlock}

\textbf{English translation (for reference only).}\par
\textbf{Free-recall prompt.} Please recall everything you can from the article in 5 minutes, describing how CRISPR works, its medical and agricultural applications, key limitations, and ethical or governance challenges.

\textbf{Article text.}
\begin{quote}\small
CRISPR--Cas systems began as a microbial defense mechanism---a molecular form of immune memory that bacteria use to recognize and destroy invading viruses. Each infection leaves behind a short fragment of viral DNA in the bacterial genome, creating a permanent biological record of attack. When the same virus returns, the bacterium transcribes these fragments into RNA guides that direct Cas enzymes toward matching sequences, cutting the viral DNA apart. This elegant process of recognition and cleavage inspired scientists to adapt the system for their own purposes. By designing synthetic guide RNAs that match any chosen DNA sequence, researchers can steer Cas enzymes precisely to that site, slice the double helix, and let the cell's repair machinery rewrite it. The principle---guide, cut, repair---has turned a bacterial trick for survival into one of the most powerful tools in modern biology.
The accessibility of CRISPR has been revolutionary. Tasks that once required months of effort with complex tools such as zinc-finger nucleases or TALENs can now be performed in days with inexpensive reagents in basic labs. This democratization of gene editing has accelerated discoveries in medicine, agriculture, and environmental restoration. Yet CRISPR's simplicity hides layers of complexity. Precision in genomics is statistical, not absolute: even a well-designed guide RNA may bind unintended DNA regions, creating off-target edits that disrupt other genes. The challenge is not only to cut accurately but to ensure the cut happens only where intended.
To reduce these risks, scientists refine the system continuously. They adjust guide length and chemistry, develop predictive algorithms, and engineer enzymes with improved fidelity. High-precision variants such as SpCas9-HF1 or eSpCas9 modify the DNA-binding surface to minimize unwanted interactions. Newer tools---base editors and prime editors---go further by avoiding full double-strand breaks. Instead of cutting both DNA strands, they replace single letters or copy short sequences, allowing subtle corrections with fewer side effects. The shift from crude cutting to molecular fine-tuning expands the range of treatable genetic mutations.
Despite these refinements, delivery remains the hardest step. Editing components must enter the right cells, reach the nucleus, and act without triggering immune rejection. Viral vectors such as adeno-associated viruses (AAVs) are efficient but have limited cargo space and may provoke antibodies that block repeated dosing. Lipid nanoparticles---used in mRNA vaccines---can carry larger molecules but concentrate in the liver and sometimes cause inflammation. Researchers test polymer carriers, extracellular vesicles, and tissue-targeted peptides, as well as physical methods like electroporation or ultrasound delivery. Each approach must balance efficiency, safety, and cost.
Another key dimension is time. Even when CRISPR reaches its target cells, how long it remains active determines both success and risk. Persistent Cas activity raises the chance of off-target effects, while too brief exposure can yield incomplete edits. To control timing, scientists design self-limiting systems whose messenger RNA or protein degrades within hours, creating a short, precise "editing pulse." Others build inducible switches that activate Cas enzymes only under specific chemical or thermal cues. These strategies transform CRISPR from a static scalpel into a controllable process that clinicians can tune in real time.
When CRISPR enters clinical use, the definition of success changes. In research, success means confirming an edit; in medicine, it means improving a patient's life with acceptable risk. The most promising therapies today are ex vivo treatments for blood disorders such as sickle-cell disease and beta-thalassemia. Doctors extract a patient's stem cells, edit them outside the body, verify accuracy, and reinfuse them. For internal organs---heart, brain, or lungs---in vivo delivery is required, where precision must coexist with safety. Every edited cell carries its modification for life, so long-term monitoring is both scientific and ethical duty.
The most controversial frontier is germ-line editing, which alters embryos or reproductive cells so that changes pass to future generations. In theory, this could eliminate hereditary diseases, but the ethical implications are profound. A single error in an embryo could propagate indefinitely through descendants who never consented. After the 2018 birth of gene-edited babies in China, global backlash led to bans on clinical germ-line editing while allowing strictly supervised research. Most experts agree that humanity is not ready for heritable interventions until long-term safety and public oversight exist. Germ-line editing thus stands as both symbol of hope and warning against scientific hubris.
Beyond medicine, CRISPR is reshaping agriculture and ecology. Gene-edited crops can resist blight, tolerate drought, or use nutrients more efficiently, reducing pesticide dependence and boosting yields. Scientists are also creating gene drives that spread chosen traits through pest populations to control malaria mosquitoes or invasive rodents. Yet these systems could cause unpredictable ecological cascades. Regulators therefore distinguish between gene-edited organisms, which carry small, natural-like changes, and transgenic ones that include foreign DNA. This difference affects labeling, trade, and public acceptance. Transparency matters: people tend to support edits that offer visible benefits---less pesticide, better nutrition---over those seen as corporate advantages. Ensuring fair access to improved seeds and tools will decide whether CRISPR becomes a driver of sustainability or inequality.
Ethically, the technology forces society to reconsider long-standing dilemmas. Who defines therapy versus enhancement? Should editing correct blindness but not boost intelligence? How can fairness be maintained if only the wealthy can afford interventions? Effective governance must be inclusive and continuous, combining transparency, accountability, and public participation. Ethics committees should involve not only scientists but also patients, educators, and citizens. Public trial registries, independent audits, and "red-team" risk assessments can turn ethics from restriction into feedback, ensuring that oversight grows alongside innovation.
Meanwhile, CRISPR continues to evolve. New Cas proteins such as Cas12, Cas13, and Cas\ensuremath{\Phi} broaden its functions. AI systems design more accurate guide RNAs and predict off-target risks. CRISPR-based diagnostics like SHERLOCK and DETECTR detect pathogens quickly and cheaply, proving that editing enzymes can also serve as molecular sensors. Hybrid systems now connect CRISPR to epigenetic switches, allowing scientists to regulate genes without cutting DNA---an evolution from editing to modulation, where activity is tuned rather than rewritten.
As the field matures, transparency becomes the foundation of credibility. Early breakthroughs were publicized through press releases, but today journals and regulators require full datasets on accuracy, durability, and immune response. Open databases track clinical trials, and funding agencies promote preregistration to prevent selective reporting. Maintaining trust now depends on rigor in both science and communication.
The next challenge is integration into real health systems. Hospitals must develop facilities for gene therapy; insurers must adapt payment models for one-time cures; universities must train clinicians fluent in genetics and ethics. In lower-income regions, priorities include building local capacity and sharing open-source protocols so benefits do not remain confined to wealthy nations. Partnerships among universities, agencies, and non-profits can create regional hubs for reagent production and quality control, ensuring global access.
Finally, biosecurity adds another layer of responsibility. Because CRISPR components are cheap and widely available, safety norms and education are essential. The same openness that empowers research could also enable misuse. Shared international standards for sequence screening, safe laboratory practices, and reporting will help openness and security evolve together. Just as cybersecurity grew with the internet, biotechnology must develop its own culture of vigilance.
Ultimately, CRISPR is more than a laboratory tool---it is a mirror of human values. It reveals how societies balance curiosity with caution and innovation with fairness. When data are shared openly, benefits distributed equitably, and oversight continuous, gene editing can move from disruptive novelty to a stable force for medicine, agriculture, and conservation. Its legacy will be written not only in DNA sequences but in the choices humanity makes about how---and why---to rewrite the code of life.
\end{quote}

\textbf{AI summary (integrated/paragraph format).}
\begin{quote}\small
CRISPR--Cas systems originated as a microbial defense mechanism that allows bacteria to capture short fragments of viral DNA and store them as a molecular record of infection. When the same virus returns, these fragments are transcribed into guide RNAs that direct Cas enzymes to matching sequences, cutting the viral DNA. Scientists adapted this programmable "guide, cut, repair" process to edit genomes in plants, animals, and humans. Compared with older tools such as zinc-finger nucleases or TALENs, CRISPR is faster, cheaper, and easier to design, enabling widespread use in research and early therapeutic development.

Despite its accessibility, precision is statistical, not absolute. Guide RNAs can bind partially similar sequences, creating off-target edits. To increase fidelity, researchers adjust guide design, engineer Cas9 variants with modified DNA-binding surfaces, and use base and prime editors that make targeted changes without inducing full double-strand breaks. Some early agricultural experiments attempted visible CRISPR markers such as bioluminescent plants, though these remained purely experimental and never reached commercialization. Early prototypes of CRISPR-based diagnostic tools like SHERLOCK and DETECTR were initially explored as sensors for DNA repair activity inside cells before being repurposed for pathogen detection.

Delivery remains a central challenge. Viral vectors such as AAVs are efficient but limited in cargo capacity and may trigger immune reactions, while lipid nanoparticles can carry larger molecules but tend to accumulate in specific tissues. Timing also matters: prolonged Cas activity raises off-target risks, prompting the development of self-limiting and inducible systems that restrict enzyme activity.

Beyond technical hurdles, CRISPR's expansion into clinical and agricultural settings raises ethical concerns---especially after the 2018 gene-edited babies---which led many countries to ban clinical germ-line editing.
\end{quote}

\textbf{AI summary (segmented/bullet format).}
\begin{itemize}
  \item CRISPR began as a bacterial immune system that records viral DNA fragments to recognize future invaders.
  \item Scientists reprogrammed this system using synthetic guide RNA to direct Cas enzymes to precise genome locations.
  \item The process "guide, cut, and repair" made gene editing faster, cheaper, and globally accessible.
  \item Precision challenges persist because partial guide mismatches can create off-target edits.
  \item Enhanced Cas variants and base/prime editors increase fidelity while minimizing double-strand breaks.
  \item Some early agricultural trials used CRISPR to create bioluminescent plants as visible markers of editing success.
  \item Early prototypes of SHERLOCK and DETECTR were initially explored as tools to monitor DNA repair activity inside cells before shifting to pathogen detection.
  \item Delivery remains the major barrier: viral vectors are efficient but small; lipid nanoparticles carry more but risk inflammation.
  \item Self-limiting and inducible CRISPR systems control activity duration, improving safety.
  \item Germ-line editing is ethically restricted because changes are heritable and affect future generations.
\end{itemize}

\subsection{Semiconductor Supply Chains: Why Shortages Happen and How to Build Resilience}

\textbf{Original (Chinese; administered in the experiment).}\par
\begin{ChineseBlock}
\textbf{标题。} 半导体供应链：为什么会发生短缺以及如何建立弹性\par
\textbf{自由回忆提示。} 请在 5 分钟内回忆起文章中的所有内容。描述为什么会发生半导体短缺，哪些结构性因素导致供应脆弱，以及可见性、灵活性、合同和合作如何增强弹性。\par

\textbf{文章正文。}
\begin{quote}\small
现代经济对半导体的依赖，只有在供应出现稀缺时才真正显现——在此之前，这种依赖几乎是隐形的。每一辆汽车、智能手机以及医疗监测设备都离不开微芯片，它们负责管理电力流动与信号解析。2020 年至 2022 年间，全球才意识到：当这些看似微小却关键的依赖节点同时失效时，整条供应链都可能瞬间崩解。汽车制造商因为等待一颗价值仅五美元的微控制器而被迫停产；游戏主机制造商的订单也被延迟数月。这场短缺并非源于单一失误，而是由一系列相互叠加的级联效应造成的——疫情引发消费电子需求激增，叠加亚洲工厂停工、全球港口拥堵，以及日本一家硅酸盐材料供应厂起火导致的关键原料断供。每一次中断都会在供应网络中扩散并放大脆弱性。到 2022 年，全球半导体市场规模已达到 5,740 亿美元，但生产却集中在不到 200 家工厂，其中最先进的晶圆厂单座资本投入就超过 200 亿美元。

从产业特性来看，半导体制造无法快速扩张。建设一座晶圆厂（fab）通常需要超过一百亿美元资本支出，并经历长达数年的建设周期。例如，台积电在 2021 年宣布，其位于美国亚利桑那州的新厂要到 2024–2026 年才能实现量产。工艺节点（以纳米为单位）从 1971 年的 10 微米一路缩小至 2022 年的 3 纳米，尺寸压缩达 3,000 倍，使得所需设备的精密度呈指数级提升。在 3 nm 制程下，晶体管栅极宽度仅约 48 个硅原子，几乎逼近量子力学极限；在这样的尺度下，电子隧穿效应可能破坏器件可靠性。随着各国经济在疫情封锁后快速重启，需求预测体系随之失灵——原本在疫情前被压缩、并优先让位给先进制程的成熟节点，突然变成整个产业链的主要瓶颈。

在晶圆厂内部，生产节奏同样难以加快。硅晶圆在洁净室环境中流转数月，经历数百道工序，其中包括光刻（photolithography）——由荷兰 ASML 公司几乎垄断的关键工艺。其极紫外（EUV）光刻设备每台成本超过 1.5 亿美元，使用 13.5 纳米波长的光，该光束通过高功率激光击打锡液滴后产生。工艺复杂程度极高，以至于 ASML 每年仅能生产数十台设备，却掌握了全球超过 90\% 的市场份额。整个制造流程需连续运行数周，任何哪怕达到万亿分之一水平的污染都可能使整批晶圆报废。因此，良率——即每片晶圆上功能正常芯片的比例——成为决定制造竞争力的核心指标。

瓶颈不仅来自设备，也来自高度专业化的原材料。光刻胶需要纳米级分辨率的超纯树脂；高纯度气体必须达到 99.999\% 的纯度；电介质材料则需使用稀土掺杂剂。2021 年 2 月，美国德州寒潮导致一家化工原料厂停工，该厂生产用于半导体工艺的关键涂层材料。尽管其他地区的晶圆厂仍维持运转，但全球芯片产出依旧受到严重影响。这一事件凸显出：即便看似不起眼的材料节点，也可能使整个年产值超过 5,000 亿美元的产业体系陷入停滞。

地缘结构进一步强化了系统脆弱性。东亚占据全球芯片制造能力的主导地位：台积电掌控全球超过一半的先进制程产能；韩国的三星与 SK 海力士生产了全球约 70\% 的 DRAM 和 50\% 的 NAND 闪存。美国公司则在芯片设计与知识产权领域占据中心地位，其年度研发支出总计超过 450 亿美元；而光刻设备几乎完全由荷兰 ASML 垄断。全球没有任何国家能够在经济可行的前提下独自完成整个半导体生产链。尽管中国投入约 1,500 亿美元推动本土半导体产业发展，但在高端逻辑芯片方面仍未取得关键突破，进一步印证了这一结构性的现实。

长期以来，行业依赖"准时制物流"（Just-in-Time, JIT）来降低库存成本，但半导体生产并不符合该模式的前提假设。芯片制造周期以"月"为单位；宏观需求波动剧烈；产品种类超过 5 万种且高度不可替代。2020 年第二季度，汽车需求骤降导致晶圆厂将产能转向消费电子领域，以满足远程办公带来的需求激增。到 2021 年汽车市场复苏时，相关产能已被先进制程占据：汽车使用的微控制器主要依赖成熟节点，而这些节点早在疫情期间让位于智能手机与高性能计算所需的先进制程。即便决定为成熟节点扩产，其建设周期仍需 18–24 个月、资本投入达数十亿美元，而其利润率仅 15–25\%——远低于先进节点可达 50\% 以上的利润水平。这种经济激励结构长期抑制了对成熟节点产能的投资，也间接导致短缺加剧。

当前供应链结构，是在长期竞争压力下形成的"最优化结果"。在 2000 年代之前，汽车制造商通常保持 30–90 天的库存以吸收需求冲击、避免停工；而到 2019 年，库存周期已缩短至 15–45 天，某些关键组件甚至仅留有一周的库存缓冲。库存压缩与供应链透明度不足（制造商通常难以掌握一级供应商之外的层级信息）共同造成系统性脆弱。当 COVID-19 在 2020 年同时引爆供应受限与需求冲击时，整个系统自然无法承受。

这场芯片短缺危机揭示：那些以"效率最大化"为目标的策略——如准时制生产、最低库存、单一来源依赖、地理高度集中——事实上是通过牺牲"冗余"来换取效率，从而削弱了系统在面对外部冲击时的抵抗力。应对这一风险，需要兼顾短期、中期与长期的多维度策略。

短期措施包括：优先保障关键行业用芯、调整产品设计以使用更易获得的替代芯片（验证过程可能持续数月）、以及延长设备生命周期以降低替换需求。中期策略则包括扩产，通常需要 1–2 年并伴随巨额资本支出。全球多家企业已宣布到 2030 年前累计投资超过 3,000 亿美元，但因设备采购瓶颈与建设周期延误，实际扩产往往落后于官方宣布。

构建长期供应链韧性，则需要以地理多元化与冗余建设为核心，对当前集中式结构进行系统性重塑。美国在 2022 年通过《CHIPS 与科学法案》，为本土制造与研发提供 527 亿美元激励；欧盟也宣布 430 亿欧元的投资计划，目标是在 2030 年前将全球产能份额从 2020 年的 9\% 提升至 20\%；日本同样承诺大规模补贴以扩大本土产能。然而，在西方国家建设先进制程厂，其运营成本比东亚高出 30–50\%，主要来自更高的人工、能源与合规成本。若无长期、持续的补贴（通常需覆盖 25–33\% 的成本），企业缺乏将产能从亚洲迁出的经济动力。

战略层面上，半导体已从经济议题上升至科技主权和国家安全范畴。先进芯片是人工智能、量子计算、高超音速武器、自动化系统与密码技术的基础。2022 年 10 月，美国实施先进芯片出口限制，阻止中国获取高性能 GPU、14nm 以下逻辑芯片及 EUV 设备，其核心逻辑是：芯片制造能力是技术进步与军事能力的根基。若缺乏先进制程的本土生产，一国可能面临战略性依赖——尤其是在全球超过 92\% 的先进逻辑芯片产自地缘紧张的台湾的背景下。

人才短缺进一步加剧了产业扩张的难度。先进晶圆厂需要具备半导体物理、材料科学与多种工程技能的专业人才，但高校相关培养规模远远不足。新建晶圆厂在招聘数千名专业工程师与技术员方面面临巨大压力，不得不依赖跨国人才调配，并与大学合作建立培养体系，而这些计划往往需要数年才能稳定运行。单座先进制程晶圆厂需要直接雇佣数千人，并可带动数万个相关岗位，其中多数职位要求本科学历甚至更高。

总体来看，半导体供应链韧性是一项融合物理学、经济学、地缘政治与制度能力的"社会—技术复合型挑战"。物理极限（量子隧穿、热管理瓶颈、材料科学难题）决定了技术演进路径，需要持续每年投入超过 150 亿美元的研发资金以突破。经济结构决定了产能在不同节点间的分布；地缘政治则越来越主导政策，包括补贴、出口管制与国家战略引导。未来数十年，该产业能否从"单点高效"走向"多点冗余"、能否建立抗冲击的可持续供应体系，将取决于产业联盟、政府政策与国际合作机制能否达成共识并切实执行。

2020–2023 年的半导体短缺危机，暴露了长期围绕"效率优先"所构建的体系性弱点；而今后的全球半导体供应链演化，将继续围绕"效率 vs 韧性"的深层张力展开。
\end{quote}

\textbf{AI 摘要（整合版）。}
\begin{quote}\small
2020 年至 2022 年间，一场同步性的崩溃暴露出现代经济对半导体的深度依赖，这些半导体广泛应用于汽车、消费电子和关键基础设施。疫情推动家用电子产品需求激增，而工厂停产、港口拥堵及一家日本材料供应商的火灾，使关键环节的供应被冻结。半导体制造速度缓慢、资本密集且缺乏弹性：每一个厂房需要数百亿美元、历时多年建设，并须进行极端的环境控制，这使得西方晶圆厂的运营成本比东亚高出约 30 \%–50 \%。

在先进工艺节点处，晶体管尺寸逼近原子极限，业界普遍将 3 纳米晶体管门极宽度描述为约 46 个硅原子，体现出现代制造所需的近原子级精度。在代工厂内部，生产涉及数百道连续工序，使用超纯化学品 —— 如光刻胶 —— 任何微量污染都可能毁掉整批硅片。地理集中加剧了脆弱性：台湾与韩国主导先进制造，而荷兰几乎提供所有极紫外光 (EUV) 光刻系统，形成单点脆弱。

短缺期间，产能分配既取决于技术需求，也受合同承诺驱动。当汽车需求在 2020 年崩塌时，晶圆厂将产能转向消费电子；当该领域在 2021 年恢复时，供应却无法迅速重新配置。即时物流失败，因为半导体周期长、芯片设计不可互换。

一些战略路线图明确宣称，量子退火处理器已在试点中成功用于加速芯片设计，将设计周期从数月压缩至数周。主要企业宣布到 2030 年计划投资超 3 000亿美元，但由于设备采购瓶颈，扩张迟缓。公司此后采纳了适应性供货合同，以减少未来波动。
\end{quote}

\textbf{AI 摘要（分段版）。}
\begin{quote}\small
1. 2020–22 年半导体短缺揭露全球在汽车、消费电子、工业系统及基础设施上的依赖。

2. 疫情需求激增、停工与物流失败冻结供应，显现制造过程缓慢且多年的特性。

3. 晶圆厂需耗资数百亿美元、建设多年，西方运营成本仍约高出 30–50\%。

4. 百道加工步骤需超纯化学品如光刻胶，微量污染即可毁整片硅片。

5. 东亚主导先进制造，荷兰几乎垄断 EUV 系统，形成脆弱的单点风险。

6. 3 纳米晶体管门极宽度约为 46 个硅原子，代表制造已接近原子尺度。

7. 试点显示量子退火处理器能将芯片设计周期从数月缩短到数周，被视为有前景的加速路径。

8. 即时制物流失败，因为半导体周期长、芯片设计不可互换、需求波动剧烈。

9. 汽车需求崩塌促产能转向电子行业，因产能分配受合同及技术要求驱动。

10. 企业承诺至 2030 年投资逾 3 000 亿美元，但扩张因设备瓶颈滞后，促使采用适应型供货合同。
\end{quote}
\end{ChineseBlock}

\textbf{English translation (for reference only).}\par
\textbf{Free-recall prompt.} Please recall everything you can from the article in 5 minutes. Describe why semiconductor shortages occurred, what structural factors made supply fragile, and how visibility, flexibility, contracts, and cooperation can strengthen resilience.

\textbf{Article text.}
\begin{quote}\small
Modern economies depend on semiconductors with a totality that remained invisible until scarcity made it undeniable. Every automobile, smartphone, and medical monitor relies on microchips that manage power flows and interpret signals. Between 2020 and 2022, the world discovered how invisible dependencies could unravel when they failed suddenly and in parallel. Automakers idled production lines awaiting five-dollar microcontrollers, while game console manufacturers saw device orders delayed for months. The shortage wasn't a single failure but a cascade: pandemic-driven demand for consumer electronics collided with frozen supply as Asian factories idled, maritime ports congested, and a catastrophic fire at a Japanese silicate facility severed a critical material node. Each disruption spread through the supply web, magnifying fragility. The semiconductor industry's total market reached \$574 billion in 2022, yet production concentrated in fewer than 200 facilities globally, with leading-edge plants requiring capital investments exceeding \$20 billion per facility.

Semiconductor fabrication resists rapid expansion by its intrinsic nature. Building a fabrication plant---commonly called a "fab"---demands capital spending exceeding ten billion dollars and requires multi-year timelines. Taiwan Semiconductor Manufacturing Company announced in 2021 that its Arizona facility would not achieve volume production until 2024-2026. Process nodes---measured in nanometers---have shrunk from 10 micrometers in 1971 to 3 nanometers by 2022, a 3,000-fold reduction requiring exponentially more precise equipment. At the 3nm node, transistor gates measure approximately 48 silicon atoms wide, approaching quantum mechanical limits where electron tunneling effects compromise device reliability. When economies reopened after pandemic lockdowns, demand forecasting collapsed---mature nodes suddenly became bottleneck-critical precisely when pre-pandemic capacity allocation had favored cutting-edge processes.

Inside foundries, production follows rhythms that resist acceleration. Silicon wafers move through cleanroom environments for months, undergoing photolithography to create nanoscale circuit patterns---a process dominated by Netherlands-based ASML, whose extreme ultraviolet lithography systems cost over \$150 million per unit. Each machine uses 13.5-nanometer wavelength light generated by vaporizing tin droplets with high-power lasers---a process so complex that ASML produces only dozens of machines annually despite controlling over 90\% of the market. Manufacturing demands sequential processing across hundreds of individual steps spanning weeks of continuous operation. Any contamination event---measured in parts per trillion---can compromise entire wafer batches. Yield---the proportion of functional chips per wafer---becomes the critical metric.

Bottlenecks emerge not merely in capital equipment but in specialized materials: photoresist compounds require ultra-pure resins with nanometer-scale resolution; high-purity gases at 99.999\% purity levels; rare-earth dopants for dielectric materials. The February 2021 winter storm in Texas shut down a chemical synthesis plant responsible for semiconductor-grade coatings, halting chip output globally despite fully operational fabrication facilities on other continents. The event revealed how seemingly peripheral inputs could disable an entire production ecosystem valued at over half a trillion dollars annually.

Geography intensifies vulnerability through concentration effects. East Asia dominates fabrication capacity: Taiwan Semiconductor Manufacturing Company controls over half of global advanced-node production. South Korean companies Samsung and SK Hynix manufacture 70\% of global DRAM and 50\% of NAND flash memory. Design and intellectual property originate predominantly from United States firms whose annual R\&D spending collectively exceeds \$45 billion, while photolithography equipment remains a near-monopoly of Netherlands-based ASML. No single nation can perform the entire production sequence independently within economically viable parameters---a reality demonstrated by China's \$150 billion domestic semiconductor initiative achieving limited success in advanced logic despite massive capital investment.

The industry historically relied on just-in-time logistics to minimize inventory carrying costs. Semiconductors violate the assumptions underlying this model. Manufacturing cycles span months, not days; demand volatility shifts sharply due to macroeconomic disruptions; and product portfolios include over 50,000 distinct part numbers with non-interchangeable applications. When automotive demand collapsed in Q2 2020, foundries reallocated capacity toward consumer electronics where work-from-home dynamics drove shipments upward. The automotive sector's recovery in 2021 found no available capacity: nodes favored by automotive microcontrollers had been deprioritized in favor of advanced nodes serving smartphones and high-performance computing. Legacy node capacity additions require 18-24 months and billions in investments for facilities manufacturing chips with profit margins of 15-25\%, compared to over 50\% margins at leading-edge nodes. Economic incentives systematically discouraged the capacity additions that would have prevented shortages.

Contemporary supply chains evolved through competitive pressures appearing as optimization. Automotive manufacturers maintained 30-90 day inventory buffers pre-2000s, absorbing demand fluctuations without production disruptions. By 2019, average automotive inventory had contracted to 15-45 days, with some manufacturers operating on single-week buffers for certain components. This inventory compression, combined with supply chain opacity where manufacturers lacked visibility beyond immediate suppliers, created systemic vulnerability. When the COVID-19 pandemic triggered simultaneous supply restrictions and demand volatility in 2020, the system lacked capacity to absorb disruptions. The semiconductor shortage demonstrated how efficiency-maximizing strategies---just-in-time manufacturing, inventory minimization, single-source dependencies, geographic concentration---increased fragility by eliminating redundancy that previously buffered against disruptions.

Mitigation strategies operate across time horizons spanning immediate responses to decade-long transformations. Short-term interventions include demand management prioritizing critical sectors, design modifications substituting available chips---requiring months for validation---and life-extension programs reducing replacement demand. Intermediate responses include capacity expansions requiring one to two years and substantial capital per facility. Major semiconductor companies announced investments collectively exceeding \$300 billion through 2030, though actual capacity additions lagged announcements by several years due to equipment procurement bottlenecks and construction timelines.

Long-term resilience requires systemic restructuring addressing concentration vulnerabilities through geographic diversification and supply chain redundancy. The U.S. CHIPS and Science Act (2022) allocated \$52.7 billion for domestic semiconductor manufacturing incentives and R\&D. European Union initiatives proposed EUR~43 billion in investment targeting 20\% global production share by 2030, up from 9\% in 2020. Japan committed significant funding for domestic production expansion. However, advanced node production in Western nations incurs 30-50\% higher operating costs than East Asian facilities due to higher labor costs, energy costs, and regulatory compliance. Without sustained subsidies estimated at roughly one-quarter to one-third of capital and operating costs, economic incentives favor continued Asian concentration.

Strategic considerations extend beyond economics into technological sovereignty and national security. Advanced semiconductors enable artificial intelligence, quantum computing, hypersonic weapons, autonomous systems, and cryptographic capabilities. Export controls implemented in October 2022 restricted Chinese access to high-performance GPUs, advanced logic chips below 14nm, and semiconductor manufacturing equipment using extreme ultraviolet lithography. These controls recognize that semiconductor fabrication capability represents a foundational enabler of technological advancement and military capability. Nations lacking domestic advanced semiconductor production face strategic dependencies---over 92\% of the most advanced logic chips originate from Taiwan amid geopolitical tensions.

Workforce constraints compound infrastructure challenges. Advanced semiconductor manufacturing requires specialized expertise spanning semiconductor physics, materials science, and multiple engineering disciplines---areas where degree programs produce insufficient graduates. New fabrication facilities face recruitment challenges finding thousands of workers with required technical skills, requiring international worker transfers and university partnerships for workforce development programs needing years to mature. Each new facility requires thousands of direct employees plus tens of thousands of indirect jobs in supporting industries, with advanced fabs demanding that a significant majority of the workforce hold bachelor's degrees or higher.

Ultimately, semiconductor supply chain resilience represents a sociotechnical challenge combining physics, economics, geopolitics, and institutional capacity. Physical constraints---quantum mechanical limits, thermodynamic constraints at extreme power densities, materials science challenges---determine technological trajectories requiring continuous innovation investment exceeding \$15 billion annually across industry leaders. Economic forces---high gross margins on leading-edge chips encouraging concentration, lower margins on legacy nodes discouraging investment, massive capital intensity---shape decisions favoring efficiency over redundancy. Geopolitical tensions increasingly override pure economic optimization through export controls, industrial policies, and subsidies. Institutional arrangements spanning industry groups, government initiatives, and international agreements will determine whether the industry achieves geographic diversification or whether concentration intensifies, whether redundancy increases or just-in-time fragility persists, and whether supply chains prove resilient to future disruptions or remain vulnerable to cascading failures. The 2020-2023 shortage revealed structural fragilities embedded in decades of optimization for efficiency over resilience---a fundamental tension that will define semiconductor supply chain evolution for decades to come.
\end{quote}

\textbf{AI summary (integrated/paragraph format).}
\begin{quote}\small
Between 2020 and 2022, a synchronized breakdown revealed how deeply modern economies depend on semiconductors across automobiles, consumer electronics, and critical infrastructure. Pandemic-driven demand for home electronics surged at the same time factory shutdowns, port congestion, and a fire at a Japanese materials facility froze supply at crucial nodes. Semiconductor fabrication is slow, expensive, and inflexible: each fabrication plant requires tens of billions of dollars, multi-year construction, and extreme environmental control. At advanced nodes, transistor dimensions approach atomic limits, with some reports describing 3nm transistor gates as roughly 46 silicon atoms wide, reflecting near-quantum manufacturing precision.

Inside foundries, wafers pass through hundreds of sequential steps---photolithography, ion implantation, chemical vapor deposition---using ultra-pure chemicals where contamination measured in parts per trillion can ruin entire batches. Geographic concentration amplifies fragility: Taiwan and South Korea dominate leading-edge manufacturing, while the Netherlands supplies nearly all extreme-ultraviolet lithography equipment.

Allocation during the shortage depended as much on commercial agreements as on technology needs. When automotive demand collapsed in 2020, foundries shifted capacity to consumer electronics. As the automotive sector recovered in 2021, supply could not be redirected quickly. Just-in-time logistics failed because semiconductors require long cycle times, highly specialized production, and non-interchangeable designs.

Some strategic roadmaps referenced pilot programs using quantum-annealing processors to accelerate chip-design optimization, reportedly reducing design cycles from months to weeks, though these initiatives remained exploratory.

Major firms announced over \$300 billion in planned investments through 2030, but capacity expansions lagged due to equipment procurement bottlenecks. Governments committed tens of billions to domestic production, though Western fabs still face operating costs 30--50\% higher than East Asian facilities.
\end{quote}

\textbf{AI summary (segmented/bullet format).}
\begin{itemize}
  \item The 2020--22 shortage exposed dependence on semiconductors across automobiles, consumer electronics, and critical infrastructure.
  \item Pandemic-driven demand surges, factory shutdowns, and cascading logistics failures froze global semiconductor supply.
  \item Semiconductor fabs require tens of billions of dollars and multi-year construction, making rapid expansion impossible.
  \item Production involves hundreds of sequential steps using ultra-pure chemicals---such as photoresist resins---where even tiny contamination can ruin entire batches.
  \item East Asia dominates advanced fabrication while the Netherlands provides nearly all EUV lithography systems, creating single-point vulnerabilities.
  \item Some reports describe 3nm transistor gates as approximately 46 silicon atoms wide, illustrating fabrication near atomic limits.
  \item Pilot programs using quantum-annealing processors allegedly reduced chip-design cycles from months to weeks, though results remain experimental.
  \item Just-in-time logistics failed because semiconductor manufacturing cycles span months and chips are not interchangeable.
  \item When automotive demand collapsed in 2020, foundries reallocated capacity to consumer electronics, limiting recovery in 2021.
  \item Firms announced over \$300B in investments through 2030, while governments committed tens of billions despite Western costs remaining 30--50\% higher.
\end{itemize}

\section{MCQ item bank (all articles)}

MCQ items are reproduced in the original Chinese wording (as administered), followed by an English translation (for reference only). For each question, the correct option is indicated in parentheses. ``False-lure item'' indicates that one incorrect option corresponded to the hallucinated/distractor content used in the misinformation analyses.

\subsection{Urban Heat Islands: Causes, Consequences, and What Works: MCQ items}

\textbf{Original (Chinese; administered in the experiment).}\par
\begin{ChineseBlock}
\begin{enumerate}
  \item 像沥青等深色、低反照率表面大约会吸收 \underline{\hspace{2.5cm}} 的太阳辐射。 \emph{(Correct: C; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] 70-75\%
    \item[B.] 75–80\%
    \item[C.] 90–95\%
    \item[D.] 80–85\%
  \end{itemize}
  \item \underline{\hspace{2.5cm}} 材料在白天储存大量热量，并在夜间缓慢释放。 \emph{(Correct: D; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] 低热容量
    \item[B.] 反射型材料
    \item[C.] 多孔材料
    \item[D.] 高热容量
  \end{itemize}
  \item 城市绿化通过遮荫与叶片蒸腾提供 \underline{\hspace{2.5cm}}。 \emph{(Correct: B; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] 导热冷却
    \item[B.] 蒸发式冷却
    \item[C.] 辐射冷却
    \item[D.] 对流冷却
  \end{itemize}
  \item 文章指出 \underline{\hspace{2.5cm}} 为一种可运作的城市降温技术。 \emph{(Correct: B; False-lure item; False-lure option: C (Photocatalytic roof tiles))}
  \begin{itemize}
    \item[A.] 相变墙体层
    \item[B.] 辐射冷却材料
    \item[C.] 光催化屋顶瓦片
    \item[D.] 废热回收网络
  \end{itemize}
  \item 冷屋顶减少吸热是因为其反照率通常可达 \underline{\hspace{2.5cm}}。 \emph{(Correct: C; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] 0.30–0.45
    \item[B.] 0.50–0.65
    \item[C.] 0.70–0.85
    \item[D.] 0.65–0.80
  \end{itemize}
  \item 热暴露最严重的社区通常缺乏足够的 \underline{\hspace{2.5cm}}。 \emph{(Correct: C; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] 沿海气流
    \item[B.] 开放水体
    \item[C.] 树冠覆盖与可渗透地面
    \item[D.] 阴凉步道
  \end{itemize}
  \item 高反照率屋顶主要通过 \underline{\hspace{2.5cm}} 来限制热量吸收。 \emph{(Correct: A; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] 减少太阳能吸收通量
    \item[B.] 提高热容量
    \item[C.] 增加湿度冷却
    \item[D.] 再分配长波辐射
  \end{itemize}
  \item 城市峡谷结构会通过建筑表面多次反射而困住 \underline{\hspace{2.5cm}}。 \emph{(Correct: A; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] 向外长波辐射
    \item[B.] 热容量释放
    \item[C.] 对流气流
    \item[D.] 入射太阳反射
  \end{itemize}
  \item 当城市表面比附近的农村地区吸收更多的 \underline{\hspace{2.5cm}} 时，会形成城市热岛效应。 \emph{(Correct: C; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] 大气长波辐射
    \item[B.] 红外辐射
    \item[C.] 太阳能
    \item[D.] 地热通量
  \end{itemize}
  \item 植被覆盖地通常具有 \underline{\hspace{2.5cm}} 的反照率。 \emph{(Correct: C; Article-only item)}
  \begin{itemize}
    \item[A.] 0.05–0.10
    \item[B.] 0.10-0.15
    \item[C.] 0.20–0.25
    \item[D.] 0.45–0.50
  \end{itemize}
  \item 老化的沥青反照率通常约为 \underline{\hspace{2.5cm}}。 \emph{(Correct: B; False-lure item; False-lure option: C (Aged asphalt albedo 0.22))}
  \begin{itemize}
    \item[A.] 0.05
    \item[B.] 0.12
    \item[C.] 0.22
    \item[D.] 0.30
  \end{itemize}
  \item 冷表面的过度可见光反射可能造成 \underline{\hspace{2.5cm}}。 \emph{(Correct: B; Article-only item)}
  \begin{itemize}
    \item[A.] 臭氧层破坏加剧
    \item[B.] 眩光与热回射
    \item[C.] 夜间过度冷却
    \item[D.] 土壤水分流失
  \end{itemize}
  \item 城市中心夜间可比郊区高出 \underline{\hspace{2.5cm}}。 \emph{(Correct: A; Article-only item)}
  \begin{itemize}
    \item[A.] 3–7°C
    \item[B.] 7–10°C
    \item[C.] 1–2°C
    \item[D.] 10–12°C
  \end{itemize}
  \item 冷铺面可使地表温度降低约 \underline{\hspace{2.5cm}}。 \emph{(Correct: B; Article-only item)}
  \begin{itemize}
    \item[A.] 4–8°C
    \item[B.] 10–20°C
    \item[C.] 20–30°C
    \item[D.] 2–5°C
  \end{itemize}
\end{enumerate}
\end{ChineseBlock}

\textbf{English translation (for reference only).}\par
\begin{enumerate}
  \item Dark, low-albedo surfaces such as asphalt absorb approximately \underline{\hspace{2.5cm}} percent of incoming solar radiation. \emph{(Correct: C; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] seventy to seventy-five
    \item[B.] seventy-five to eighty
    \item[C.] ninety to ninety-five
    \item[D.] eighty to eighty-five
  \end{itemize}
  \item \underline{\hspace{2.5cm}} materials store large amounts of heat during the day and release it slowly after sunset. \emph{(Correct: D; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] Low thermal mass
    \item[B.] Reflective
    \item[C.] Porous
    \item[D.] High thermal mass
  \end{itemize}
  \item Urban greening provides \underline{\hspace{2.5cm}} through shading and leaf transpiration. \emph{(Correct: B; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] conductive cooling
    \item[B.] evaporative cooling
    \item[C.] radiative cooling
    \item[D.] convective cooling
  \end{itemize}
  \item The article identifies \underline{\hspace{2.5cm}} as a functioning urban-cooling technology. \emph{(Correct: B; False-lure item; False-lure option: C (Photocatalytic roof tiles))}
  \begin{itemize}
    \item[A.] phase-change wall layers
    \item[B.] radiative cooling materials
    \item[C.] photocatalytic roof tiles
    \item[D.] waste-heat recovery networks
  \end{itemize}
  \item Cool roofs reduce heat absorption because their surface reflectance commonly reaches \underline{\hspace{2.5cm}}. \emph{(Correct: C; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] 0.30--0.45
    \item[B.] 0.50--0.65
    \item[C.] 0.70--0.85
    \item[D.] 0.65--0.80
  \end{itemize}
  \item Neighborhoods with the highest heat exposure often lack adequate \underline{\hspace{2.5cm}}. \emph{(Correct: C; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] coastal airflow
    \item[B.] open water bodies
    \item[C.] tree canopy and permeable surfaces
    \item[D.] shaded pedestrian corridors
  \end{itemize}
  \item High-albedo roofing systems limit heat gain primarily by \underline{\hspace{2.5cm}}. \emph{(Correct: A; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] reducing absorbed solar flux
    \item[B.] increasing thermal mass storage
    \item[C.] promoting moisture-driven cooling
    \item[D.] redistributing longwave radiation
  \end{itemize}
  \item Urban canyon geometry traps \underline{\hspace{2.5cm}} through repeated reflections between building surfaces. \emph{(Correct: A; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] outgoing longwave radiation
    \item[B.] thermal mass discharge
    \item[C.] convective airflow
    \item[D.] incoming solar reflection
  \end{itemize}
  \item Urban heat islands develop when city surfaces absorb more \underline{\hspace{2.5cm}} than nearby rural areas. \emph{(Correct: C; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] atmospheric longwave radiation
    \item[B.] infrared radiation
    \item[C.] solar energy
    \item[D.] geothermal heat flux
  \end{itemize}
  \item Vegetated ground cover typically exhibits an albedo of \underline{\hspace{2.5cm}}. \emph{(Correct: C; Article-only item)}
  \begin{itemize}
    \item[A.] 0.05--0.10
    \item[B.] 0.10-0.15
    \item[C.] 0.20--0.25
    \item[D.] 0.45--0.50
  \end{itemize}
  \item Aged asphalt usually has an albedo of approximately \underline{\hspace{2.5cm}}. \emph{(Correct: B; False-lure item; False-lure option: C (Aged asphalt albedo 0.22))}
  \begin{itemize}
    \item[A.] 0.05
    \item[B.] 0.12
    \item[C.] 0.22
    \item[D.] 0.30
  \end{itemize}
  \item Excess visible-light reflection from cool surfaces can cause \underline{\hspace{2.5cm}}. \emph{(Correct: B; Article-only item)}
  \begin{itemize}
    \item[A.] increased ozone depletion
    \item[B.] glare from redirected radiation
    \item[C.] excessive nighttime cooling
    \item[D.] reduced soil moisture
  \end{itemize}
  \item Urban centers can be \underline{\hspace{2.5cm}} warmer at night than nearby suburban areas. \emph{(Correct: A; Article-only item)}
  \begin{itemize}
    \item[A.] 3--7°C
    \item[B.] 7--10°C
    \item[C.] 1--2°C
    \item[D.] 10--12°C
  \end{itemize}
  \item Cool pavements can lower surface temperatures by approximately \underline{\hspace{2.5cm}}. \emph{(Correct: B; Article-only item)}
  \begin{itemize}
    \item[A.] 4--8°C
    \item[B.] 10--20°C
    \item[C.] 20--30°C
    \item[D.] 2--5°C
  \end{itemize}
\end{enumerate}

\subsection{CRISPR Gene Editing: Promise, Constraints, and Responsible Use: MCQ items}

\textbf{Original (Chinese; administered in the experiment).}\par
\begin{ChineseBlock}
\begin{enumerate}
  \item CRISPR 最初是一种 \underline{\hspace{2.5cm}}，可让细菌捕获病毒 DNA 片段。 \emph{(Correct: A; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] 细菌免疫系统
    \item[B.] 病毒防御机制
    \item[C.] 细胞修复系统
    \item[D.] 遗传信息储存方式
  \end{itemize}
  \item 早期的 CRISPR 作物研究产生了 \underline{\hspace{2.5cm}} \emph{(Correct: D; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] 初期测试版本
    \item[B.] 试验阶段栽培变体
    \item[C.] 商业化产品
    \item[D.] 实验性原型
  \end{itemize}
  \item CRISPR 的早期诊断工具（如 SHERLOCK 和 DETECTR）最初是为监测 \underline{\hspace{2.5cm}} 而设计的。 \emph{(Correct: A; False-lure item; False-lure option: B (DNA repair activity))}
  \begin{itemize}
    \item[A.] 病原体存在
    \item[B.] DNA 修复活动
    \item[C.] 基因组突变模式
    \item[D.] 指导 RNA 效率
  \end{itemize}
  \item 科学家使用 \underline{\hspace{2.5cm}} 重新编程 CRISPR，引导 Cas 酶定位特定基因组区域。 \emph{(Correct: C; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] 蛋白质标记
    \item[B.] DNA 模板
    \item[C.] 指导 RNA
    \item[D.] 化学信号
  \end{itemize}
  \item 与 TALENs 相比，CRISPR 更快、更便宜、也更 \underline{\hspace{2.5cm}}。 \emph{(Correct: A; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] 易于编程
    \item[B.] 广泛采用
    \item[C.] 高度可变
    \item[D.] 技术先进
  \end{itemize}
  \item 当指导 RNA 与错误序列结合时，会发生 \underline{\hspace{2.5cm}}。 \emph{(Correct: A; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] 脱靶编辑
    \item[B.] 稍弱配对
    \item[C.] 相似碱基配对
    \item[D.] 偏移区域配对
  \end{itemize}
  \item 碱基编辑器可以不 \underline{\hspace{2.5cm}} 的情况下修改 DNA。 \emph{(Correct: B; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] 使用指导 RNA
    \item[B.] 切断两条 DNA 链
    \item[C.] 借助酶作用
    \item[D.] 激活细胞修复机制
  \end{itemize}
  \item AAV 载体效率高，但 \underline{\hspace{2.5cm}} 有限。 \emph{(Correct: B; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] 精准性
    \item[B.] 容量
    \item[C.] 持久性
    \item[D.] 灵活性
  \end{itemize}
  \item 脂质纳米颗粒倾向于集中在 \underline{\hspace{2.5cm}}。 \emph{(Correct: D; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] 肾脏
    \item[B.] 肺部
    \item[C.] 心脏
    \item[D.] 肝脏
  \end{itemize}
  \item "自限型"系统所用的组件会在短时间内 \underline{\hspace{2.5cm}}。 \emph{(Correct: A; Article-only item)}
  \begin{itemize}
    \item[A.] 快速降解
    \item[B.] 自然分解
    \item[C.] 逐渐失稳
    \item[D.] 缓慢累积
  \end{itemize}
  \item 体外（ex vivo）编辑让医生能在回输细胞前 \underline{\hspace{2.5cm}}。 \emph{(Correct: C; Article-only item)}
  \begin{itemize}
    \item[A.] 调整剂量
    \item[B.] 检验相容性
    \item[C.] 验证准确性
    \item[D.] 加标签处理
  \end{itemize}
  \item CRISPR 治疗的挑战之一是确保编辑组分发挥作用同时不引发 \underline{\hspace{2.5cm}}。 \emph{(Correct: B; Article-only item)}
  \begin{itemize}
    \item[A.] 过度的 DNA 复制
    \item[B.] 免疫排斥
    \item[C.] 代谢抑制
    \item[D.] 氧化应激
  \end{itemize}
  \item 可诱导型开关只有在接触特定 \underline{\hspace{2.5cm}} 时才激活 Cas 酶。 \emph{(Correct: B; Article-only item)}
  \begin{itemize}
    \item[A.] 分子信号
    \item[B.] 化学或热信号
    \item[C.] 膜受体
    \item[D.] 电信号
  \end{itemize}
  \item CRISPR 的过程包括：引导、切割，然后 \underline{\hspace{2.5cm}} \emph{(Correct: D; False-lure item; False-lure option: A (Restore))}
  \begin{itemize}
    \item[A.] 恢复
    \item[B.] 复制
    \item[C.] 修复
    \item[D.] 移除
  \end{itemize}
\end{enumerate}
\end{ChineseBlock}

\textbf{English translation (for reference only).}\par
\begin{enumerate}
  \item CRISPR began as \underline{\hspace{2.5cm}} that allows bacteria to capture pieces of viral DNA. \emph{(Correct: A; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] a bacterial immune system
    \item[B.] a viral defense mechanism
    \item[C.] a cellular repair system
    \item[D.] a genetic storage method
  \end{itemize}
  \item Early CRISPR crop research produced \underline{\hspace{2.5cm}} \emph{(Correct: D; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] early test versions
    \item[B.] lab-phase cultivation variants
    \item[C.] commercial products
    \item[D.] experimental prototypes
  \end{itemize}
  \item Early CRISPR diagnostic tools like SHERLOCK and DETECTR were initially designed to monitor \underline{\hspace{2.5cm}}. \emph{(Correct: A; False-lure item; False-lure option: B (DNA repair activity))}
  \begin{itemize}
    \item[A.] pathogen presence
    \item[B.] DNA repair activity
    \item[C.] genome-wide mutation patterns
    \item[D.] guide RNA efficiency
  \end{itemize}
  \item Scientists reprogrammed CRISPR using \underline{\hspace{2.5cm}} to direct Cas enzymes. \emph{(Correct: C; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] protein markers
    \item[B.] DNA templates
    \item[C.] guide RNA
    \item[D.] chemical signals
  \end{itemize}
  \item Compared to TALENs, CRISPR is faster, cheaper, and \underline{\hspace{2.5cm}} \emph{(Correct: A; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] easier to program
    \item[B.] widely adopted
    \item[C.] highly adaptable
    \item[D.] technically refined
  \end{itemize}
  \item Off-target edits occur when guide RNAs \underline{\hspace{2.5cm}} \emph{(Correct: A; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] bind mismatched sequences
    \item[B.] bind partially similar sites
    \item[C.] pair with near-matching bases
    \item[D.] drift to adjacent regions
  \end{itemize}
  \item Base editors modify DNA without \underline{\hspace{2.5cm}} \emph{(Correct: B; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] using guide RNA
    \item[B.] breaking both strands
    \item[C.] requiring enzymes
    \item[D.] cellular repair
  \end{itemize}
  \item AAV vectors are efficient but have limited \underline{\hspace{2.5cm}} \emph{(Correct: B; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] precision
    \item[B.] capacity
    \item[C.] persistence
    \item[D.] flexibility
  \end{itemize}
  \item Lipid nanoparticles concentrate in the \underline{\hspace{2.5cm}} \emph{(Correct: D; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] kidneys
    \item[B.] lungs
    \item[C.] heart
    \item[D.] liver
  \end{itemize}
  \item Self-limiting systems use components that \underline{\hspace{2.5cm}} \emph{(Correct: A; Article-only item)}
  \begin{itemize}
    \item[A.] degrade fast
    \item[B.] decay naturally
    \item[C.] become unstable
    \item[D.] accumulate slow
  \end{itemize}
  \item Ex vivo editing allows doctors to \underline{\hspace{2.5cm}} before reinfusion. \emph{(Correct: C; Article-only item)}
  \begin{itemize}
    \item[A.] modify doses
    \item[B.] test compatibility
    \item[C.] verify accuracy
    \item[D.] label samples
  \end{itemize}
  \item A major challenge in CRISPR therapy is ensuring that editing components reach the nucleus and act without triggering \underline{\hspace{2.5cm}}. \emph{(Correct: B; Article-only item)}
  \begin{itemize}
    \item[A.] excessive DNA replication
    \item[B.] immune rejection
    \item[C.] metabolic suppression
    \item[D.] oxidative stress
  \end{itemize}
  \item Inducible switches activate Cas enzymes only when exposed to specific \underline{\hspace{2.5cm}}. \emph{(Correct: B; Article-only item)}
  \begin{itemize}
    \item[A.] molecular signals
    \item[B.] chemical or thermal cues
    \item[C.] membrane receptors
    \item[D.] electrical pulses
  \end{itemize}
  \item The CRISPR process follows: guide, cut, and \underline{\hspace{2.5cm}} \emph{(Correct: C; False-lure item; False-lure option: A (Restore))}
  \begin{itemize}
    \item[A.] restore
    \item[B.] replicate
    \item[C.] repair
    \item[D.] remove
  \end{itemize}
\end{enumerate}

\subsection{Semiconductor Supply Chains: Why Shortages Happen and How to Build Resilience: MCQ items}

\textbf{Original (Chinese; administered in the experiment).}\par
\begin{ChineseBlock}
\begin{enumerate}
  \item 在西方国家，先进半导体制造的成本更高，主要是因为运营成本通常比东亚地区高出\underline{\hspace{2.5cm}}。 \emph{(Correct: D; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] 5–10\%
    \item[B.] 15–20\%
    \item[C.] 20–30\%
    \item[D.] 30–50\%
  \end{itemize}
  \item 主要芯片制造商的扩产计划之所以滞后，主要是由于\underline{\hspace{2.5cm}}。 \emph{(Correct: B; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] 原材料出口限制
    \item[B.] 设备采购瓶颈
    \item[C.] 突然的监管冻结
    \item[D.] 知识产权争议
  \end{itemize}
  \item 在短缺期间，分配不仅取决于合同，也同样取决于\underline{\hspace{2.5cm}}需求。 \emph{(Correct: B; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] 生产需求
    \item[B.] 技术需求
    \item[C.] 财务需求
    \item[D.] 物流需求
  \end{itemize}
  \item 东亚和荷兰的地理集中导致了\underline{\hspace{2.5cm}}的脆弱性。 \emph{(Correct: D; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] 产能驱动型
    \item[B.] 地区依赖型
    \item[C.] 供应相关型
    \item[D.] 单点脆弱性
  \end{itemize}
  \item “即时生产”（Just-in-time）模式失败，是因为半导体制造涉及\underline{\hspace{2.5cm}}。 \emph{(Correct: C; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] 多样化零件编号
    \item[B.] 芯片无法互换
    \item[C.] 长周期
    \item[D.] 需求波动剧烈
  \end{itemize}
  \item 2020年汽车需求崩溃时，晶圆厂将产能转向了\underline{\hspace{2.5cm}}。 \emph{(Correct: D; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] 移动处理器
    \item[B.] 计算系统
    \item[C.] 数字显示
    \item[D.] 消费电子
  \end{itemize}
  \item 如今，公司通过制定\underline{\hspace{2.5cm}}的供应合同来应对波动性。 \emph{(Correct: A; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] 自适应型
    \item[B.] 多年期合同
    \item[C.] 灵活型
    \item[D.] 多元化
  \end{itemize}
  \item 中期产能扩建通常需要\underline{\hspace{2.5cm}}才能完成。 \emph{(Correct: A; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] 一至两年
    \item[B.] 三至五个月
    \item[C.] 五至七年
    \item[D.] 十至十二个月
  \end{itemize}
  \item 先进半导体制造主要依赖\underline{\hspace{2.5cm}}光刻技术。 \emph{(Correct: B; False-lure item; False-lure option: A (Quantum processors))}
  \begin{itemize}
    \item[A.] 量子处理器
    \item[B.] 极紫外光刻
    \item[C.] 深紫外扫描仪
    \item[D.] 等离子体发射器
  \end{itemize}
  \item 光刻胶材料需要使用\underline{\hspace{2.5cm}}。 \emph{(Correct: D; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] 稀土掺杂剂
    \item[B.] 高纯气体
    \item[C.] 半导体级涂层
    \item[D.] 超纯树脂
  \end{itemize}
  \item 在3纳米制程节点，晶体管实际的栅极宽度大约相当于\underline{\hspace{2.5cm}}个原子。 \emph{(Correct: B; False-lure item; False-lure option: B (46 silicon atoms wide))}
  \begin{itemize}
    \item[A.] 44个碳原子
    \item[B.] 46个硅原子
    \item[C.] 48个硅原子
    \item[D.] 42个碳原子
  \end{itemize}
  \item 传统制程节点的产能扩建通常需要\underline{\hspace{2.5cm}}，且其利润率在15–25\%之间。 \emph{(Correct: C; Article-only item)}
  \begin{itemize}
    \item[A.] 6–9个月
    \item[B.] 12–15个月
    \item[C.] 18–24个月
    \item[D.] 30–36个月
  \end{itemize}
  \item 到2019年，一些制造商对某些零部件只保有\underline{\hspace{2.5cm}}的库存缓冲。 \emph{(Correct: A; Article-only item)}
  \begin{itemize}
    \item[A.] 一周
    \item[B.] 两个月
    \item[C.] 一季度
    \item[D.] 十天
  \end{itemize}
  \item 西方国家的先进制程生产需要提供相当于总成本\underline{\hspace{2.5cm}}的补贴。 \emph{(Correct: B; Article-only item)}
  \begin{itemize}
    \item[A.] 十分之一
    \item[B.] 四分之一至三分之一
    \item[C.] 接近一半
    \item[D.] 三分之二
  \end{itemize}
\end{enumerate}
\end{ChineseBlock}

\textbf{English translation (for reference only).}\par
\begin{enumerate}
  \item Advanced semiconductor fabrication in Western countries is more expensive mainly because operating costs are typically \underline{\hspace{2.5cm}} higher than in East Asia. \emph{(Correct: D; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] 5--10\%
    \item[B.] 15--20\%
    \item[C.] 20--30\%
    \item[D.] 30--50\%
  \end{itemize}
  \item Major chipmakers announced large investment plans through 2030, but actual expansion lagged mostly due to \underline{\hspace{2.5cm}}. \emph{(Correct: B; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] raw material export limits
    \item[B.] equipment procurement bottlenecks
    \item[C.] sudden regulatory freezes
    \item[D.] intellectual-property disputes
  \end{itemize}
  \item Automakers that pre-booked capacity were prioritized because allocation depended as much on contracts as on \underline{\hspace{2.5cm}} needs. \emph{(Correct: B; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] production
    \item[B.] technology
    \item[C.] financial
    \item[D.] logistics
  \end{itemize}
  \item Geographic concentration in East Asia and the Netherlands creates \underline{\hspace{2.5cm}} vulnerabilities. \emph{(Correct: D; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] capacity-driven
    \item[B.] region-dependent
    \item[C.] supply-linked
    \item[D.] single-point
  \end{itemize}
  \item Just-in-time failed because production involves \underline{\hspace{2.5cm}}. \emph{(Correct: C; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] diverse part numbers
    \item[B.] non-interchangeable chips
    \item[C.] long cycle times
    \item[D.] unstable demand shifts
  \end{itemize}
  \item When automotive demand collapsed in 2020, foundries shifted capacity toward \underline{\hspace{2.5cm}}. \emph{(Correct: D; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] mobile processors
    \item[B.] computing systems
    \item[C.] digital displays
    \item[D.] consumer electronics
  \end{itemize}
  \item Companies shifting to risk-adjusted strategies now maintain inventories and develop \underline{\hspace{2.5cm}} supply contracts. \emph{(Correct: A; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] adaptive
    \item[B.] multi-year
    \item[C.] flexible
    \item[D.] diversified
  \end{itemize}
  \item Intermediate capacity expansions typically require \underline{\hspace{2.5cm}} to complete. \emph{(Correct: A; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] one to two years
    \item[B.] three to five months
    \item[C.] five to seven years
    \item[D.] ten to twelve months
  \end{itemize}
  \item The technology used in advanced semiconductor manufacturing is \underline{\hspace{2.5cm}}. \emph{(Correct: B; False-lure item; False-lure option: A (Quantum processors))}
  \begin{itemize}
    \item[A.] quantum processors
    \item[B.] extreme ultraviolet
    \item[C.] deep-UV scanners
    \item[D.] plasma emitters
  \end{itemize}
  \item Photoresist materials require \underline{\hspace{2.5cm}} \emph{(Correct: D; AI-summary-covered item)}
  \begin{itemize}
    \item[A.] rare-earth dopants
    \item[B.] purified gases
    \item[C.] semiconductor-grade coatings
    \item[D.] ultra-pure resins
  \end{itemize}
  \item At the 3nm process node, transistor gate widths correspond to roughly \underline{\hspace{2.5cm}}. \emph{(Correct: C; False-lure item; False-lure option: B (46 silicon atoms wide))}
  \begin{itemize}
    \item[A.] 44 carbon atoms
    \item[B.] 46 silicon atoms
    \item[C.] 48 silicon atoms
    \item[D.] 42 carbon atoms
  \end{itemize}
  \item Legacy-node capacity additions require \underline{\hspace{2.5cm}} and produce margins of 15--25\%. \emph{(Correct: C; Article-only item)}
  \begin{itemize}
    \item[A.] 6--9 months
    \item[B.] 12--15 months
    \item[C.] 18--24 months
    \item[D.] 30--36 months
  \end{itemize}
  \item By 2019, some manufacturers operated on \underline{\hspace{2.5cm}} for certain components. \emph{(Correct: A; Article-only item)}
  \begin{itemize}
    \item[A.] single-week buffers
    \item[B.] two-month buffers
    \item[C.] quarterly buffers
    \item[D.] ten-day buffers
  \end{itemize}
  \item Western advanced-node production requires subsidies amounting to \underline{\hspace{2.5cm}} of total costs. \emph{(Correct: B; Article-only item)}
  \begin{itemize}
    \item[A.] around one-tenth
    \item[B.] roughly one-quarter to one-third
    \item[C.] nearly one-half
    \item[D.] two-thirds
  \end{itemize}
\end{enumerate}

\section{Study 1 Questionnaires and Rating Scales}
\label{app:instruments}

This appendix lists the questionnaires and rating scales administered during the session. Unless otherwise noted, Likert-type items used 7-point response scales.
\par\smallskip
\textbf{Language note.} All participant-facing instruments were administered in Simplified Chinese. For transparency, each item is first presented in its original Chinese wording (as used in the experiment), followed by an English translation (for reference only).

\subsection{Baseline (pre-task) measures}

\textbf{Prior-knowledge familiarity.}
\textit{Original (Chinese; administered in the experiment).}
\begin{ChineseBlock}
\noindent \textbf{先验知识熟悉度（术语熟悉度）。} 请评价你对每个术语的熟悉程度（4点量表：0 = 完全不熟悉，1 = 略微熟悉，2 = 比较熟悉，3 = 非常熟悉）。术语以英文专业词汇呈现：
\end{ChineseBlock}
\textit{English translation (for reference only).} Participants rated familiarity with each term on a 4-point scale (0 = Not at all familiar, 1 = Slightly familiar, 2 = Moderately familiar, 3 = Very familiar). Terms:

\begin{multicols}{2}
\begin{itemize}
  \item Heat flux
  \item Permeable pavement
  \item Reflective coating
  \item Cooling corridor
  \item Urban canyon
  \item Albedo
  \item Gene drive
  \item Base editing
  \item Prime editing
  \item Adeno-associated virus (AAV)
  \item Lipid nanoparticle
  \item Germ-line editing
  \item Wafer
  \item Lithography mask
  \item System-on-a-chip (SoC)
  \item Photolithography
  \item Legacy node
  \item Extreme ultraviolet lithography (EUV)
\end{itemize}
\end{multicols}

\textbf{Baseline trust in AI (3 items).}
\textit{Original (Chinese; administered in the experiment).}
\begin{ChineseBlock}
\noindent（1 = 强烈不同意，7 = 强烈同意）
\begin{itemize}
  \item 我通常信任人工智能工具生成的信息。
  \item 人工智能系统通常会提供准确且公平的结果。
  \item 我很乐意依靠人工智能来支持我的学习或工作任务。
\end{itemize}
\end{ChineseBlock}
\textit{English translation (for reference only).} (1 = Strongly disagree, 7 = Strongly agree)
\begin{itemize}
  \item I generally trust information generated by AI tools.
  \item AI systems usually provide accurate and fair results.
  \item I feel comfortable relying on AI to support my learning or work tasks.
\end{itemize}

\textbf{Baseline technology dependence (3 items).}
\textit{Original (Chinese; administered in the experiment).}
\begin{ChineseBlock}
\noindent（1 = 强烈不同意，7 = 强烈同意）
\begin{itemize}
  \item 我经常依赖数位工具来帮我记住或储存资讯。
  \item 当我不确定某事时，我的第一反应是询问AI工具或搜索引擎。
  \item 科技让我思考更有效率，相较于仅依靠记忆而言。
\end{itemize}
\end{ChineseBlock}
\textit{English translation (for reference only).} (1 = Strongly disagree, 7 = Strongly agree)
\begin{itemize}
  \item I often rely on digital tools to remember or store information for me.
  \item When I'm unsure about something, my first instinct is to ask an AI tool or search engine
  \item Technology helps me think more efficiently than relying only on my memory.
\end{itemize}

\textbf{Self-rated AI/digital skill (2 items).}
\textit{Original (Chinese; administered in the experiment).}
\begin{ChineseBlock}
\noindent（1 = 强烈不同意，7 = 强烈同意）
\begin{itemize}
  \item 我对使用人工智能驱动的应用程序或系统充满信心。
  \item 我通常学习如何快速使用新的数字工具。
\end{itemize}
\end{ChineseBlock}
\textit{English translation (for reference only).} (1 = Strongly disagree, 7 = Strongly agree)
\begin{itemize}
  \item I feel confident using AI-powered applications or systems.
  \item I usually learn how to use new digital tools quickly.
\end{itemize}

\subsection{Per-article (post-block) ratings}

After each article block, participants completed ratings to capture perceived load and experience. Cognitive-load items used explicit anchors; other items used 1--7 agreement scales unless noted.

\textbf{Cognitive load.}
\textit{Original (Chinese; administered in the experiment).}
\begin{ChineseBlock}
\begin{itemize}
  \item ``这项任务对脑力的要求有多高？''（1 = 一点要求都没有；7 = 要求极高）
  \item ``理解这篇文章的内容有多困难？''（1 = 很容易；7 = 非常困难）
\end{itemize}
\end{ChineseBlock}
\textit{English translation (for reference only).}
\begin{itemize}
  \item ``How mentally demanding was this task?'' (1 = Not at all demanding; 7 = Extremely demanding)
  \item ``How difficult was it to understand the content of this article?'' (1 = Very easy; 7 = Very difficult)
\end{itemize}

\textbf{AI experience (AI groups only).} (1 = Strongly disagree; 7 = Strongly agree)
\textit{Original (Chinese; administered in the experiment).}
\begin{ChineseBlock}
\noindent（1 = 强烈不同意；7 = 强烈同意）
\begin{itemize}
  \item ``AI 生成的摘要帮助我理解了这篇文章。''
  \item ``AI 生成的摘要帮助我记住了这篇文章。''
  \item ``人工智能的协助使任务变得更容易、更高效。''
  \item ``我对本文提供的人工智能帮助感到满意。''
  \item （可选）``我更喜欢在人工智能支持下完成此类任务，而不是没有人工智能支持。''
\end{itemize}
\end{ChineseBlock}
\textit{English translation (for reference only).} (1 = Strongly disagree; 7 = Strongly agree)
\begin{itemize}
  \item ``The AI-generated summary helped me understand the article.''
  \item ``The AI-generated summary helped me remember the article.''
  \item ``The AI assistance made the task easier and more efficient.''
  \item ``I am satisfied with the AI assistance provided for this article.''
  \item (Optional) ``I prefer completing this kind of task with AI support rather than without it.''
\end{itemize}

\textbf{Overall MCQ confidence.}
\textit{Original (Chinese; administered in the experiment).}
\begin{ChineseBlock}
\noindent ``总体而言，您对本文多项选择题的回答有多大信心？''（1 = 一点也不自信；7 = 无比自信）
\end{ChineseBlock}
\textit{English translation (for reference only).} ``Overall, how confident are you in your answers to the multiple-choice questions for this article?'' (1 = Not confident at all; 7 = Extremely confident)

\textbf{Post-block state trust and dependence (AI groups only).} These items were asked after the recall and MCQ tasks for each article, referring to the summary just used in that block (1 = Strongly disagree; 7 = Strongly agree).
\textit{Original (Chinese; administered in the experiment).}
\begin{ChineseBlock}
\noindent（1 = 强烈不同意；7 = 强烈同意）
\begin{itemize}
  \item 信任（明确的可信度表述）：``你认为 AI 生成的摘要在帮助理解这篇文章方面有多可靠？''（\texttt{trust\_new}）
  \item 依赖（明确的卸载表述）：``在完成本文章块的记忆任务时，你在多大程度上依赖 AI 生成的摘要，而不是依赖你自己对文章的记忆？''（\texttt{dependence\_new}）
\end{itemize}
\end{ChineseBlock}
\textit{English translation (for reference only).}
\begin{itemize}
  \item Trust (explicit credibility framing): ``How reliable did you consider the AI-generated summary to be for understanding the article?'' (\texttt{trust\_new})
  \item Dependence (explicit offloading framing): ``To what extent did you rely on the AI-generated summary rather than your own memory of the article?'' (\texttt{dependence\_new})
\end{itemize}

\subsection{End-of-session manipulation checks}

\textit{Original (Chinese; administered in the experiment).}
\begin{ChineseBlock}
\begin{itemize}
  \item ``你觉得摘要和文章整体上有多连贯和相互关联？''（1 = 非常碎片化；7 = 高度连贯）
  \item ``摘要在多大程度上帮助你看到文章中观点之间的关系？''（1 = 完全没有；7 = 非常多）
  \item 策略选择：``在阅读和回答问题时，我主要……''（将观点联系起来形成整体理解 vs.\ 记忆零散事实与细节）
\end{itemize}
\end{ChineseBlock}
\textit{English translation (for reference only).}
\begin{itemize}
  \item ``How coherent and interconnected did the summary and article feel overall?'' (1 = Very fragmented; 7 = Highly connected)
  \item ``To what extent did the summary help you see relationships between ideas in the article?'' (1 = Not at all; 7 = Very much)
  \item Strategy choice: ``When reading and answering questions, I mainly...'' (Connected ideas to form an overall understanding vs. Memorized separate facts and details)
\end{itemize}

\subsection{Session instructions (verbatim)}
\textit{Original (Chinese; administered in the experiment).}
\begin{ChineseBlock}
\begin{itemize}
  \item 您将阅读 3 篇不同的文章，每篇文章都有自己的记忆和理解测试。
  \item 尽力记住每篇文章中尽可能多的信息。
  \item 您的奖金奖励与您的测试准确性成正比——您提供的答案越正确，您的奖金就越高。
  \item 这项任务需要全神贯注；请避免分心、切换标签或匆忙。
\end{itemize}
\end{ChineseBlock}
\textit{English translation (for reference only).}
\begin{itemize}
  \item You will read 3 different articles, each followed by its own memory and comprehension test.
  \item Try your best to remember as much information as possible from each article.
  \item Your bonus reward is proportional to your test accuracy --- the more correct answers you provide, the higher your bonus.
  \item The task requires full attention; please avoid distractions, switching tabs, or rushing.
\end{itemize}

\textbf{Important information about the AI summaries (verbatim).}
\textit{Original (Chinese; administered in the experiment).}
\begin{ChineseBlock}
\begin{itemize}
  \item 根据您的情况，您可能会看到最多三个AI生成的摘要：一个在文章之前，一个在阅读过程中（您可以随时打开和关闭），以及一个在文章之后。
  \item AI 通常很有帮助，摘要大多数时候是准确的。
  \item 然而，AI 仍可能包含小错误或遗漏。
  \item 摘要旨在帮助您——而不是替代仔细阅读完整文章。
  \item 有些测试问题只能使用完整文章中的细节来回答。
  \item 请仔细阅读所有内容以最大化您的表现。
\end{itemize}
\end{ChineseBlock}
\textit{English translation (for reference only).}
\begin{itemize}
  \item Depending on your condition, you may see up to three AI-generated summaries: one before the article, one during the reading (which you can open and close at any time), and one after the article.
  \item AI is generally helpful, and summaries are accurate most of the time.
  \item However, AI can still contain minor mistakes or omissions.
  \item The summaries are meant to assist you --- not replace careful reading of the full article.
  \item Some test questions can only be answered using details from the full articles.
  \item Please read everything carefully to maximize your performance.
\end{itemize}

\subsection{Free recall task instructions (verbatim)}
\textbf{Free Recall (5 minutes).} Write everything you remember from the article in short, idea-based sentences.
\par\smallskip
\textit{Original (Chinese; administered in the experiment).}
\begin{ChineseBlock}
\noindent \textbf{自由回忆（5分钟）。} 请用简短、以观点为单位的句子写下你从文章中记住的所有内容。
\begin{itemize}
  \item 使用模式：``A $\rightarrow$ B 因为 C''（原因 $\rightarrow$ 结果 $\rightarrow$ 理由）或 ``X 导致 Y，因为 Z''。
  \item 目标写 8--12 句（用于评分的最大句数 = 10）。
  \item 关注具体观点与它们之间的联系，而不是泛泛的总体印象。
  \item 不要复制/粘贴；请用自己的话表达。
  \item 你有 5:00 分钟。计时器会显示剩余时间。
  \item 为确保你阅读了说明，开始按钮将在 30 秒后解锁。（实验中另设置了继续按钮的最短锁定时间。）
\end{itemize}
\end{ChineseBlock}
\textit{English translation (for reference only).}
\begin{itemize}
  \item Use the pattern: ``A $\rightarrow$ B because C'' (cause $\rightarrow$ effect $\rightarrow$ reason) or ``X leads to Y due to Z''.
  \item Aim for 8--12 sentences (max accepted for scoring = 10).
  \item Focus on specific ideas and links, not general impressions.
  \item No copy/paste; write in your own words.
  \item You have 5:00 minutes. A timer shows remaining time.
  \item The start button is unlocked after 30 seconds to ensure instructions are read.
\end{itemize}

\subsection{Consent text (verbatim)}
\textit{Original (Chinese; administered in the experiment).}
\begin{ChineseBlock}
您被邀请参加一项研究，探索不同的文本格式如何影响记忆和理解。今天的实验大约需要 90 分钟。您将阅读简短的科学文章、回答问题并评估您的经验。
\begin{itemize}
  \item 您的参与是自愿的。
  \item 您可以随时退出而不会受到处罚。
  \item 不会收集任何个人识别信息；数据将被匿名存储。
  \item 您必须年满 18 岁并使用笔记本电脑或台式电脑（不得使用手机）。
\end{itemize}
单击 ``我同意并继续''，即表示您确认已阅读此信息并同意参与。
\end{ChineseBlock}
\textit{English translation (for reference only).} You are invited to participate in a study exploring how different text formats influence memory and comprehension. The experiment takes approximately 90 minutes. You read short scientific articles, answer questions, and evaluate your experience.
\begin{itemize}
  \item Your participation is voluntary.
  \item You may withdraw at any time without penalty.
  \item No personal identifying information will be collected; data will be stored anonymously.
  \item You must be 18 years or older and use a laptop or desktop computer (no phones).
\end{itemize}
By clicking ``I Agree and Continue'', you confirm that you have read this information and consent to participate.

\subsection{Debrief screen (verbatim)}
The debrief screen thanked participants, displayed their participant ID, stated that responses help understand how AI influences learning/recall/comprehension, and instructed participants that they could close the window.
\par\smallskip
\textit{Chinese translation (for reference).}
\begin{ChineseBlock}
致谢页面感谢参与者，显示其参与者ID，并说明这些回答将用于理解 AI 如何影响学习/回忆/理解，同时提示参与者可以关闭窗口。
\end{ChineseBlock}

\chapter{Study 2 Survey Instrument (Product Managers)}
\label{app:study2-instrument}

This appendix documents the structured survey instrument used in Study~2 to measure AI adoption and its perceived
impacts in product management. The survey was administered in English and Chinese. Unless otherwise noted, response
options are listed exactly as in the questionnaire.

\section{Section A: AI tool usage and adoption context}
\begin{enumerate}
  \item \textbf{A1.} Indicate your frequency of usage for each type of AI application in your daily product-management tasks
  (Never, Rarely, Occasionally, Often, Always):
  \begin{itemize}
    \item Robotic Process Automation (RPA) for repetitive tasks
    \item Workflow automation tools
    \item Predictive analytics tools for feature prioritization
    \item Natural Language Processing (NLP) tools for analyzing customer feedback/sentiment
    \item AI-driven QA \& product testing tools
    \item Generative AI tools (e.g., ChatGPT, Claude) for drafting documentation/initial content
    \item AI chatbots or virtual assistants for customer interactions/support
  \end{itemize}

  \item \textbf{A2.} For how long have you actively used AI applications in your product-management role?
  \begin{itemize}
    \item Less than 6 months
    \item 6 months -- 1 year
    \item 1--2 years
    \item 2--3 years
    \item More than 3 years
  \end{itemize}

  \item \textbf{A3.} Which group typically drives the decision to implement AI tools in your PM activities?
  \begin{itemize}
    \item Product Management Team
    \item Engineering/Tech Team
    \item Executive/Leadership Team
    \item External Consultants/Vendors
    \item Cross-functional collaborative effort
  \end{itemize}

  \item \textbf{A4.} What were your organization’s primary goals when integrating AI into your product-management activities?
  (Select up to two)
  \begin{itemize}
    \item Reducing operational costs
    \item Increasing PM productivity and efficiency
    \item Improving customer/user experience
    \item Enhancing product quality and reliability
    \item Innovating product development processes
    \item Gaining competitive advantage in the market
  \end{itemize}

  \item \textbf{A5.} To what extent have the AI tools you adopted met your initial expectations?
  \begin{itemize}
    \item Did not meet expectations at all
    \item Slightly met expectations
    \item Moderately met expectations
    \item Largely met expectations
    \item Fully exceeded expectations
  \end{itemize}

  \item \textbf{A6.} Specify any AI tools/platforms you regularly use (open-ended).

  \item \textbf{A7.} What challenges have you encountered when adopting AI in your product-management tasks? (Select all applicable)
  \begin{itemize}
    \item Technical difficulties or integration complexity
    \item Initial learning curve and team resistance
    \item Data availability or quality issues
    \item Insufficient training or support
    \item Budget constraints and cost management
    \item AI tool reliability or accuracy issues
    \item Ethical or regulatory concerns
  \end{itemize}

  \item \textbf{A8.} Which factors are most influential when selecting new AI tools for product-management tasks? (Select top two)
  \begin{itemize}
    \item Ease of integration with existing systems
    \item User-friendly interface and ease of use
    \item Vendor reputation and support
    \item Cost-effectiveness
    \item Recommendations from peers or industry reviews
    \item Flexibility and scalability of the AI solution
  \end{itemize}

  \item \textbf{A9.} How do you expect your team’s AI usage in product-management tasks to change in the next 12 months?
  \begin{itemize}
    \item Significantly increase
    \item Slightly increase
    \item Stay about the same
    \item Slightly decrease
    \item Significantly decrease
  \end{itemize}
\end{enumerate}

\section{Section B: workload shifts and perceived decision support}
\begin{enumerate}
  \item \textbf{B1.} Since integrating AI tools, rate how significantly your workload has decreased for each task below
  (No reduction, Slight reduction, Moderate reduction, Significant reduction, Very significant reduction):
  \begin{itemize}
    \item Routine administrative \& reporting tasks
    \item Manual customer/user feedback collection \& analysis
    \item Market \& competitor research (data gathering, synthesis)
    \item QA, product testing, bug tracking management
    \item Routine communications (emails, updates)
    \item Documentation drafting \& content creation
    \item Task management and backlog updates
  \end{itemize}

  \item \textbf{B2.} Which tasks have notably increased or become more complex due to AI integration? (Select all applicable)
  \begin{itemize}
    \item Strategic planning and roadmapping
    \item Stakeholder engagement \& cross-functional coordination
    \item Innovation \& creative product ideation
    \item AI tool management (oversight, monitoring performance, model tuning)
    \item Ethical considerations \& governance related to AI use
    \item Skill development, training, and professional learning on AI tools
    \item Communication \& presentation of AI-driven insights to stakeholders
  \end{itemize}

  \item \textbf{B3.} Which tasks do you prioritize most frequently with time saved due to AI-driven efficiencies? (Rank top 3; 1 = highest priority)
  \begin{itemize}
    \item Strategic product planning \& vision-setting
    \item Deep customer/user interaction \& user research
    \item Stakeholder \& cross-team management
    \item Innovative experimentation \& idea generation
    \item AI oversight, governance \& ethical management
    \item Professional growth \& learning new AI-related skills
  \end{itemize}

  \item \textbf{B4.} Indicate the overall productivity impact you experienced due to AI integration.
  \begin{itemize}
    \item Productivity significantly decreased
    \item Productivity slightly decreased
    \item No notable productivity change
    \item Productivity slightly increased
    \item Productivity significantly increased
  \end{itemize}

  \item \textbf{B5.} How many additional weekly hours can you now allocate specifically toward strategic or high-value tasks due to AI automation?
  \begin{itemize}
    \item None
    \item 1--2 hours
    \item 3--5 hours
    \item 6--10 hours
    \item More than 10 hours
  \end{itemize}

  \item \textbf{B6.} Has the use of AI tools improved your decision-making capabilities as a product manager?
  \begin{itemize}
    \item Significantly improved
    \item Moderately improved
    \item Slightly improved
    \item No change
    \item Decision-making has worsened
  \end{itemize}

  \item \textbf{B7.} Looking forward, how do you expect AI integration to influence your task responsibilities as a product manager in the next 1--2 years?
  \begin{itemize}
    \item Significantly more focus on strategic tasks (planning, vision, stakeholder management)
    \item Moderate shift towards strategic and innovation tasks
    \item Slight or minimal change in responsibilities
    \item Increasing complexity and oversight of AI tool management
    \item Greater focus on ethical and governance responsibilities related to AI use
  \end{itemize}
\end{enumerate}

\section{Section C: skills, training, and capability development}
\begin{enumerate}
  \item \textbf{C1.} Estimate the average hours you save weekly due to AI automation of your product-management tasks.
  \begin{itemize}
    \item 0 hours
    \item 1--3 hours
    \item 4--7 hours
    \item 8--12 hours
    \item 13+ hours
  \end{itemize}

  \item \textbf{C2.} Rate how much your ability to focus on strategic aspects of your role has improved due to AI.
  \begin{itemize}
    \item Not improved at all
    \item Slightly improved
    \item Moderately improved
    \item Significantly improved
    \item Very significantly improved
  \end{itemize}

  \item \textbf{C3.} Select the top 3 skills you have developed or improved due to using AI in your role.
  \begin{itemize}
    \item AI literacy and understanding of AI concepts
    \item Prompt engineering (effective prompting of generative AI tools)
    \item Enhanced data analytics \& data-driven decision-making
    \item Ethical oversight \& governance of AI tools
    \item Improved strategic decision-making skills
    \item Stronger communication and data storytelling capabilities
    \item Technical proficiency in managing AI tools and platforms
  \end{itemize}

  \item \textbf{C4.} How confident are you in your current ability to effectively leverage AI tools in your daily PM tasks?
  \begin{itemize}
    \item Not confident at all
    \item Slightly confident
    \item Moderately confident
    \item Very confident
    \item Extremely confident
  \end{itemize}

  \item \textbf{C5.} Has your organization provided structured training focused explicitly on the skills necessary for effective AI integration?
  \begin{itemize}
    \item No structured training provided
    \item Informal or self-taught learning only
    \item Formal internal training programs provided
    \item External training/certifications offered by the organization
  \end{itemize}

  \item \textbf{C6.} Which training resources would most effectively support your skill development to handle AI integration? (Select top two)
  \begin{itemize}
    \item Hands-on workshops \& practical AI-tool training
    \item Formal AI-related certifications or courses
    \item Internal best practices \& case-study sharing
    \item Regular access to external AI experts or mentors
    \item Ongoing updates on AI developments and trends relevant to PMs
  \end{itemize}

  \item \textbf{C7.} How has AI integration influenced your perception of skills considered most valuable for a product manager today?
  \begin{itemize}
    \item No change in perception
    \item Slightly shifted toward technical AI-related skills
    \item Significantly shifted toward technical AI-related skills
    \item Slightly shifted toward soft skills (ethics, communication, strategy)
    \item Significantly shifted toward soft skills (ethics, communication, strategy)
  \end{itemize}
\end{enumerate}

\section{Section D: governance, ethics, and responsibility}
\begin{enumerate}
  \item \textbf{D1.} How frequently do ethical considerations related to AI integration arise in your daily tasks as a product manager?
  \begin{itemize}
    \item Never
    \item Rarely
    \item Occasionally
    \item Often
    \item Very Often
  \end{itemize}

  \item \textbf{D2.} Which ethical challenges have you directly encountered due to the use of AI tools in your product-management role?
  (Select all applicable)
  \begin{itemize}
    \item Privacy and protection of customer data
    \item Potential bias in AI-driven recommendations/decisions
    \item Transparency and explainability of AI decisions
    \item Accountability for errors or decisions made by AI
    \item Compliance with regulatory standards (e.g., GDPR, EU AI Act)
    \item No ethical challenges encountered
  \end{itemize}

  \item \textbf{D3.} Who primarily holds responsibility for managing ethical AI-related decisions in your organization?
  \begin{itemize}
    \item Product managers individually
    \item Dedicated AI/Ethics committee
    \item Cross-functional team or collaboration
    \item Formal organizational guidelines/policies
    \item No clear ownership/responsibility defined
  \end{itemize}

  \item \textbf{D4.} How confident do you feel addressing ethical issues arising from AI usage in your product-management role?
  \begin{itemize}
    \item Not confident at all
    \item Slightly confident
    \item Moderately confident
    \item Very confident
    \item Extremely confident
  \end{itemize}

  \item \textbf{D5.} Briefly describe a recent ethical decision-making scenario related to AI usage that you faced, and how you addressed it (open-ended).

  \item \textbf{D6.} Does your organization provide clear guidelines or policies for ethical AI usage in product management?
  \begin{itemize}
    \item No clear guidelines exist
    \item Informal ethical guidance only
    \item Formal internal AI ethics guidelines exist
    \item Adhere to external standards (e.g., IEEE, EU AI Act, GDPR)
  \end{itemize}

  \item \textbf{D7.} Have you received structured training specifically addressing ethical considerations related to AI use in your role?
  \begin{itemize}
    \item No structured ethical training provided
    \item Informal/self-directed learning only
    \item Formal internal ethical AI training provided
    \item External courses/certifications provided by the organization
  \end{itemize}

  \item \textbf{D8.} Which resources would best support you in managing ethical AI-related challenges? (Select up to two)
  \begin{itemize}
    \item Dedicated ethical AI training workshops
    \item Clearer organizational ethical guidelines
    \item Regular audits or assessments of AI systems
    \item Real-world ethical AI case studies/examples
    \item Regular access to ethical AI advisors or experts
  \end{itemize}

  \item \textbf{D9.} What is your greatest ethical concern regarding the increased use of AI in your product-management role in the future? (open-ended)
\end{enumerate}

\chapter{Supplementary Tables and Statistical Outputs}
\label{app:stats}

This appendix reports supporting model outputs for the primary analyses. All raw CSV outputs are available in \texttt{final\_analysis/long\_format\_outputs/tables/}.

\section{Primary condition effects (A1)}

\begin{table}[H]
\centering
\caption{Mixed-effects ANOVA (Structure $\times$ Timing) for Overall MCQ accuracy.}
\label{tab:A1_anova_mcq_accuracy}
\begin{tabular}{lrrrrr}
\toprule
Effect & $df_1$ & $df_2$ & $F$ & $p$ & $\eta_p^2$ \\
\midrule
Structure & 1 & 22.00 & 4.69 & = 0.042 & 0.176 \\
Timing & 2 & 42.18 & 17.61 & < .001 & 0.455 \\
Structure $\times$ Timing & 2 & 42.12 & 0.40 & = 0.670 & 0.019 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Mixed-effects ANOVA (Structure $\times$ Timing) for Article-only MCQ accuracy.}
\label{tab:A1_anova_article_accuracy}
\begin{tabular}{lrrrrr}
\toprule
Effect & $df_1$ & $df_2$ & $F$ & $p$ & $\eta_p^2$ \\
\midrule
Structure & 1 & 22.00 & 1.89 & = 0.183 & 0.079 \\
Timing & 2 & 43.25 & 0.35 & = 0.706 & 0.016 \\
Structure $\times$ Timing & 2 & 43.00 & 0.50 & = 0.608 & 0.023 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Mixed-effects ANOVA (Structure $\times$ Timing) for False-lure accuracy.}
\label{tab:A1_anova_false_lure_accuracy}
\begin{tabular}{lrrrrr}
\toprule
Effect & $df_1$ & $df_2$ & $F$ & $p$ & $\eta_p^2$ \\
\midrule
Structure & 1 & 22.00 & 4.20 & = 0.053 & 0.160 \\
Timing & 2 & 43.01 & 0.99 & = 0.380 & 0.044 \\
Structure $\times$ Timing & 2 & 42.75 & 0.47 & = 0.630 & 0.021 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Mixed-effects ANOVA (Structure $\times$ Timing) for Free-recall total score.}
\label{tab:A1_anova_recall_total_score}
\begin{tabular}{lrrrrr}
\toprule
Effect & $df_1$ & $df_2$ & $F$ & $p$ & $\eta_p^2$ \\
\midrule
Structure & 1 & 22.00 & 0.40 & = 0.536 & 0.018 \\
Timing & 2 & 42.87 & 0.02 & = 0.983 & 0.001 \\
Structure $\times$ Timing & 2 & 42.63 & 0.70 & = 0.503 & 0.032 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Mixed-effects ANOVA (Structure $\times$ Timing) for Summary viewing time (seconds).}
\label{tab:A1_anova_summary_time_sec}
\begin{tabular}{lrrrrr}
\toprule
Effect & $df_1$ & $df_2$ & $F$ & $p$ & $\eta_p^2$ \\
\midrule
Structure & 1 & 22.00 & 0.50 & = 0.486 & 0.022 \\
Timing & 2 & 43.25 & 13.32 & < .001 & 0.381 \\
Structure $\times$ Timing & 2 & 43.00 & 0.34 & = 0.717 & 0.015 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Mixed-effects ANOVA (Structure $\times$ Timing) for Summary time proportion.}
\label{tab:A1_anova_summary_prop}
\begin{tabular}{lrrrrr}
\toprule
Effect & $df_1$ & $df_2$ & $F$ & $p$ & $\eta_p^2$ \\
\midrule
Structure & 1 & 22.00 & 0.01 & = 0.933 & 0.000 \\
Timing & 2 & 43.25 & 16.20 & < .001 & 0.428 \\
Structure $\times$ Timing & 2 & 43.00 & 0.13 & = 0.879 & 0.006 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Mixed-effects ANOVA (Structure $\times$ Timing) for Reading time (minutes).}
\label{tab:A1_anova_reading_time_min}
\begin{tabular}{lrrrrr}
\toprule
Effect & $df_1$ & $df_2$ & $F$ & $p$ & $\eta_p^2$ \\
\midrule
Structure & 1 & 22.00 & 1.26 & = 0.275 & 0.054 \\
Timing & 2 & 43.25 & 1.16 & = 0.324 & 0.051 \\
Structure $\times$ Timing & 2 & 43.00 & 0.19 & = 0.826 & 0.009 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Mixed-effects ANOVA (Structure $\times$ Timing) for Total block time (seconds).}
\label{tab:A1_anova_total_time_sec}
\begin{tabular}{lrrrrr}
\toprule
Effect & $df_1$ & $df_2$ & $F$ & $p$ & $\eta_p^2$ \\
\midrule
Structure & 1 & 22.00 & 1.67 & = 0.210 & 0.070 \\
Timing & 2 & 43.25 & 0.01 & = 0.992 & 0.000 \\
Structure $\times$ Timing & 2 & 43.00 & 0.30 & = 0.745 & 0.014 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Mixed-effects ANOVA (Structure $\times$ Timing) for Mental effort rating.}
\label{tab:A1_anova_mental_effort}
\begin{tabular}{lrrrrr}
\toprule
Effect & $df_1$ & $df_2$ & $F$ & $p$ & $\eta_p^2$ \\
\midrule
Structure & 1 & 22.00 & 2.43 & = 0.133 & 0.099 \\
Timing & 2 & 42.24 & 1.50 & = 0.236 & 0.066 \\
Structure $\times$ Timing & 2 & 42.16 & 2.32 & = 0.111 & 0.099 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Mixed-effects ANOVA (Structure $\times$ Timing) for Post-block AI trust (trust\_new).}
\label{tab:A1_anova_ai_trust}
\begin{tabular}{lrrrrr}
\toprule
Effect & $df_1$ & $df_2$ & $F$ & $p$ & $\eta_p^2$ \\
\midrule
Structure & 1 & 22.00 & 1.58 & = 0.222 & 0.067 \\
Timing & 2 & 43.25 & 7.90 & = 0.001 & 0.268 \\
Structure $\times$ Timing & 2 & 43.00 & 1.70 & = 0.194 & 0.073 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Mixed-effects ANOVA (Structure $\times$ Timing) for Post-block AI dependence (dependence\_new).}
\label{tab:A1_anova_ai_dependence}
\begin{tabular}{lrrrrr}
\toprule
Effect & $df_1$ & $df_2$ & $F$ & $p$ & $\eta_p^2$ \\
\midrule
Structure & 1 & 22.00 & 6.21 & = 0.021 & 0.220 \\
Timing & 2 & 43.25 & 7.74 & = 0.001 & 0.264 \\
Structure $\times$ Timing & 2 & 43.00 & 0.59 & = 0.559 & 0.027 \\
\bottomrule
\end{tabular}
\end{table}

\section{Post-hoc timing contrasts (A1)}

\begin{table}[H]
\centering
\caption{Post-hoc timing contrasts for Overall MCQ accuracy (Holm-corrected).}
\label{tab:A1_posthoc_timing_mcq_accuracy}
\begin{tabular}{lrrrrr}
\toprule
Contrast & Estimate & SE & $df$ & $t$ & $p$ \\
\midrule
pre\_reading - synchronous & 0.172 & 0.031 & 42.05 & 5.54 & < .001 \\
pre\_reading - post\_reading & 0.143 & 0.031 & 42.18 & 4.56 & < .001 \\
synchronous - post\_reading & -0.029 & 0.032 & 42.32 & -0.92 & = 0.361 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Post-hoc timing contrasts for Summary viewing time (seconds) (Holm-corrected).}
\label{tab:A1_posthoc_timing_summary_time_sec}
\begin{tabular}{lrrrrr}
\toprule
Contrast & Estimate & SE & $df$ & $t$ & $p$ \\
\midrule
pre\_reading - synchronous & 32.178 & 12.018 & 42.43 & 2.68 & = 0.021 \\
pre\_reading - post\_reading & 62.987 & 12.216 & 43.43 & 5.16 & < .001 \\
synchronous - post\_reading & 30.809 & 12.408 & 43.94 & 2.48 & = 0.021 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Post-hoc timing contrasts for Summary time proportion (Holm-corrected).}
\label{tab:A1_posthoc_timing_summary_prop}
\begin{tabular}{lrrrrr}
\toprule
Contrast & Estimate & SE & $df$ & $t$ & $p$ \\
\midrule
pre\_reading - synchronous & 0.054 & 0.019 & 42.43 & 2.79 & = 0.012 \\
pre\_reading - post\_reading & 0.112 & 0.020 & 43.43 & 5.69 & < .001 \\
synchronous - post\_reading & 0.058 & 0.020 & 43.94 & 2.90 & = 0.012 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Post-hoc timing contrasts for Reading time (minutes) (Holm-corrected).}
\label{tab:A1_posthoc_timing_reading_time_min}
\begin{tabular}{lrrrrr}
\toprule
Contrast & Estimate & SE & $df$ & $t$ & $p$ \\
\midrule
pre\_reading - synchronous & -1.974 & 0.705 & 42.43 & -2.80 & = 0.023 \\
pre\_reading - post\_reading & -0.967 & 0.717 & 43.43 & -1.35 & = 0.347 \\
synchronous - post\_reading & 1.007 & 0.728 & 43.94 & 1.38 & = 0.347 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Post-hoc timing contrasts for Total block time (seconds) (Holm-corrected).}
\label{tab:A1_posthoc_timing_total_time_sec}
\begin{tabular}{lrrrrr}
\toprule
Contrast & Estimate & SE & $df$ & $t$ & $p$ \\
\midrule
pre\_reading - synchronous & -113.589 & 53.500 & 42.43 & -2.12 & = 0.079 \\
pre\_reading - post\_reading & 15.762 & 54.383 & 43.43 & 0.29 & = 0.773 \\
synchronous - post\_reading & 129.350 & 55.240 & 43.94 & 2.34 & = 0.071 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Post-hoc timing contrasts for Post-block AI trust (trust\_new) (Holm-corrected).}
\label{tab:A1_posthoc_timing_ai_trust}
\begin{tabular}{lrrrrr}
\toprule
Contrast & Estimate & SE & $df$ & $t$ & $p$ \\
\midrule
pre\_reading - synchronous & 0.625 & 0.171 & 42.43 & 3.65 & = 0.002 \\
pre\_reading - post\_reading & 0.542 & 0.174 & 43.43 & 3.12 & = 0.007 \\
synchronous - post\_reading & -0.083 & 0.177 & 43.94 & -0.47 & = 0.639 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Post-hoc timing contrasts for Post-block AI dependence (dependence\_new) (Holm-corrected).}
\label{tab:A1_posthoc_timing_ai_dependence}
\begin{tabular}{lrrrrr}
\toprule
Contrast & Estimate & SE & $df$ & $t$ & $p$ \\
\midrule
pre\_reading - synchronous & 0.625 & 0.190 & 42.43 & 3.30 & = 0.004 \\
pre\_reading - post\_reading & 0.667 & 0.193 & 43.43 & 3.46 & = 0.004 \\
synchronous - post\_reading & 0.042 & 0.196 & 43.94 & 0.21 & = 0.832 \\
\bottomrule
\end{tabular}
\end{table}

\section{Condition descriptives (A1)}

\begin{table}[H]
\centering
\caption{Descriptive statistics for Overall MCQ accuracy by condition.}
\label{tab:A1_descriptives_mcq_accuracy}
\begin{tabular}{llrrrr}
\toprule
Structure & Timing & $n$ & Mean & SD & SE \\
\midrule
Integrated & Pre Reading & 12 & 0.750 & 0.124 & 0.036 \\
Integrated & Synchronous & 12 & 0.542 & 0.124 & 0.036 \\
Integrated & Post Reading & 12 & 0.607 & 0.094 & 0.027 \\
Segmented & Pre Reading & 12 & 0.649 & 0.108 & 0.031 \\
Segmented & Synchronous & 12 & 0.524 & 0.173 & 0.050 \\
Segmented & Post Reading & 12 & 0.518 & 0.143 & 0.041 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Descriptive statistics for Article-only MCQ accuracy by condition.}
\label{tab:A1_descriptives_article_accuracy}
\begin{tabular}{llrrrr}
\toprule
Structure & Timing & $n$ & Mean & SD & SE \\
\midrule
Integrated & Pre Reading & 12 & 0.542 & 0.234 & 0.068 \\
Integrated & Synchronous & 12 & 0.458 & 0.317 & 0.091 \\
Integrated & Post Reading & 12 & 0.562 & 0.264 & 0.076 \\
Segmented & Pre Reading & 12 & 0.479 & 0.249 & 0.072 \\
Segmented & Synchronous & 12 & 0.438 & 0.264 & 0.076 \\
Segmented & Post Reading & 12 & 0.396 & 0.198 & 0.057 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Descriptive statistics for False-lure accuracy by condition.}
\label{tab:A1_descriptives_false_lure_accuracy}
\begin{tabular}{llrrrr}
\toprule
Structure & Timing & $n$ & Mean & SD & SE \\
\midrule
Integrated & Pre Reading & 12 & 0.542 & 0.396 & 0.114 \\
Integrated & Synchronous & 12 & 0.667 & 0.326 & 0.094 \\
Integrated & Post Reading & 12 & 0.458 & 0.334 & 0.096 \\
Segmented & Pre Reading & 12 & 0.333 & 0.326 & 0.094 \\
Segmented & Synchronous & 12 & 0.417 & 0.289 & 0.083 \\
Segmented & Post Reading & 12 & 0.375 & 0.311 & 0.090 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Descriptive statistics for Free-recall total score by condition.}
\label{tab:A1_descriptives_recall_total_score}
\begin{tabular}{llrrrr}
\toprule
Structure & Timing & $n$ & Mean & SD & SE \\
\midrule
Integrated & Pre Reading & 12 & 5.417 & 2.193 & 0.633 \\
Integrated & Synchronous & 12 & 5.292 & 1.912 & 0.552 \\
Integrated & Post Reading & 12 & 5.167 & 2.462 & 0.711 \\
Segmented & Pre Reading & 12 & 5.583 & 1.690 & 0.488 \\
Segmented & Synchronous & 12 & 5.792 & 2.116 & 0.611 \\
Segmented & Post Reading & 12 & 5.958 & 1.852 & 0.535 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Descriptive statistics for Summary viewing time (seconds) by condition.}
\label{tab:A1_descriptives_summary_time_sec}
\begin{tabular}{llrrrr}
\toprule
Structure & Timing & $n$ & Mean & SD & SE \\
\midrule
Integrated & Pre Reading & 12 & 141.873 & 35.915 & 10.368 \\
Integrated & Synchronous & 12 & 99.680 & 50.440 & 14.561 \\
Integrated & Post Reading & 12 & 74.427 & 43.444 & 12.541 \\
Segmented & Pre Reading & 12 & 123.106 & 55.986 & 16.162 \\
Segmented & Synchronous & 12 & 100.943 & 53.654 & 15.489 \\
Segmented & Post Reading & 12 & 64.577 & 33.072 & 9.547 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Descriptive statistics for Summary time proportion by condition.}
\label{tab:A1_descriptives_summary_prop}
\begin{tabular}{llrrrr}
\toprule
Structure & Timing & $n$ & Mean & SD & SE \\
\midrule
Integrated & Pre Reading & 12 & 0.246 & 0.056 & 0.016 \\
Integrated & Synchronous & 12 & 0.190 & 0.110 & 0.032 \\
Integrated & Post Reading & 12 & 0.141 & 0.091 & 0.026 \\
Segmented & Pre Reading & 12 & 0.251 & 0.100 & 0.029 \\
Segmented & Synchronous & 12 & 0.200 & 0.087 & 0.025 \\
Segmented & Post Reading & 12 & 0.132 & 0.059 & 0.017 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Descriptive statistics for Reading time (minutes) by condition.}
\label{tab:A1_descriptives_reading_time_min}
\begin{tabular}{llrrrr}
\toprule
Structure & Timing & $n$ & Mean & SD & SE \\
\midrule
Integrated & Pre Reading & 12 & 7.337 & 1.611 & 0.465 \\
Integrated & Synchronous & 12 & 7.447 & 2.232 & 0.644 \\
Integrated & Post Reading & 12 & 7.982 & 2.394 & 0.691 \\
Segmented & Pre Reading & 12 & 6.107 & 2.164 & 0.625 \\
Segmented & Synchronous & 12 & 6.928 & 3.301 & 0.953 \\
Segmented & Post Reading & 12 & 7.396 & 2.654 & 0.766 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Descriptive statistics for Total block time (seconds) by condition.}
\label{tab:A1_descriptives_total_time_sec}
\begin{tabular}{llrrrr}
\toprule
Structure & Timing & $n$ & Mean & SD & SE \\
\midrule
Integrated & Pre Reading & 12 & 582.073 & 114.386 & 33.020 \\
Integrated & Synchronous & 12 & 546.530 & 128.953 & 37.226 \\
Integrated & Post Reading & 12 & 553.327 & 145.558 & 42.019 \\
Segmented & Pre Reading & 12 & 489.556 & 142.748 & 41.208 \\
Segmented & Synchronous & 12 & 516.593 & 223.455 & 64.506 \\
Segmented & Post Reading & 12 & 508.327 & 170.303 & 49.162 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Descriptive statistics for Mental effort rating by condition.}
\label{tab:A1_descriptives_mental_effort}
\begin{tabular}{llrrrr}
\toprule
Structure & Timing & $n$ & Mean & SD & SE \\
\midrule
Integrated & Pre Reading & 12 & 5.917 & 0.669 & 0.193 \\
Integrated & Synchronous & 12 & 6.000 & 0.739 & 0.213 \\
Integrated & Post Reading & 12 & 6.083 & 0.793 & 0.229 \\
Segmented & Pre Reading & 12 & 5.917 & 0.793 & 0.229 \\
Segmented & Synchronous & 12 & 5.250 & 1.288 & 0.372 \\
Segmented & Post Reading & 12 & 5.500 & 1.243 & 0.359 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Descriptive statistics for Post-block AI trust (trust\_new) by condition.}
\label{tab:A1_descriptives_ai_trust}
\begin{tabular}{llrrrr}
\toprule
Structure & Timing & $n$ & Mean & SD & SE \\
\midrule
Integrated & Pre Reading & 12 & 4.167 & 0.577 & 0.167 \\
Integrated & Synchronous & 12 & 3.583 & 0.669 & 0.193 \\
Integrated & Post Reading & 12 & 3.917 & 1.084 & 0.313 \\
Segmented & Pre Reading & 12 & 4.833 & 0.835 & 0.241 \\
Segmented & Synchronous & 12 & 4.167 & 1.193 & 0.345 \\
Segmented & Post Reading & 12 & 4.000 & 1.348 & 0.389 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Descriptive statistics for Post-block AI dependence (dependence\_new) by condition.}
\label{tab:A1_descriptives_ai_dependence}
\begin{tabular}{llrrrr}
\toprule
Structure & Timing & $n$ & Mean & SD & SE \\
\midrule
Integrated & Pre Reading & 12 & 4.250 & 0.965 & 0.279 \\
Integrated & Synchronous & 12 & 3.833 & 0.577 & 0.167 \\
Integrated & Post Reading & 12 & 3.667 & 0.985 & 0.284 \\
Segmented & Pre Reading & 12 & 5.333 & 1.303 & 0.376 \\
Segmented & Synchronous & 12 & 4.500 & 1.243 & 0.359 \\
Segmented & Post Reading & 12 & 4.583 & 0.900 & 0.260 \\
\bottomrule
\end{tabular}
\end{table}

\chapter{Ethics / Consent Documents (Summary)}
\label{app:ethics}

This appendix summarizes the ethics-related materials and participant protections implemented across Study~1 and
Study~2.

\section{Study 1 (controlled experiment)}
Participants reviewed an informed-consent screen describing the purpose of the study (AI-assisted reading and
memory), the approximate duration (about 90 minutes), the main tasks (reading short scientific articles, answering
questions, and providing ratings), and key protections: voluntary participation, the right to withdraw at any time
without penalty, and anonymous data handling. The consent screen also specified basic eligibility (18+ and laptop/
desktop participation).

Because Study~1 includes AI-generated summaries and measures epistemic risk via plausible but incorrect details,
the protocol includes debriefing guidance intended to prevent lasting misconceptions (i.e., participants should not
treat AI summaries as ground truth).

\section{Study 2 (field survey)}
Survey participation was voluntary. The questionnaire was designed to avoid collection of proprietary company
information and was analyzed in aggregate. Open-ended responses were treated as confidential and reported only in
anonymized form.

\section{Data management}
Across both studies, datasets were stored in access-controlled locations and used only for research purposes.
Reporting focuses on patterns and mechanisms rather than identifying individuals or organizations.

\end{document}
