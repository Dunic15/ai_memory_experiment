% A LaTeX template for MSc Thesis submissions to 
% Politecnico di Milano (PoliMi) - School of Industrial and Information Engineering
%
% S. Bonetti, A. Gruttadauria, G. Mescolini, A. Zingaro
% e-mail: template-tesi-ingind@polimi.it
%
% Last Revision: October 2021
%
% Copyright 2021 Politecnico di Milano, Italy. NC-BY

\documentclass{Configuration_Files/PoliMi3i_thesis}

%------------------------------------------------------------------------------
%	REQUIRED PACKAGES AND  CONFIGURATIONS
%------------------------------------------------------------------------------

% CONFIGURATIONS
\usepackage{parskip} % For paragraph layout
\usepackage{setspace} % For using single or double spacing
\usepackage{emptypage} % To insert empty pages
\usepackage{multicol} % To write in multiple columns (executive summary)
\setlength\columnsep{15pt} % Column separation in executive summary
\setlength\parindent{0pt} % Indentation



% PACKAGES FOR TITLES
\usepackage{titlesec}
% \titlespacing{\section}{left spacing}{before spacing}{after spacing}
\titlespacing{\section}{0pt}{3.3ex}{2ex}
\titlespacing{\subsection}{0pt}{3.3ex}{1.65ex}
\titlespacing{\subsubsection}{0pt}{3.3ex}{1ex}
\usepackage{color}

% PACKAGES FOR LANGUAGE AND FONT
\usepackage[english]{babel} % The document is in English  
\usepackage[utf8]{inputenc} % UTF8 encoding
\usepackage[T1]{fontenc} % Font encoding
\usepackage[11pt]{moresize} % Big fonts
\usepackage{CJKutf8} % Enable Chinese text blocks under pdfLaTeX

% PACKAGES FOR IMAGES
\usepackage{graphicx}
\usepackage{transparent} % Enables transparent images
\usepackage{eso-pic} % For the background picture on the title page
\usepackage{subfig} % Numbered and caption subfigures using \subfloat.
\usepackage{tikz} % A package for high-quality hand-made figures.
\usetikzlibrary{}
\graphicspath{{./Images/}} % Directory of the images
\usepackage{caption} % Coloured captions
\usepackage{xcolor} % Coloured captions
\usepackage{amsthm,thmtools,xcolor} % Coloured "Theorem"
\usepackage{float}

% STANDARD MATH PACKAGES
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage[overload]{empheq} % For braced-style systems of equations.
\usepackage{fix-cm} % To override original LaTeX restrictions on sizes

% PACKAGES FOR TABLES
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{longtable} % Tables that can span several pages
\usepackage{colortbl}

% PACKAGES FOR ALGORITHMS (PSEUDO-CODE)
\usepackage{algorithm}
\usepackage{algorithmic}

% PACKAGES FOR REFERENCES & BIBLIOGRAPHY
\usepackage[colorlinks=true,linkcolor=black,anchorcolor=black,citecolor=black,filecolor=black,menucolor=black,runcolor=black,urlcolor=black]{hyperref} % Adds clickable links at references
\usepackage{cleveref}
\usepackage[square, numbers, sort&compress]{natbib} % Square brackets, citing references with numbers, citations sorted by appearance in the text and compressed
\bibliographystyle{abbrvnat} % You may use a different style adapted to your field

% OTHER PACKAGES
\usepackage{pdfpages} % To include a pdf file
\usepackage{afterpage}
\usepackage{lipsum} % DUMMY PACKAGE
\usepackage{fancyhdr} % For the headers
\fancyhf{}

% Input of configuration file. Do not change config.tex file unless you really know what you are doing. 
\input{Configuration_Files/config}

%----------------------------------------------------------------------------
%	NEW COMMANDS DEFINED
%----------------------------------------------------------------------------

% EXAMPLES OF NEW COMMANDS
\newcommand{\bea}{\begin{eqnarray}} % Shortcut for equation arrays
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\e}[1]{\times 10^{#1}}  % Powers of 10 notation
\newenvironment{ChineseBlock}{\begin{CJK*}{UTF8}{gbsn}}{\end{CJK*}}

%----------------------------------------------------------------------------
%	ADD YOUR PACKAGES (be careful of package interaction)
%----------------------------------------------------------------------------

%----------------------------------------------------------------------------
%	ADD YOUR DEFINITIONS AND COMMANDS (be careful of existing commands)
%----------------------------------------------------------------------------

%----------------------------------------------------------------------------
%	BEGIN OF YOUR DOCUMENT
%----------------------------------------------------------------------------

\begin{document}

\fancypagestyle{plain}{%
  \fancyhf{}% clear all header and footer fields
  \fancyfoot[C]{\thepage}% page number in the centre of the footer
  \renewcommand{\headrulewidth}{0pt}% no header line
  \renewcommand{\footrulewidth}{0pt}% no footer line
}

%----------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------

\pagestyle{empty} % No page numbers
\frontmatter % Use roman page numbering style (i, ii, iii, iv...) for the preamble pages

\puttitle{
    title = {Title of the Thesis}, % Insert the final thesis title
    name = {Duccio Profeti \quad\quad Edoardo Pinzauti}, % Authors on one line
    course = {  Management Engineering}, % Programme in English
    ID = {XXXXX \quad\quad YYYYY}, % Student IDs on one line
    advisor = { Prof. Sergio Terzi}, % Advisor in English
    coadvisor = {}, % Leave empty if none
    academicyear = {Academic Year: 2026--27}, % Academic year in English
}
%----------------------------------------------------------------------------
%	PREAMBLE PAGES: ABSTRACT (inglese e italiano), EXECUTIVE SUMMARY
%----------------------------------------------------------------------------
\startpreamble
\setcounter{page}{1} % Set page counter to 1

% ABSTRACT IN ENGLISH
\chapter*{Abstract} 
Here goes the Abstract in English of your thesis followed by a list of keywords.
The Abstract is a concise summary of the content of the thesis (single page of text)
and a guide to the most important contributions included in your thesis.
The Abstract is the very last thing you write.
It should be a self-contained text and should be clear to someone who hasn't (yet) read the whole manuscript.
The Abstract should contain the answers to the main scientific questions that have been addressed in your thesis.
It needs to summarize the adopted motivations and the adopted methodological approach as well as the findings of your work and their relevance and impact.
The Abstract is the part appearing in the record of your thesis inside POLITesi,
the Digital Archive of PhD and Master Theses (Laurea Magistrale) of Politecnico di Milano.
The Abstract will be followed by a list of four to six keywords.
Keywords are a tool to help indexers and search engines to find relevant documents.
To be relevant and effective, keywords must be chosen carefully.
They should represent the content of your work and be specific to your field or sub-field.
Keywords may be a single word or two to four words. 
\\
\\
\textbf{Keywords:} here, the keywords, of your thesis % Keywords

% ABSTRACT IN ITALIAN
\chapter*{Abstract in lingua italiana}
Qui va l'Abstract in lingua italiana della tesi seguito dalla lista di parole chiave.
\\
\\
\textbf{Parole chiave:} qui, vanno, le parole chiave, della tesi % Keywords (italian)

%----------------------------------------------------------------------------
%	LIST OF CONTENTS/FIGURES/TABLES/SYMBOLS
%----------------------------------------------------------------------------

% TABLE OF CONTENTS
\thispagestyle{empty}
\tableofcontents % Table of contents 
\thispagestyle{empty}
\cleardoublepage

%-------------------------------------------------------------------------
%	THESIS MAIN TEXT
%-------------------------------------------------------------------------
% In the main text of your thesis you can write the chapters in two different ways:
%
%(1) As presented in this template you can write:
%    \chapter{Title of the chapter}
%    *body of the chapter*
%
%(2) You can write your chapter in a separated .tex file and then include it in the main file with the following command:
%    \chapter{Title of the chapter}
%    \input{chapter_file.tex}
%
% Especially for long thesis, we recommend you the second option.

\addtocontents{toc}{\vspace{2em}} % Add a gap in the Contents, for aesthetics
\mainmatter % Begin numeric (1,2,3...) page numbering

% Use only footer page numbers, no header
\pagestyle{fancy}
\fancyhf{}                % clear header & footer
\fancyfoot[C]{\thepage}   % page number centered in footer
\renewcommand{\headrulewidth}{0pt} % remove header line
\renewcommand{\footrulewidth}{0pt} % optional: no footer line

% --------------------------------------------------------------------------
% NUMBERED CHAPTERS % Regular chapters following
% -------------------------------------------------------------------------

% --------------------------------------------------------------------------
% NUMBERED CHAPTERS % Regular chapters following
% -------------------------------------------------------------------------


\chapter{Introduction}
\label{ch:introduction}
Generative AI systems are increasingly embedded in everyday information work. Two use cases are especially salient
for learning and decision-intensive knowledge work: (i) AI-generated summaries and explanations that accompany
reading and study, and (ii) AI-generated analyses and recommendations that accompany professional judgment. In both
cases, the AI produces \emph{external representations}---persistent, content-bearing artifacts that can guide
attention, provide cues, and reshape how information is encoded, retrieved, and acted upon.

These developments sit on top of a longer trajectory of AI research and deployment. Historical and bibliometric
accounts describe multiple waves of AI evolution---from early symbolic systems to contemporary deep-learning
approaches and the emergence of new ``generations'' of AI capabilities---that have expanded the feasibility of
natural-language interfaces and large-scale decision support \citep{SchmidhuberAli2022AnnotatedHistoryModernAI,ShaoRen2022EvolutionAIBibliometric,Zhang2020ThirdGenerationAI,DwivediDutot2023EvolutionAIResearchTFSC}.

From a cognitive perspective, external representations can support performance by reducing extraneous processing
costs, but they can also change encoding strategies via cognitive offloading
\citep{Risko2016CognitiveOffloading,Sparrow2011GoogleEffect}. From a decision-making perspective, the same
representations can shape reliance: users may defer to AI outputs without fully scrutinizing their provenance or
quality, creating epistemic and governance risks \citep{Pasquale2015BlackBoxSociety,Zhang2024KnowledgeImbalanceAI}.
These tensions are particularly relevant for roles such as product management, where decisions integrate
heterogeneous evidence (user research, analytics, technical constraints, and stakeholder input) and where
generative AI is increasingly used to summarize, prioritize, and communicate \citep{Pinzauti2025ReinventingPM,GnanasambandamHall2024GenAIAccelerateTimeToMarket}.

Across sectors, AI is framed as both an operational technology (automation of routine processes) and a strategic
capability (augmenting scenario exploration, planning, and coordination). Industry and practitioner sources
document diffusion of AI applications across corporate functions---from customer service and operations management
to finance and human resources---and emphasize that value capture requires changes in workflows, roles, and
governance rather than model deployment alone \citep{GoodwinQuinteroValencia2024AIBusinessIBM,RashidAhmed2024AIRevolutionizingIndustries,Luu2024AIApplicationsBestarion,EdlichIqbal2018BotsAlgorithmsCorporateSupport,McKinsey2025StateOfAIRewiring,McGrath2024AITransformingOperationsManagementIBM,NawazAhmad2024AIAdoptionHRMPractices,Bailey2025AITransformsScenarioAnalysisCFI}.

This thesis addresses these issues through a \emph{multi-level perspective} that links cognitive mechanisms to
organizational practice. At the cognitive level, Study~1 uses a controlled reading-and-memory experiment in which
AI assistance is operationalized as pre-generated summaries that vary in \emph{structure} (integrated vs.\ segmented)
and \emph{timing} (pre-reading vs.\ synchronous vs.\ post-reading). At the organizational level, Study~2 examines
how product managers integrate generative AI tools into decision-intensive workflows and how they perceive impacts
on efficiency, judgment, and ethical responsibility \citep{Pinzauti2025ReinventingPM}. The thesis proposes the
\emph{AI Buffer} as a unifying concept: AI-generated external representations can function as persistent buffers
that support cue-based cognition while also introducing source-monitoring and reliance vulnerabilities.

\section{Context and Motivation}
\label{sec:motivation}
Learning from text depends on how readers allocate attention during encoding, how they manage limited working-memory
resources, and which cues are available at retrieval
\citep{Baddeley2012WorkingMemory,Craik1972LevelsOfProcessing,Tulving1973EncodingSpecificity}. In real-world settings,
learners frequently rely on external representations---notes, outlines, highlighted passages, and search
engines---to support these processes. Generative AI introduces a new, scalable form of external representation:
systems can produce structured summaries instantly and present them alongside source material.

At the same time, external representations can shift effort away from internal encoding. The ``Google effect''
shows that when information is easy to re-access, people are more likely to remember where it is than what it is
\citep{Sparrow2011GoogleEffect,Risko2016CognitiveOffloading,Gong2024GoogleEffectsMetaAnalysis}. In generative-AI
settings, this creates a core tension: summaries may support understanding and recognition by providing cues, but
they may also encourage shallow processing, reduce detail-rich encoding, and increase vulnerability to source
confusion when AI content blends with the original material
\citep{Johnson1993SourceMonitoring,Chan2024ConversationalAIFalseMemories}.

Beyond individual learning, the same dynamics play out in professional contexts where decisions depend on accurate,
contextual recall. Knowledge-management research suggests that AI can help organizations retrieve prior decisions
and maintain continuity across projects \citep{Pai2022AIKnowledgeManagement}. However, AI-advised decision-making
also raises risks of epistemic dependence and opaque reasoning
\citep{Pasquale2015BlackBoxSociety,Zhang2024KnowledgeImbalanceAI}. Product management provides a concrete setting in
which these issues are amplified: product managers are expected to synthesize noisy information into decisions and
to communicate rationale clearly across teams, making AI-provided summaries and recommendations both useful and
potentially hazardous \citep{Pinzauti2025ReinventingPM}.

\section{Research Problem and Gap}
\label{sec:research_problem}
Despite rapid adoption, there remains limited integrated evidence on \emph{how} AI-generated external
representations shape cognition and decision-making.

At the cognitive level, evidence on generative AI is still emerging and often treats ``AI assistance'' as a
unitary intervention. Yet design choices matter: \emph{when} a summary is available and \emph{how} it is structured
can change attention allocation, encoding depth, and the balance between internal reconstruction and reliance on
external cues. Controlled studies suggest that conversational AI can amplify false memories under certain
conditions \citep{Chan2024ConversationalAIFalseMemories}, and reviews warn about over-reliance and reduced cognitive
engagement \citep{Zhai2024OverRelianceAIDialogue}, but there is still a need for causal evidence that isolates
interface-relevant dimensions (timing, format) and measures both performance and epistemic risk.

At the organizational level, many accounts focus on productivity or adoption narratives, while fewer connect
observed reliance patterns to cognitive mechanisms such as offloading and source monitoring. For decision-intensive
roles such as product management, the open question is not only whether AI improves efficiency, but whether it
changes how decisions are formed, justified, and governed---including how trust, transparency, and accountability
are maintained when AI outputs shape reasoning \citep{Pasquale2015BlackBoxSociety,Pinzauti2025ReinventingPM}.

The central gap motivating this thesis is therefore the lack of a \emph{multi-level framework} that links (i)
mechanisms of AI-assisted encoding and retrieval to (ii) patterns of reliance and judgment in professional
decision-making contexts.

\section{Research Objectives and Multi-Level Perspective}
\label{sec:objectives}
Following common master-thesis conventions, the work is organized around concrete objectives that span the two
levels of analysis and their integration.
\begin{itemize}
  \item \textbf{O1 --- Cognitive baseline:} quantify memory differences between AI-assisted reading and unaided
  reading under matched constraints (Study~1).
  \item \textbf{O2 --- Cognitive design dimensions:} test how summary \emph{timing} and \emph{structure} affect
  recall quality, recognition accuracy, and behavioral indicators of reliance (Study~1).
  \item \textbf{O3 --- Epistemic risk:} measure susceptibility to false-lure endorsement and source-monitoring
  errors when AI-generated content is present (Study~1).
  \item \textbf{O4 --- Organizational practice:} characterize how product managers deploy generative AI tools in
  decision-intensive workflows, including perceived benefits, risks, and ethical responsibilities (Study~2)
  \citep{Pinzauti2025ReinventingPM}.
  \item \textbf{O5 --- Cross-level integration:} develop an integrated account linking AI-generated external
  representations to both memory mechanisms and professional decision processes, and derive propositions that can
  guide safer tool design and governance.
\end{itemize}

\section{Research Questions}
\label{sec:research_questions}
Building on the research gap identified in the previous section, this thesis investigates how artificial
intelligence reshapes knowledge-intensive decision-making by simultaneously affecting individual cognitive
processes and organizational practices. To address this objective, the study adopts a multi-level perspective and
is guided by the following research questions:

\begin{itemize}
  \item \textbf{RQ1 -- Cognitive Level (Study 1).} \textbf{RQ1:} How do AI-generated external representations
  influence core human memory processes during knowledge-intensive tasks?
  \begin{itemize}
    \item \textit{Memory encoding} (attention allocation, depth of processing).
    \item \textit{Recall and recognition} (quantity and quality of retained information).
    \item \textit{Source monitoring} (discriminating article content from AI-generated content).
    \item \textit{Confidence calibration} (alignment between confidence and accuracy).
    \item \textit{Cognitive load} (perceived effort and split-attention costs).
  \end{itemize}
  Conceptually, RQ1 corresponds to the AI Buffer Model formulated as a research question: it asks how persistent
  AI-generated representations shape encoding, retrieval, and metacognitive monitoring.

  \item \textbf{RQ2 -- Organizational Level (Study 2).} \textbf{RQ2:} How does the adoption of AI tools shape
  decision-making practices, role configuration, and perceived responsibility in product management?
  \begin{itemize}
    \item Role transformation and reallocation of tasks toward more strategic work.
    \item AI-supported decision-making (sensemaking, prioritization, and communication).
    \item Governance and ethics (accountability, transparency, oversight practices).
    \item Cross-context comparison (e.g., differences across organizational and cultural settings such as USA--China).
  \end{itemize}
  Importantly, RQ2 is not treated as a mere ``application'' of Study~1, but as an autonomous analytical level that
  characterizes AI adoption in a decision-intensive managerial domain.

  \item \textbf{RQ3 -- Integrative Level (Cross-study).} \textbf{RQ3:} How can changes in organizational
  decision-making practices enabled by AI be explained through underlying cognitive mechanisms of AI-assisted
  memory?
\end{itemize}

RQ3 is the key integrating question of the thesis: rather than reporting two disconnected sets of findings, the
thesis aims to explain observed organizational patterns (Study~2) using a coherent account of memory mechanisms
identified under controlled conditions (Study~1). This integrative aim motivates the conceptual framework in
Chapter~3 and the dedicated cross-study integration logic developed later in the thesis.

\section{Contribution of the Thesis}
\label{sec:contribution}
The thesis contributes at three levels: cognitive theory, organizational understanding, and integrative framing.

\textbf{Cognitive contribution.} Study~1 provides design-sensitive evidence on AI-assisted reading by measuring both
performance (recall and recognition) and epistemic risk (false-lure endorsement), explicitly manipulating timing
and structure of AI-generated summaries.

\textbf{Organizational contribution.} Study~2 synthesizes patterns of AI use in product management and clarifies how
AI tools are positioned as automation and augmentation within decision-intensive workflows, including perceived
shifts in responsibility and ethical vigilance \citep{Pinzauti2025ReinventingPM}.

\textbf{Integrative contribution.} The thesis advances the \emph{AI Buffer} as a compact bridge concept:
AI-generated representations can function as persistent external buffers that support cue-based cognition while
also altering source monitoring and reliance conditions. This framing supports actionable implications for AI
interface design, training, and governance in knowledge work.

\section{Scope, Definitions, and Assumptions}
\label{sec:scope}
This thesis focuses on AI-generated external representations and their consequences under two complementary
methodological lenses.

\textbf{Key definitions.} \emph{Generative AI} refers to systems that produce novel text based on learned patterns
from data. \emph{External representations} refer to persistent artifacts (summaries, notes, recommendations) that
users can consult during a task. \emph{Cognitive offloading} refers to shifting cognitive work from internal memory
to external aids \citep{Risko2016CognitiveOffloading}. \emph{Source monitoring} refers to processes by which people
attribute information to its origin \citep{Johnson1993SourceMonitoring}. In this thesis, the \emph{AI Buffer} is
defined as an AI-generated representation that persists during a task and can shape encoding, retrieval, and
metacognitive monitoring.

\textbf{Study boundaries.} Study~1 is intentionally scoped to controlled laboratory conditions, pre-generated
summaries (rather than open-ended dialogue), and within-session memory measures. Study~2 is scoped to product
management as a decision-intensive domain and emphasizes professional practice and governance rather than direct
cognitive performance measurement.

\textbf{Assumptions.} The thesis treats AI summaries as fallible representations rather than authoritative ground
truth and assumes that interface properties (timing, structure, provenance cues) can change cognitive and
organizational outcomes by shaping attention, cue availability, and reliance norms.

\section{Structure of the Thesis}
\label{sec:thesis_structure}
The remainder of the thesis follows the structure below:
\begin{itemize}
  \item \textbf{Chapter~2 (Literature Review)} synthesizes human-memory foundations and research on cognitive
  offloading, external representations, and human--AI reliance in knowledge work and managerial contexts.
  \item \textbf{Chapter~3 (Conceptual Framework and Hypotheses)} introduces the AI Buffer model and derives
  cross-level propositions linking cognitive mechanisms to decision processes.
  \item \textbf{Chapter~4 (Methodology)} details Study~1 (controlled experiment) and Study~2 (field study on product
  managers), including instruments and ethical considerations.
  \item \textbf{Chapters~5--7 (Analysis, Results, Integration)} report analytical strategy and findings for each
  study and then integrate evidence across levels.
  \item \textbf{Chapters~8--9 (Discussion and Conclusion)} interpret implications, limitations, and future research
  directions, and summarize the thesis contributions.
  \item \textbf{Appendices} provide study materials, instruments, and supplementary outputs.
\end{itemize}


\chapter{Literature Review}
\label{ch:literature-review}
This chapter reviews prior work that motivates the thesis structure introduced in \Cref{sec:thesis_structure}. It
first establishes foundations of human memory and learning from text, then synthesizes research on cognitive
offloading and external representations. Building on this, it reviews how generative AI functions as a new kind of
external representation in knowledge work, with particular attention to confidence, source monitoring, cognitive
load, and reliance in decision-making. The chapter closes by synthesizing these strands into a multi-level research
gap that motivates the conceptual framework developed in Chapter~3.

\section{Foundations of Human Memory}

\subsection{Theoretical Foundations of Human Memory}
The study of human memory has evolved through several major theoretical frameworks that describe how information is
encoded, stored, and retrieved. The earliest comprehensive account is the \emph{modal model} proposed by Atkinson
and Shiffrin \citep{Atkinson1968HumanMemory}, which conceptualizes memory as a system composed of three structural
stores: sensory memory, short-term memory (STM), and long-term memory (LTM). According to this framework,
information enters through high-capacity but rapidly decaying sensory registers, is filtered into STM via
attentional control, and may be consolidated into LTM through rehearsal and elaborative processing. This model
introduced the influential distinction between temporary and durable memory systems and highlighted the central
role of control processes---particularly attention and rehearsal---in regulating information flow.

A major theoretical shift occurred with Baddeley’s reconceptualization of STM as \emph{working memory}
\citep{Baddeley2012WorkingMemory}. Rather than a passive buffer, working memory is understood as an active
cognitive workspace composed of multiple subsystems (the phonological loop, visuospatial sketchpad, episodic
buffer, and a central executive) that enable the integration of multimodal information. This framework supports
reasoning, decision-making, and sustained mental effort, and it aligns with extensive neuropsychological evidence
for domain-specific maintenance mechanisms and executive-control functions distributed across the cortex.

Building on these models, Tulving introduced the distinction between \emph{episodic} and \emph{semantic} memory
\citep{Tulving1972EpisodicSemantic}. Episodic memory refers to the recollection of personally experienced,
context-rich events, whereas semantic memory stores generalized knowledge about the world. Tulving further proposed
the \emph{encoding specificity} principle \citep{Tulving1973EncodingSpecificity}, which posits that retrieval is
most successful when the cues available at encoding are reinstated at recall. This principle is central for
understanding how external aids (including AI-generated summaries) may facilitate or distort later recall by
changing which cues are available and salient.

Craik and Lockhart’s \emph{Levels of Processing} framework \citep{Craik1972LevelsOfProcessing} emphasized that the
durability of a memory trace depends not on which store it resides in, but on the \emph{depth} and \emph{semantic
elaboration} of processing applied to the information. Deep, meaning-oriented encoding produces more persistent
memory traces than shallow, perceptual processing---a finding repeatedly confirmed by subsequent behavioral and
neuroscientific studies.

Neurophysiological research has extended classical models by illuminating the biological basis of encoding and
retrieval. Single-neuron recording studies show that hippocampal and medial-temporal lobe neurons respond
selectively to abstract concepts, forming building blocks of episodic and semantic representations
\citep{Rutishauser2021ArchitectureHumanMemory}. Electrophysiological evidence further demonstrates that stronger
neural activation during encoding predicts greater likelihood of later retrieval success---the subsequent-memory
effect \citep{Caplan2009EEGAssociativeOrder}.

Memory research has also embraced ecological and contextual perspectives. Studies using immersive virtual
environments indicate that spatially rich, multisensory contexts enhance episodic encoding
\citep{Plancher2018VREpisodicMemory}, while ``real-life'' neuroscience approaches argue that memory should be
studied in dynamic, socially embedded settings rather than in isolation \citep{ShamayTsoory2019RealLifeNeuroscience}.
At a systems level, contemporary accounts emphasize that memory emerges from distributed interactions across
cortical--hippocampal networks \citep{Moscovitch2006CognitiveNeuroscienceRemoteMemory}, and neuroimaging evidence
suggests that retrieving a memory can reactivate cortical patterns present during encoding
\citep{Liu2021TransformativeNeuralRepresentations}. Taken together, these perspectives motivate two principles used
throughout this thesis: (i) encoding depth and attentional control strongly shape memory outcomes, and (ii) memory
is cue-dependent, reconstructive, and sensitive to external context.

A complementary tradition in cognitive psychology emphasizes that remembering is not a verbatim replay but a
\emph{constructive} process shaped by schemas and expectations. In his seminal work, Bartlett argued that recall
reflects reconstruction guided by prior knowledge, which can yield distortions that are coherent but not faithful to
the original experience \citep{Bartlett1932Remembering}. This reconstructive property is central for AI-assisted
cognition: when users encounter fluent AI-generated representations, those representations can become integrated into
the schema that guides later reconstruction, increasing the risk that plausible but incorrect details are remembered
as ``having been in the source''.

From a systems perspective, episodic remembering is distinguished from semantic knowledge because it carries
contextual tags (where, when, and how information was acquired) that enable source attribution and confidence
judgments. Reviews of episodic-memory mechanisms highlight that contextual reinstatement and source cues are key
determinants of retrieval fidelity \citep{Papagno2015EpisodicMemory}. In this thesis, this motivates treating source
monitoring and misinformation susceptibility as first-class outcomes rather than as peripheral errors.

Finally, because the thesis links cognitive constructs (encoding, retrieval, source monitoring) to organizational
practices (decision workflows, governance), it requires a coherent mapping between levels and paradigms. Work on
cognitive ontologies argues that many ``constructs'' are families of measures embedded in specific experimental
paradigms \citep{Turner2012CognitiveParadigmOntology}. This motivates the thesis’s multi-method strategy: rather than
assuming one-to-one equivalence between laboratory tasks and professional practice, the thesis uses Study~1 as
mechanistic evidence and Study~2 as contextual evidence and integrates them via the AI Buffer Model.

\subsection{How Readers Learn from Expository Texts}
Humans do not memorize information in a vacuum; when reading expository texts (such as scientific articles or
technical reports), they actively construct a mental representation of the content that integrates new information
with prior knowledge. According to discourse comprehension theory, readers form a hierarchical \emph{situation
model} of a text, which entails establishing local coherence between consecutive ideas and global coherence across
the entire discourse \citep{vanDijk1983Discourse}. In building this situation model, the reader must identify the
text’s structure, discern relationships between key concepts, and continuously update their understanding as new
information is introduced. This process places substantial demands on attention and working memory.

Two lines of learning research are particularly relevant for interpreting AI summaries in reading contexts. First,
active generation typically improves later memory compared with passive exposure---the generation effect
\citep{SlameckaGraf1978GenerationEffect}. When readers produce their own explanations, outlines, or summaries, they
create retrieval routes that support later reconstruction. Second, retrieval practice improves durable learning
relative to restudying---the testing effect \citep{RoedigerKarpicke2006TestingEffect}. In applied settings, inserting
interpolated tests while studying long materials reduces mind-wandering and improves later performance
\citep{Szpunar2013InterpolatedTestingPNAS,Jing2016InterpolatedTestingApplied}, and the forward testing effect suggests
that retrieval can also enhance learning of subsequent material by improving attention and encoding processes
\citep{Chan2018ForwardTestingEffect}. These findings imply a key boundary condition for AI assistance: if AI summaries
substitute for self-generated processing, they may improve short-term task performance while weakening generative
retrieval and long-term integration.

A related implication concerns measurement: recognition and recall often reflect partially distinct retrieval
processes. Process dissociation approaches highlight that recognition can be supported by familiarity in addition to
recollection, whereas free recall depends more strongly on self-generated retrieval routes
\citep{Jacoby1991ProcessDissociation}. This distinction foreshadows the recognition--recall dissociation observed in
Study~1 and helps interpret why an AI-provided scaffold may boost MCQ performance without producing parallel gains in
free recall.

Because of these cognitive demands, the way an expository text is structured---and the presence of guiding cues or
previews---can significantly influence learning outcomes. Pre-reading aids, often termed \emph{advance organizers}
\citep{Ausubel1960AdvanceOrganizers,Ausubel1968EducationalPsychology}, activate relevant schemas and highlight the
organizational framework of the forthcoming content. Empirical studies find that students who receive a conceptual
outline or summary before reading complex text often achieve better comprehension and organization of recall
\citep{MayerBromage1980AdvanceOrganizersRecall,HartleyDavies1976PreinstructionalStrategies}. Structural cues such as
informative headings guide attention and improve memory for the organization of ideas \citep{LorchLorch1996HeadingsRecall}.

Importantly, these aids often improve \emph{integration} and comprehension even when total free recall of facts is
unchanged. Hartley and Davies \citep{HartleyDavies1976PreinstructionalStrategies} found that overviews do not
always increase verbatim recall but consistently improve learners’ ability to grasp and transfer key concepts.
Similarly, Stull and Mayer \citep{Stull2007GraphicOrganizers} showed that instructor-generated graphic organizers
improve conceptual performance even without an advantage on immediate factual recall. This distinction between
organizational understanding and verbatim detail is relevant for evaluating AI-generated summaries: summaries may
strengthen gist and coherence while potentially weakening detail-rich encoding.

\section{Cognitive Offloading and External Representations}
External representations are a long-standing feature of human cognition: people routinely use notes, outlines, and
digital tools to reduce memory demands and to coordinate complex tasks. In cognitive terms, this is often
described as \emph{cognitive offloading}---the strategic use of external artifacts to store information or to
reduce internal processing requirements \citep{Risko2016CognitiveOffloading}. Offloading can be adaptive in
environments where working-memory resources are limited and tasks are time-constrained
\citep{Baddeley2012WorkingMemory}.

The most visible modern example is internet search. When information is reliably accessible, people encode less of
the content itself and more of the location or access strategy \citep{Sparrow2011GoogleEffect}. Meta-analytic
evidence suggests that frequent search behavior is associated with reduced recall of content alongside improved
memory for where information can be found \citep{Gong2024GoogleEffectsMetaAnalysis}. These findings clarify an
important trade-off: external availability can improve task performance while weakening internal knowledge
structures if it displaces deep processing \citep{Craik1972LevelsOfProcessing}.

This trade-off has motivated broader arguments that pervasive digital scaffolding may reshape cognition over time by
changing the balance between internal memory and external retrieval habits. The ``online brain'' perspective
synthesizes evidence that ubiquitous access technologies can shift attentional strategies and memory reliance
\citep{Firth2019OnlineBrain}. While such shifts can be adaptive in complex environments, they can also create
dependence on external systems for knowledge continuity and reduce opportunities for effortful encoding and
self-generated retrieval.

Generative AI extends offloading beyond retrieval by producing \emph{transformed} representations (summaries,
explanations, and drafts). This transformation can function as a scaffold for comprehension and decision-making,
but it also changes source-monitoring conditions: when AI-generated content is fluent and plausible, users may
later confuse its origin or accept it with insufficient scrutiny
\citep{Johnson1993SourceMonitoring,Zhai2024OverRelianceAIDialogue}. Accordingly, the effects of generative AI cannot
be understood solely as ``more information access''; they depend on how AI representations are timed, structured,
and integrated into the user’s workflow.

\section{Generative AI as External Representation in Knowledge Work}
\subsection{Cognitive Effects of Generative AI Systems}
Generative AI systems can influence cognition through mechanisms such as prompting deeper processing, offloading
cognitive tasks, providing meta-cognitive feedback, and offering context-specific cues. Research indicates that AI
tools can improve learning outcomes when they encourage reflection and elaboration. For example, \citet{Bai2023ChatGPTLearningMemory}
found that students who interact with a large language model by asking questions and explaining answers can
achieve better learning results than those who study without AI. The dialogue can prompt deeper semantic processing,
consistent with the Levels of Processing framework \citep{Craik1972LevelsOfProcessing}. Similarly, \citet{Haider2024AICognitiveFunctions}
report improvements in recall and problem-solving in AI-assisted learning settings.

Beyond encoding, AI can support rehearsal and retrieval by providing external memory scaffolds. The Memoro system,
for instance, offers a real-time dialogue interface that prompts users to restate and verify information during
study \citep{Memoro2024RealTimeMemoryAugmentation}. This resembles retrieval practice: periodic prompting and
feedback can strengthen retention by reactivating information and integrating it with existing knowledge.
Complementing behavioral evidence, preliminary neuroscience work suggests that AI-assisted writing can modulate
neural engagement associated with attention and semantic processing \citep{MITMediaLab2023BrainOnChatGPT}.

AI systems can also support \emph{metacognition}---the monitoring and regulation of one’s own cognitive processes.
\citet{Sun2025GenerativeAICreativity} show that generative AI can improve metacognitive calibration in creative
tasks by prompting evaluation of alternatives and reflection on uncertainty. In learning contexts, similar prompts
may improve confidence judgments by making uncertainty explicit and by encouraging verification behaviors.

\subsection{Benefits and Risks of AI as External Representation}
The benefits of AI assistance are coupled with significant risks. One concern is that AI tools can encourage users
to invest less effort in understanding and remembering, since fluent answers and summaries are available on demand.
This may produce an illusion of competence: a user appears capable with AI support but has not internalized the
information \citep{Oakley2025MemoryParadox}. Such risk is consistent with the offloading literature and with
theories that emphasize deep processing and active engagement for durable memory
\citep{Craik1972LevelsOfProcessing,Baddeley2012WorkingMemory}.

Another risk is \emph{memory distortion} through AI-generated misinformation or suggestion. \citet{Chan2024ConversationalAIFalseMemories}
show that conversational AI can inadvertently increase false memories in tasks analogous to witness interviews,
highlighting how plausible, fluent AI output can become intertwined with a user’s own recollections. Systematic
reviews also caution that heavy dependence on AI tutors can reduce active engagement and weaken long-term skill
development \citep{Zhai2024OverRelianceAIDialogue}.

At a broader level, pervasive AI tools may shift cognitive habits by reallocating effort from memorizing details
toward managing interfaces and interpreting AI outputs \citep{Gerlich2025AIToolsSociety}. AI can also shape the
emotional and contextual aspects of memory: AI-curated experiences may heighten arousal or personalization, which
can strengthen certain memories, but can also narrow exposure and reduce diversity of encoded experiences
\citep{Beyaria2024NeuromarketingAIEmotionMemory}. Understanding when AI functions as a beneficial extension versus a
detrimental replacement of human memory is a central challenge; this thesis approaches the problem by examining
design-sensitive mechanisms, particularly timing and structure of AI-generated summaries.

\section{Confidence, Source Monitoring, and Cognitive Load in Assisted Cognition}
AI-generated summaries can alter not only what is learned, but also the conditions under which confidence is
formed and sources are discriminated. This section synthesizes three theoretical strands that motivate the key
design manipulations in Study~1: (i) cognitive load and split attention during learning, (ii) how timing of access
changes encoding and cue availability, and (iii) how source-monitoring demands shape vulnerability to plausible but
incorrect information.

\subsection{Foundational Theories Motivating Timing and Structure}\label{subsec:theories-timing-structure}
The experimental manipulations in this thesis are grounded in well-established findings in the learning sciences.
The AI-generated summary functions as a compact surrogate for an article’s key propositions. The timing of access
to this summary is manipulated to test whether seeing a summary \emph{before} reading helps to organize and
scaffold encoding of the full text, as opposed to seeing it mid-way or only after reading. The structure of the
summary is manipulated to test whether presenting it as an \emph{integrated paragraph} versus as \emph{segmented
bullet points} affects coherence building and source attributions.

\paragraph{Advance organizers and pre-reading scaffolds.}
Ausubel’s theory of advance organizers predicts that providing a high-level conceptual framework \emph{prior} to
learning enhances integration of new knowledge by activating relevant schemas and guiding attention during encoding
\citep{Ausubel1960AdvanceOrganizers,Ausubel1968EducationalPsychology}. Evidence on pre-instructional strategies
shows that previews (e.g., outlines or summaries) can improve comprehension and organization
\citep{MayerBromage1980AdvanceOrganizersRecall,HartleyDavies1976PreinstructionalStrategies}, and headings can guide
attention and improve recall of structure \citep{LorchLorch1996HeadingsRecall}. This suggests that pre-reading access
to AI summaries may be especially beneficial when the summary acts as an encoding scaffold rather than as a
post-hoc reminder.

\paragraph{Mid-reading interruptions and divided attention.}
Presenting an AI summary synchronously may introduce costs via task-switching and disrupted coherence: readers may
need to pause, reorient, and rebuild a situation model. Task interruption and divided attention can impose extra
working-memory demands, potentially weakening integration of information and increasing reliance on surface-level
cues \citep{Baddeley2012WorkingMemory}.

\paragraph{Cognitive load, split attention, and integrated presentation.}
Cognitive load theory emphasizes limited working-memory resources and predicts that formats that minimize
extraneous processing demands yield better learning \citep{Sweller1988CognitiveLoad}. The split-attention effect
shows that when related information is separated, learners must expend effort to integrate it, which can impair
learning \citep{ChandlerSweller1992SplitAttention}. Applied evidence confirms that integration vs.\ separation can
produce measurable performance differences \citep{PociaskMorrison2008SplitAttentionRedundancy}. In this context, an
integrated paragraph summary may reduce unnecessary scanning and facilitate verification against the source text,
whereas segmented bullet points may encourage piecemeal processing and increase coordination demands.

\subsection{Memory Distortion and Epistemic Risk}\label{subsec:distortion-risk}
The Source Monitoring Framework \citep{Johnson1993SourceMonitoring} emphasizes that remembering involves
determining where information came from (e.g., ``Did I read this in the article, or did it come from the summary?'').
Source attributions rely on contextual and qualitative features of a memory; when different sources overlap and
memories lack distinctive details, misattribution becomes more likely. The misinformation effect illustrates this
risk: misleading post-event information can be incorporated into memory and later misattributed to the original
event \citep{LoftusPalmer1974Misinformation,Loftus2005PlantingMisinformation}. Related work shows that suggestibility
increases when sources share overlapping details and contextual tags are weak \citep{LindsayJohnson1989SuggestibilitySource}.

Beyond individual paradigms, reviews of misinformation and memory distortion emphasize that false memories are not
rare anomalies but systematic outcomes of normal reconstructive processes under conditions of suggestion,
source-confusability, and fluency \citep{Frenda2011MisinformationAdvances}. For AI-assisted cognition, this is a
critical framing: when AI outputs are plausible and presented in ways that blur provenance cues, they can function
as post-event information that competes with or overwrites source material in memory.

False-memory research further underscores how gist-based representations can generate confident errors. In the DRM
paradigm, people falsely recall theme-consistent lure words \citep{RoedigerMcDermott1995FalseMemoriesDRM}. Fuzzy
Trace Theory explains this by positing that both gist and verbatim traces are encoded, but reliance on gist can
dominate when verbatim traces are weak \citep{BrainerdReyna2005ScienceFalseMemory}. Summaries emphasize gist and can
thus increase vulnerability to gist-consistent false recognitions, especially if AI output is plausible and fluent.
This motivates measuring epistemic risk (e.g., false-lure endorsement) in Study~1 and motivates design choices that
support provenance discrimination.

\section{Human--AI Decision-Making (Trust, Reliance, Bias, Accountability)}
\label{sec:human-ai-decision-making}
When AI outputs support decisions, the central behavioral question becomes not only \emph{accuracy} but also
\emph{calibrated reliance}: users must decide when to accept an AI representation, when to verify it, and when to
disregard it. In educational contexts, over-reliance can reduce active engagement and long-term skill development
\citep{Zhai2024OverRelianceAIDialogue}. In professional contexts, similar dynamics can create a dependence loop in
which decision-makers defer to AI recommendations even when they could reason independently, producing a knowledge
imbalance and increasing epistemic risk \citep{Zhang2024KnowledgeImbalanceAI}. These dynamics are more likely when
AI outputs are fluent, time pressure is high, and verification costs are non-trivial.

Work on AI assistants and user interpretations further suggests that interface framing can shape expectations about
agency and competence, influencing both trust and perceived responsibility---a dynamic that becomes more salient as
AI outputs are embedded into workflows and presented with conversational fluency \citep{KaplanHaenlein2018SiriSiriAI}.

Importantly, ``trust'' and ``reliance'' should be distinguished. Users may express general trust in AI systems while
still choosing not to rely on them in a given task, or they may rely heavily because the workflow makes
verification costly even when trust is low. In assisted cognition, reliance becomes a form of \emph{resource
allocation}: deciding whether to invest effort in independent reasoning, to cross-check sources, or to offload to an
external representation \citep{Risko2016CognitiveOffloading}. This implies that improving outcomes is not only about
increasing AI accuracy, but also about shaping the conditions of calibrated reliance (feedback, provenance cues, and
low-friction verification).

Reliance is also mediated by skill and literacy. Because generative AI outputs are probabilistic and can contain
plausible errors, effective use requires AI literacy: the ability to interpret outputs, recognize uncertainty, and
choose appropriate validation strategies \citep{WangPham2022AILiteracyScale}. Without such literacy, users may adopt
uncritical patterns of use (e.g., accepting fluent summaries as ground truth) or may underuse AI due to uncertainty
about when it is appropriate. This thesis therefore treats reliance not as a stable attitude, but as a dynamic
interaction between task constraints, user competencies, and representational design.

Reliance is also shaped by how AI is embedded in team settings. In organizational decision-making, the relevant
unit is often not the individual user but the \emph{human--AI--team} system: AI outputs are interpreted, debated,
and propagated through artifacts that multiple people consume. Conceptual and empirical work on combining human and
artificial intelligence for strategic decisions highlights both complementarities (speed of synthesis, scenario
exploration) and coordination risks when AI recommendations are treated as authoritative without scrutiny
\citep{TrunkBreitenecker2020HumanAIStrategicDecisionMaking,KaggwaFonsah2024AIDecisionMakingBusinessStrategies}. In
collaborative work, the benefits of AI assistance can also depend on experience and role configuration: evidence on
human--AI teaming suggests that workers with different levels of experience may collaborate with AI differently,
which can shape whether AI augments judgement or induces over-reliance \citep{WangGino2023FriendOrFoeAITeaming}.

Reliance is also shaped by accountability and transparency constraints. If an AI system influences decisions but
its rationale is opaque, organizations may struggle to audit errors, detect bias, or assign responsibility. The
``black-box'' problem is therefore not only a technical issue but a governance issue: opaque systems can encourage
uncritical acceptance while making failures hard to diagnose \citep{Pasquale2015BlackBoxSociety}. For roles that
mediate between stakeholders and technical systems---such as product management---this elevates the importance of
provenance cues, documentation practices, and norms of verification when AI-generated summaries and
recommendations enter decision workflows \citep{Pinzauti2025ReinventingPM}.

Finally, human--AI decision-making has an explicit ethical dimension. When AI representations influence judgments,
errors can be difficult to detect and responsibility can be difficult to assign, especially when AI content is
reused and propagated through documents, dashboards, and presentations. In such contexts, accountability is not only
about explaining model outputs; it is also about maintaining a decision trace that links claims to sources and
indicates which parts of a decision were AI-mediated \citep{Pasquale2015BlackBoxSociety}. This is directly relevant
for roles like product management, where decisions must be justified to stakeholders and later revisited.

\section{AI in Professional and Managerial Contexts}
\subsection{AI--Augmented Memory in Organizations and Applied Systems}
Beyond individual cognition, the integration of AI into memory processes has significant implications for
organizations and knowledge-intensive work. Enterprises increasingly rely on digital ecosystems in which AI
systems support information recall, contextual understanding, and knowledge retrieval, effectively functioning as
external, collective memory infrastructures.

AI tools can act as cognitive extensions in professional environments by organizing personal and collective
knowledge into searchable, semantically structured archives. By surfacing prior meeting notes, decision histories,
or relevant research exactly when needed, such systems externalize elements of working memory and help maintain
continuity in fast-paced workflows \citep{Memoro2024RealTimeMemoryAugmentation,Gerlich2025AIToolsSociety}. This
extends cognitive offloading \citep{Sparrow2011GoogleEffect,Risko2016CognitiveOffloading} into organizational
settings where information fragmentation is a constant challenge.

This perspective aligns with work on AI-assisted knowledge management, which emphasizes complementarity between AI
systems and human expertise \citep{Pai2022AIKnowledgeManagement}. AI can rapidly organize and filter information,
but it is most effective when guided by humans who provide context and interpretive insight. Together, they can
sustain an organizational memory that is more comprehensive and accessible than unaided human memory, yet more
flexible and meaningful than data left to accumulate without human interpretation.

\subsection{AI Adoption, Digital Transformation, and Role Reconfiguration}
At the organizational level, AI is increasingly framed as a general-purpose technology that reshapes business
processes, role boundaries, and strategic decision-making rather than as a point solution. Strategy and management
research discusses how AI can be combined with human judgement in organizational decisions, potentially improving
speed and breadth of analysis while creating new coordination and accountability challenges
\citep{TrunkBreitenecker2020HumanAIStrategicDecisionMaking,KamariotouKitsios2021AIBusinessStrategyDigitalTransformation,LiZhao2025TransformingOrganisationsThroughAI}. Industry
reports similarly emphasize that value creation depends on how organizations ``rewire'' processes and capabilities
around AI deployment rather than on model access alone \citep{McKinsey2025StateOfAIRewiring}.

This transformation is visible across business functions, where AI is adopted for customer-facing and back-office
work. Practitioner overviews document AI use in customer support and engagement \citep{Echegu2024AIinCustomerService},
operations management \citep{McGrath2024AITransformingOperationsManagementIBM}, financial services \citep{ChlouverakisKarapanos2024AIReshapingFinancialServices},
scenario analysis in corporate finance \citep{Bailey2025AITransformsScenarioAnalysisCFI}, and human-resource
management practices \citep{NawazAhmad2024AIAdoptionHRMPractices}. In many cases, adoption is paired with automation
technologies such as RPA and workflow tooling, which shift the division of labor by externalizing routine decision
steps into software artifacts \citep{Mangal2023RPAandAIAutomatingBusinessProcesses,EdlichIqbal2018BotsAlgorithmsCorporateSupport}.

At the same time, AI is increasingly treated as a strategic capability for competitive positioning and business
model innovation. Reviews and agendas highlight AI’s role in strategic decision-making and the emergence of new
business-model configurations enabled by algorithmic capabilities \citep{KaggwaFonsah2024AIDecisionMakingBusinessStrategies,Jorzik2024AIDrivenBusinessModelInnovationReview,Atsmon2023AIInStrategyMcKinsey,CorboFountaine2021ReinforcementLearningBusinessCourse,SoniKrishnan2019ImpactAIOnBusinesses}.
Such changes create new demands for organizational learning and training, including ``hands-on'' AI skill
development in business education and professional contexts \citep{Freire2024BusinessEducationHandsOnAI,WangPham2022AILiteracyScale}.

Applied AI--memory systems illustrate how algorithmic tools can function as external supports for encoding,
retrieval, and metacognitive regulation in real-world settings. In healthcare and neurotechnology, AI-guided
stimulation approaches have been shown to enhance episodic recall in clinical contexts
\citep{Kucewicz2023BrainStimulationMemory}. Complementing these intervention-oriented approaches, applied frameworks
also examine how AI can support memory impairments and caregiver decision-making (e.g., dementia care) through
monitoring, prompting, and personalized assistance \citep{Guzzi2023DementiaAIFramework}. In education, intelligent
tutoring systems improve learning outcomes by tailoring practice and feedback
\citep{Ma2014ITSMetaanalysis,Pane2014CognitiveTutorAlgebra,Alshaikh2021ITS}, and active learning improves durable
retention relative to passive reading \citep{Freeman2014ActiveLearning}. Generative AI tutors can leverage these
principles by sustaining reflective dialogue and prompting retrieval and self-explanation \citep{Abrar2025Cognitive}.
In everyday settings, augmented reality interfaces are emerging as a platform for on-demand contextual recall; the
``AR Secretary Agent'' combines wearable AR glasses with an LLM-based assistant to provide real-time memory
augmentation \citep{ElHaddad2025ARSecretaryAgent}.

\section{Product Management as a Decision-Intensive Domain}
\label{sec:pm-decision-domain}
Product management is a decision-intensive role that integrates heterogeneous evidence (user feedback, market
signals, analytics, technical feasibility, and stakeholder constraints) into product choices under uncertainty and
time pressure. In agile settings, product managers coordinate discovery and prioritization work while aligning
cross-functional teams around goals and trade-offs \citep{TkaliRomanova2022AgilePM}. These responsibilities make the
role highly dependent on \emph{external representations}: roadmaps, requirement documents, research summaries, and
internal knowledge bases stabilize collective understanding over time.

From a decision-process perspective, product management can be described as a recurring cycle of (i) problem
framing (defining user needs and business objectives), (ii) option generation (alternative solutions and
feature sets), (iii) prioritization under constraints (engineering capacity, time-to-market, risk), and (iv)
evaluation and iteration after release. Each step relies on artifacts that function as shared memory objects: PRDs,
user stories, sprint backlogs, dashboards, meeting notes, and stakeholder narratives. These artifacts do not merely
store information; they coordinate attention, align mental models across functions, and preserve rationale so that
decisions remain revisitable when assumptions change.

This artifact-centric nature makes product management a sensitive domain for AI-generated external representations.
If AI becomes the first producer of the artifacts that others rely on (summaries of research, synthesized user
feedback themes, competitive analyses, and executive-ready narratives), then AI outputs can shape the initial frame
of the decision problem and thereby influence downstream choices even when humans remain nominally in control. In
this sense, AI adoption in product management is not a narrow productivity add-on; it is a change in the cognitive
infrastructure through which teams remember, justify, and coordinate decisions.

Generative AI is increasingly positioned as an additional representational layer in product-management workflows,
supporting tasks such as summarizing customer feedback, drafting product narratives, and accelerating synthesis
across fragmented information sources \citep{Pinzauti2025ReinventingPM}. These uses are best understood as a mix of
automation (reducing repetitive synthesis work) and augmentation (supporting judgment with condensed
representations). However, they also shift part of the role toward \emph{evaluation and governance} of AI outputs:
product managers may need to detect when AI-generated summaries are incomplete or biased, maintain provenance cues,
and ensure that accountability for decisions remains clear \citep{Pasquale2015BlackBoxSociety}.

Practitioner and industry sources make this shift concrete by describing how AI is inserted into the everyday
artifact pipeline of product work. Guides aimed at product managers emphasize automation of documentation and
coordination tasks (e.g., drafting PRDs, writing user stories, generating meeting summaries, and preparing
stakeholder updates), as well as use cases in discovery and synthesis (e.g., clustering feedback themes, proposing
hypotheses, and generating experiment variants) \citep{Huryn2024AutomateWorkProductManager,Gupta2024AIUseCasesProductManagers,Nest2024AIProductManagementWorkflows}. In software-heavy settings, analyses of generative AI in product
development also highlight potential acceleration of time-to-market and changes in cross-functional collaboration,
as AI-generated artifacts flow between product, design, and engineering teams \citep{GnanasambandamHall2024GenAIAccelerateTimeToMarket}.

These accounts align with a broader business-operations framing in which AI supports both operational efficiency
and strategic exploration \citep{TarafdarMeijer2019AIEnhanceBusinessOperations,Atsmon2023AIInStrategyMcKinsey}. For
product managers, the implication is that AI becomes part of the decision infrastructure: it can influence which
options are generated, which evidence is highlighted, and how trade-offs are communicated, with downstream
implications for responsibility allocation and governance when AI-generated content is reused across the
organization.

The literature often distinguishes between \emph{automation} and \emph{augmentation} as two modes of AI value in
professional work. In product management, automation is typically associated with removing routine workload
(drafting documentation, reporting, ticket triage, and basic synthesis), whereas augmentation is associated with
improving higher-level cognitive work (sensemaking, scenario exploration, strategy articulation). While automation
benefits can be captured through time saved, augmentation benefits depend on whether the representations preserve
key assumptions, uncertainties, and trade-offs that support judgement. This distinction motivates Study~2’s focus on
both perceived efficiency gains and perceived decision-making changes (\Cref{sec:results-study2}).

At the same time, because product decisions are accountable to multiple stakeholders, AI-generated representations
can create organizational risks that mirror cognitive risks. If AI outputs are reused across documents without clear
provenance, errors can propagate and responsibility can diffuse. This makes governance mechanisms (documentation,
traceability, and explicit validation steps) part of the representational system itself rather than an external
compliance layer.

Because product management combines heavy information processing with high-stakes communication and justification,
it provides a natural applied setting for studying how AI-generated representations shape both cognition and
reliance. Competence in this setting plausibly includes \emph{AI literacy}---the ability to interpret, evaluate,
and appropriately use AI outputs rather than treating them as authoritative \citep{WangPham2022AILiteracyScale}.

\section{Synthesis and Research Gap}
Across cognitive and organizational perspectives, prior work converges on a common pattern: external
representations can enhance performance by providing cues and reducing load, but they can also induce cognitive
offloading and source confusion when they displace deep processing
\citep{Craik1972LevelsOfProcessing,Risko2016CognitiveOffloading,Sparrow2011GoogleEffect,Johnson1993SourceMonitoring}.
Generative AI intensifies this trade-off because it produces fluent, transformed representations whose provenance
may be ambiguous and whose errors can be incorporated into memory and judgment
\citep{Chan2024ConversationalAIFalseMemories,Zhai2024OverRelianceAIDialogue}. In organizations, AI-augmented
knowledge management promises continuity and efficiency \citep{Pai2022AIKnowledgeManagement}, yet can create
epistemic dependence and accountability challenges when AI outputs shape professional decisions
\citep{Zhang2024KnowledgeImbalanceAI,Pasquale2015BlackBoxSociety}.

Despite the rapid diffusion of generative AI, two limitations remain common in the current evidence base. First,
many cognitive studies operationalize ``AI assistance'' as a coarse treatment (AI vs no AI) without testing how
representation design features---timing, structure, and provenance cues---shape outcomes. This makes it difficult to
derive actionable design guidance: if outcomes depend on \emph{how} AI is embedded into the task, then binary
comparisons can obscure mechanisms and lead to inconsistent conclusions. Second, performance benefits are often
reported without parallel measurement of epistemic risk (e.g., misinformation acceptance, source confusions, or
miscalibrated reliance), even though these risks may be more consequential in decision settings than small changes in
accuracy.

On the organizational side, the emerging literature often documents adoption, perceived productivity gains, and
general attitudes, but less frequently connects observed practices to cognitive mechanisms. This is a meaningful gap
because organizations do not adopt ``AI'' in the abstract; they adopt representational artifacts (summaries, drafts,
analyses) that change how evidence is encoded, shared, and retrieved in collective work. Without a mechanism-based
link, it remains unclear why some adoption patterns improve decision quality while others introduce fragility,
accountability diffusion, or over-reliance.

A key gap is that these strands are often studied in isolation. Cognitive studies rarely connect interface-level
design choices (timing, structure, provenance cues) to professional reliance patterns, while organizational
accounts rarely link observed AI use to mechanisms such as cue-dependent retrieval and source monitoring. This
thesis addresses that gap with a multi-level design: Study~1 provides causal evidence on how timing and structure
of AI-generated summaries affect memory and epistemic risk in a controlled reading task, while Study~2
characterizes how product managers deploy AI-generated representations in decision-intensive work
\citep{Pinzauti2025ReinventingPM}. The next chapter introduces the \emph{AI Buffer} model as an integrative
framework to connect these levels and to derive propositions for safer, more effective AI-assisted cognition and
decision-making.


\chapter{Conceptual Framework}
\label{ch:conceptual-framework}

This chapter develops the conceptual framework that links the cognitive and organizational levels of analysis in
this thesis. Building on the literature review, it positions AI-generated external representations as a form of
cognitive infrastructure, introduces the AI Buffer Model proposed in this thesis, and then bridges cognitive
mechanisms to decision processes in product management. The chapter concludes with an integrated multi-level
framework and a set of research propositions/hypotheses aligned with RQ1--RQ3.

\section{Positioning: AI-Assisted External Representations as Cognitive Infrastructure}
\label{sec:positioning-cognitive-infrastructure}
Across domains, humans use external representations to stabilize information, reduce memory demands, and coordinate
complex tasks. In cognitive terms, this practice is often described as \emph{cognitive offloading}---the strategic
use of external artifacts to store information or reduce internal processing requirements
\citep{Risko2016CognitiveOffloading}. Digital search technologies provide an emblematic case: when information is
reliably accessible, people may encode less of the content itself and more of the access strategy
\citep{Sparrow2011GoogleEffect}. This does not imply that offloading is inherently harmful; rather, it highlights
that cognition adapts to the structure of the environment and the availability of external resources.

Generative AI extends external representation beyond retrieval by producing \emph{transformed} representations:
summaries, explanations, and recommendations that compress, reorganize, and reframe source material. Compared with
notes or static documents, these representations are often fluent, context-rich, and immediately reusable. As a
result, they can become part of the \emph{cognitive infrastructure} of a task---not merely an output to be read, but
an always-available representational layer that shapes what users attend to, what they encode, and which cues are
available during retrieval. From the perspective of encoding specificity \citep{Tulving1973EncodingSpecificity},
the introduction of a persistent AI representation can change both encoding conditions (what cues are present while
learning) and retrieval conditions (what cues are present when remembering), with direct implications for memory
performance and error.

AI-generated representations also introduce distinctive epistemic risks. When an external representation is
plausible and blends seamlessly with source content, users may later confuse its origin. The Source Monitoring
Framework emphasizes that remembering involves attributing content to its source; when contextual tags are weak,
source confusions and misattributions become more likely \citep{Johnson1993SourceMonitoring}. The risk is amplified
when AI content includes subtle inaccuracies or leading suggestions: generative systems can introduce details that
are later incorporated into a user’s recollection or judgment \citep{Chan2024ConversationalAIFalseMemories}.
Accordingly, AI-assisted external representations should be conceptualized not only as aids, but also as potential
drivers of distortion, over-reliance, and miscalibrated confidence \citep{Zhai2024OverRelianceAIDialogue}.

Finally, the notion of cognitive infrastructure has an organizational dimension. Knowledge-intensive work is
mediated by shared artifacts---documents, roadmaps, dashboards, and decision logs. When AI-generated summaries and
recommendations enter these artifact ecosystems, they can reshape collective memory and decision processes.
Organizations may benefit from improved knowledge continuity and retrieval \citep{Pai2022AIKnowledgeManagement},
while simultaneously facing new challenges of accountability, transparency, and epistemic dependence
\citep{Pasquale2015BlackBoxSociety,Zhang2024KnowledgeImbalanceAI}. This thesis therefore treats AI-assisted external
representations as a multi-level phenomenon: they influence individual cognition (RQ1), organizational practice in
product management (RQ2), and the link between these levels (RQ3).

\section{The AI Buffer Model (Proposed in this Thesis)}
\label{sec:ai-buffer-model}
Classical models of memory conceptualize encoding, storage, and retrieval as primarily internal processes,
regulated by attention, rehearsal, and executive coordination
\citep{Atkinson1968HumanMemory,Baddeley2012WorkingMemory,Tulving1972EpisodicSemantic}. However, contemporary AI
applications increasingly provide persistent, content-based representations that accompany users during cognitive
tasks, introducing an external layer of informational support.

To interpret the cognitive effects of such support, this thesis introduces the \emph{AI Buffer Model} as a
parsimonious conceptual lens. The \textbf{AI Buffer} refers to AI-generated external representations that persist
during a task and can influence memory encoding, retrieval, and metacognitive monitoring. The model does not posit
a new neurocognitive module, nor does it claim to be a universal theory of AI-augmented work. Instead, it
characterizes a \emph{functional role} that AI-generated representations may play in shaping memory-related
behavior under controlled conditions (Study~1) and in decision-intensive work settings (Study~2).

\subsection{Model Components and Design Levers}
The AI Buffer Model distinguishes three elements:
\begin{itemize}
  \item \textbf{Human memory system.} A capacity-limited working-memory workspace and long-term memory stores that
  support encoding, consolidation, and retrieval \citep{Baddeley2012WorkingMemory}.
  \item \textbf{AI-generated representation (the buffer).} A persistent external representation (e.g., a summary)
  that can be consulted during task execution and that provides semantic content and retrieval cues.
  \item \textbf{Task and interface context.} The constraints and interaction structure that govern how the buffer
  is accessed (e.g., timing of availability, integrated vs.\ segmented formatting) and how the user allocates
  attention between sources.
\end{itemize}

In Study~1, two interface-level levers operationalize properties of the AI Buffer:
\begin{itemize}
  \item \textbf{Timing of access:} whether the buffer is available before reading (pre-reading), during reading
  (synchronous), or after reading (post-reading). Timing changes whether the buffer functions as an advance
  organizer, an interruption, or a review aid \citep{Ausubel1960AdvanceOrganizers,Ausubel1968EducationalPsychology}.
  \item \textbf{Structure of presentation:} whether the buffer is presented as an integrated paragraph or as
  segmented bullet points. Structure changes coherence cues and coordination demands, and is expected to affect
  split-attention costs and verification behavior \citep{Sweller1988CognitiveLoad,ChandlerSweller1992SplitAttention}.
\end{itemize}

Other properties (e.g., factual accuracy, provenance cues, and the degree of interactivity) are treated as
important contextual variables, but are held constant or discussed as boundary conditions in the present work.

\subsection{Mechanisms Aligned with RQ1}
RQ1 asks how AI-generated external representations influence core human memory processes. Within the AI Buffer
Model, these influences are expressed through five tightly linked constructs: encoding, recall/recognition, source
monitoring, confidence calibration, and cognitive load.

\paragraph{Memory encoding.}
The buffer can shape encoding by changing what users attend to and how deeply they process the source material.
When the buffer functions as an advance organizer, it may activate schemas and provide a high-level framework that
guides attention toward central propositions \citep{Ausubel1960AdvanceOrganizers,Ausubel1968EducationalPsychology}.
This can promote semantic elaboration and more organized encoding, consistent with Levels of Processing theory
\citep{Craik1972LevelsOfProcessing}. Conversely, when a buffer is perceived as a reliable substitute for internal
encoding, it may encourage shallow processing and cognitive offloading: users may allocate less working-memory
effort to constructing a detailed internal representation because the external representation remains available
\citep{Risko2016CognitiveOffloading,Sparrow2011GoogleEffect}.

\paragraph{Recall and recognition.}
At retrieval, a buffer can serve as a cue source that reinstates aspects of the encoding context. Encoding
specificity predicts better retrieval when cues present at encoding are available at recall
\citep{Tulving1973EncodingSpecificity}. This cueing logic suggests that buffer access can improve recognition of
summary-aligned propositions even if free recall remains unchanged. At the same time, reliance on external cues may
reduce the need to reconstruct information from internal traces, which could weaken detail-rich recall when the
buffer is unavailable or when retrieval requires reconstruction beyond the buffer’s coverage.

\paragraph{Source monitoring.}
Because AI-generated content is fluent and semantically plausible, it can blur boundaries between what was read in
the original text and what was supplied by the buffer. The Source Monitoring Framework predicts greater
misattribution when sources share overlapping content and when contextual cues are weak
\citep{Johnson1993SourceMonitoring}. In AI-assisted settings, this risk extends to both inadvertent inaccuracies and
to structurally plausible but incorrect details. The potential for suggestion-driven errors is supported by
evidence that conversational AI can increase false memories under certain conditions
\citep{Chan2024ConversationalAIFalseMemories}. In the AI Buffer Model, source monitoring is therefore a central
epistemic-risk channel, not a peripheral outcome.

\paragraph{Confidence calibration.}
Confidence is a metacognitive judgment about the reliability of one’s own memory or decision. AI buffers can
increase confidence by providing fluent explanations and by reducing uncertainty during retrieval. However, fluency
is not equivalent to correctness: if the buffer introduces inaccuracies or if the user misattributes buffered
content to the source, confidence can become miscalibrated. This is especially problematic when high confidence
drives uncritical reliance on AI-generated representations \citep{Zhai2024OverRelianceAIDialogue}.

\paragraph{Cognitive load.}
The buffer can reduce load by compressing information and by offering structured cues, but it can also add load by
creating split attention and task-switching demands. Cognitive load theory predicts that learning is constrained
by working-memory capacity and that formats imposing extraneous coordination demands reduce learning efficiency
\citep{Sweller1988CognitiveLoad}. The split-attention effect predicts that separated information sources require
extra integration effort, whereas integrated formats reduce this burden
\citep{ChandlerSweller1992SplitAttention}. Accordingly, a segmented summary may increase coordination costs relative
to an integrated summary, while synchronous access may introduce interruption costs during reading.

\subsection{Boundary Conditions and Individual Differences}
The AI Buffer Model also highlights boundary conditions that shape whether external representations augment or
replace cognition. Prior knowledge can reduce reliance on the buffer because users can evaluate and integrate
information more independently. Conversely, high time pressure or low familiarity can increase reliance because
verification costs are higher. At the organizational level, AI literacy---the ability to interpret, evaluate, and
appropriately use AI outputs---is expected to moderate reliance behaviors and perceived benefits
\citep{WangPham2022AILiteracyScale}. These factors are relevant for both Study~1 (as individual differences) and
Study~2 (as professional competencies and organizational practices).

\section{From Cognitive Mechanisms to Decision Processes (Bridge)}
\label{sec:bridge-cognition-decisions}
While RQ1 focuses on memory mechanisms, knowledge-intensive work is ultimately evaluated through decisions and
actions. Many professional decisions require (i) retrieving relevant evidence from memory and artifacts, (ii)
integrating that evidence into a coherent situation model, and (iii) communicating a rationale that is defensible
to stakeholders. AI-generated external representations can influence each step by changing what information is
available, how it is trusted, and how it is justified.

The AI Buffer Model bridges cognition to decisions through three pathways.

\textbf{(1) Evidence selection and salience.} By compressing and reordering information, the buffer makes certain
propositions more salient than others. This can improve efficiency (faster access to core points), but also
introduce representational bias: what is summarized and how it is framed can shape which evidence is considered.
When users offload heavily, the buffer becomes a primary evidence source rather than a secondary aid
\citep{Risko2016CognitiveOffloading,Sparrow2011GoogleEffect}.

\textbf{(2) Epistemic control and verification.} Decisions in organizations often require traceability: the ability
to justify a choice by pointing to sources. Source monitoring failures can therefore become governance failures.
If users misattribute AI content to authoritative sources, decision rationales may be built on unverified claims.
This risk is magnified when AI systems are opaque or when their reasoning is difficult to audit
\citep{Pasquale2015BlackBoxSociety}.

\textbf{(3) Confidence-driven reliance.} Confidence affects whether people seek additional information, challenge a
recommendation, or defer to an automated output. If AI buffers increase confidence without improving accuracy,
they can encourage over-reliance. At scale, this can produce a ``knowledge imbalance'' in AI-advised decision-making
where decision-makers become dependent on representations they cannot fully evaluate
\citep{Zhang2024KnowledgeImbalanceAI}.

These pathways suggest that cognitive outcomes (encoding depth, source monitoring, confidence calibration, and
load) should be interpreted not only as individual-level effects, but also as mechanisms that can explain changes
in professional decision processes.

\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.15}
\begin{tabularx}{\textwidth}{l X X}
\hline
\textbf{Cognitive mechanism (RQ1)} & \textbf{Decision-process implication} & \textbf{Organizational risk / design need} \\
\hline
Offloading and cue-dependent retrieval & Faster access to ``good enough'' evidence; less internal reconstruction & Over-reliance under time pressure; need for verification norms \\
Source monitoring and misinformation & Misattribution of AI claims as authoritative sources & Accountability failures; need for provenance cues and audit trails \\
Confidence calibration & Greater decisiveness; reduced information-seeking when confidence is high & Overconfidence in flawed outputs; need for calibration feedback \\
Cognitive load and split attention & Reduced effort when summaries compress information; increased effort when switching sources & Interface design matters (integration, timing); need to minimize extraneous load \\
\hline
\end{tabularx}
\caption{How AI-assisted memory mechanisms can translate into decision-process dynamics.}
\label{tab:cognition-to-decisions}
\end{table}

\section{Organizational Layer: AI Adoption in Product Management}
\label{sec:org-layer-pm}
RQ2 focuses on product management as a decision-intensive domain in which AI adoption is expected to reshape both
work practices and responsibility structures. Product managers routinely integrate heterogeneous evidence---user
research, market signals, analytics, technical feasibility, and stakeholder constraints---into prioritization and
strategy under uncertainty and time pressure. In agile settings, they coordinate discovery and delivery while
aligning cross-functional teams around goals and trade-offs \citep{TkaliRomanova2022AgilePM}. These responsibilities
depend heavily on external representations: roadmaps, PRDs, experiment results, meeting notes, and decision logs
stabilize shared understanding and enable coordination across time.

Generative AI tools are increasingly positioned as an additional representational layer in these workflows
\citep{Pinzauti2025ReinventingPM}. Typical uses include summarizing customer feedback, drafting product narratives,
accelerating synthesis across fragmented sources, and supporting decision preparation (e.g., ``pros/cons'' lists,
alternatives, and risk summaries). These uses combine \emph{automation} (reducing repetitive synthesis work) and
\emph{augmentation} (supporting judgment with condensed representations). However, they also shift part of the PM
role toward evaluation and governance of AI outputs: product managers must detect when AI-generated summaries are
incomplete or biased, maintain provenance cues, and ensure that accountability for decisions remains clear
\citep{Pasquale2015BlackBoxSociety}.

The organizational layer therefore foregrounds three constructs that parallel the cognitive layer:
\begin{itemize}
  \item \textbf{Workflow reconfiguration:} which tasks are delegated to AI representations and which remain
  human-led.
  \item \textbf{Role configuration and skill shift:} movement toward strategic, interpretive, and oversight
  responsibilities; increasing importance of AI literacy \citep{WangPham2022AILiteracyScale}.
  \item \textbf{Governance and perceived responsibility:} who is accountable for AI-influenced decisions, and what
  oversight practices are adopted to prevent uncritical acceptance and mitigate bias \citep{Zhang2024KnowledgeImbalanceAI}.
\end{itemize}

Finally, adoption is expected to be context-sensitive. Organizational maturity, regulatory expectations, and
cultural norms can shape how much AI is used, which tasks are automated versus augmented, and how governance is
implemented. This motivates explicit cross-context comparison within Study~2 (e.g., USA--China) as part of RQ2’s
scope.

\section{Integrated Multi-Level Framework (Cognitive $\rightarrow$ Organizational)}
\label{sec:integrated-framework}
The core contribution of this thesis is an integrated framework that connects cognitive mechanisms of AI-assisted
memory (RQ1) to organizational decision-making practices in product management (RQ2), and uses the former to
explain the latter (RQ3).

At the \textbf{cognitive level}, the AI Buffer Model specifies how AI-generated external representations shape
encoding, retrieval, source monitoring, confidence calibration, and cognitive load. These mechanisms determine not
only whether people remember more or less, but also what \emph{kind} of knowledge is retained (gist vs.\ detail),
how confidently it is held, and whether its source is correctly attributed.

At the \textbf{decision-process level}, these cognitive outcomes translate into observable behaviors: how evidence
is selected, how options are generated and evaluated, and how rationales are communicated. For example, cue-driven
recognition may speed up decision preparation, whereas source confusions can undermine traceability and learning
from past decisions.

At the \textbf{organizational level}, repeated use of AI-generated representations can stabilize into norms of
reliance, documentation, and accountability. Over time, teams may increasingly treat AI outputs as default inputs
to decisions, potentially creating epistemic dependence and shifting expertise toward those who can evaluate and
govern AI systems \citep{Zhang2024KnowledgeImbalanceAI}. In parallel, organizations may invest in knowledge
management practices that embed AI into collective memory systems \citep{Pai2022AIKnowledgeManagement}.

Crucially, the framework includes \textbf{feedback loops}. Organizational norms and training influence individual
reliance and verification behavior (e.g., AI literacy and governance practices), while individual experiences of
accuracy, effort reduction, and error influence organizational adoption decisions. These loops imply that AI’s
impact is not static: the same tool can produce different outcomes depending on governance maturity, interface
design, and task context.

\section{Research Propositions / Hypotheses}
\label{sec:propositions-hypotheses}
This section formalizes testable expectations derived from the conceptual framework. Hypotheses are stated for the
controlled experiment (Study~1; RQ1), while propositions are stated for the organizational analysis (Study~2; RQ2)
and the cross-study integration (RQ3).

\subsection{Study 1 (RQ1): Cognitive-Level Hypotheses}
\label{sec:hypotheses-study1}
\begin{enumerate}
  \item \textbf{H1 (AI Buffer vs.\ No-AI; recall/recognition).} Relative to unaided reading, access to an AI Buffer
  (operationalized as an AI-generated summary) is expected to improve recognition performance for core
  propositions, with weaker or no improvement expected for free recall quality.

  \item \textbf{H2 (Timing; encoding scaffold vs.\ interruption).} Pre-reading access to the AI Buffer is expected
  to yield stronger encoding and higher recognition accuracy than synchronous or post-reading access, because it
  can function as an advance organizer rather than as an interruption or a post-hoc reminder
  \citep{Ausubel1960AdvanceOrganizers,Ausubel1968EducationalPsychology}.

  \item \textbf{H3 (Structure; split-attention and coherence).} Integrated-paragraph buffers are expected to reduce
  split-attention costs and support more coherent mental-model construction than segmented bullet-point buffers,
  resulting in higher recall quality and/or recognition accuracy \citep{Sweller1988CognitiveLoad,ChandlerSweller1992SplitAttention}.

  \item \textbf{H4 (Epistemic risk; source monitoring).} The presence of an AI Buffer is expected to increase
  vulnerability to source-monitoring errors and false recognitions for plausible but non-present details. This
  risk is expected to be higher under segmented structure than integrated structure, because segmentation can
  promote decontextualized processing and weaker provenance cues \citep{Johnson1993SourceMonitoring}.

  \item \textbf{H5 (Confidence calibration).} AI Buffer access is expected to increase subjective confidence in
  memory judgments. However, confidence may be less well calibrated to accuracy under high reliance conditions,
  especially for buffer-consistent false lures \citep{Zhai2024OverRelianceAIDialogue}.
\end{enumerate}

\subsection{Study 2 (RQ2): Organizational-Level Propositions}
\label{sec:propositions-study2}
\begin{enumerate}
  \item \textbf{P1 (Role transformation).} AI adoption in product management is expected to shift effort away from
  routine synthesis and coordination toward more strategic, interpretive, and oversight activities
  \citep{Pinzauti2025ReinventingPM}.

  \item \textbf{P2 (Decision-support centrality).} AI-generated external representations are expected to become
  central inputs to decision preparation and communication (e.g., summarizing evidence, framing options, and
  drafting rationale), increasing the speed of decision cycles while changing how evidence is selected and
  narrated \citep{Pinzauti2025ReinventingPM}.

  \item \textbf{P3 (Governance and accountability).} As AI outputs influence product decisions, perceived
  responsibility for verification, transparency, and ethical oversight is expected to increase, and governance
  mechanisms are expected to become more salient in PM practice \citep{Pasquale2015BlackBoxSociety}.

  \item \textbf{P4 (AI literacy as a capability).} Higher AI literacy is expected to be associated with more
  calibrated reliance and more effective integration of AI into PM workflows \citep{WangPham2022AILiteracyScale}.

  \item \textbf{P5 (Contextual moderation).} Adoption patterns, perceived responsibility, and governance emphasis
  are expected to vary across organizational and cultural contexts (e.g., USA--China), reflecting differences in
  norms, constraints, and maturity of AI deployment \citep{Pinzauti2025ReinventingPM}.
\end{enumerate}

\subsection{Cross-Study Integration (RQ3): Integrative Propositions}
\label{sec:propositions-rq3}
\begin{enumerate}
  \item \textbf{I1 (Offloading $\rightarrow$ reliance norms).} When AI Buffers reliably reduce cognitive effort,
  users and teams are expected to offload more frequently, which can scale into organizational norms of reliance
  under time pressure \citep{Risko2016CognitiveOffloading,Zhang2024KnowledgeImbalanceAI}.

  \item \textbf{I2 (Source monitoring $\rightarrow$ accountability).} Source-monitoring difficulties at the
  individual level are expected to manifest as accountability and traceability challenges at the organizational
  level, increasing the need for provenance cues, documentation, and verification practices
  \citep{Johnson1993SourceMonitoring,Pasquale2015BlackBoxSociety}.

  \item \textbf{I3 (Confidence calibration $\rightarrow$ trust and escalation).} Miscalibrated confidence in AI
  representations is expected to contribute to over-trust and insufficient escalation or verification, motivating
  organizational interventions that improve calibration (training, feedback, and auditability)
  \citep{Zhai2024OverRelianceAIDialogue}.
\end{enumerate}

\chapter{Methodology and Research Design}
\label{ch:methodology}

This chapter describes the research design and methods used to answer the three research questions introduced in
\Cref{sec:research_questions}. The thesis adopts a multi-level approach: Study~1 provides controlled evidence on
how AI-generated external representations affect memory mechanisms (RQ1), Study~2 provides field evidence on how
AI adoption reshapes decision practices and perceived responsibility in product management (RQ2), and the two are
integrated conceptually and analytically to explain organizational patterns through cognitive mechanisms (RQ3).

\section{Research Design Overview}
\label{sec:research-design-overview}
The central methodological challenge of this thesis is that the phenomenon of interest is inherently
multi-level. At the individual level, AI-generated representations can change attention allocation, encoding
depth, cue availability, and source monitoring, with downstream effects on recall, recognition, confidence, and
perceived cognitive load. At the organizational level, the same representational layer can become embedded in
workflows, shaping how evidence is synthesized, how decisions are justified, and how responsibility is distributed
in practice. No single method is sufficient to capture both levels with high validity.

Accordingly, the thesis combines two complementary designs:
\begin{itemize}
  \item \textbf{Internal validity (Study~1).} A controlled experiment isolates the causal effect of AI-generated
  summaries as an external representation by manipulating key design properties of the AI Buffer (timing and
  structure) under matched task constraints.
  \item \textbf{Ecological validity (Study~2).} A field study captures how AI tools are adopted and governed in a
  real managerial role (product management), including role configuration, decision support practices, and
  perceived accountability.
\end{itemize}

The two studies are linked by a shared conceptual object: \emph{AI-assisted external representations}. In Study~1,
this object is operationalized as pre-generated summaries that persist during a reading-and-memory task. In
Study~2, the object is operationalized as generative AI tools that produce summaries, analyses, and drafts within
product-management workflows. The conceptual bridge is provided by the AI Buffer Model introduced in
\Cref{sec:ai-buffer-model}.

\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.15}
\begin{tabularx}{\textwidth}{l l X X}
\hline
\textbf{Component} & \textbf{RQ} & \textbf{Design and unit of analysis} & \textbf{Primary measurements} \\
\hline
Study 1 & RQ1 & Controlled experiment; individuals performing a timed reading-and-memory task & Free recall, recognition, false-lure endorsement/source monitoring, confidence ratings, cognitive-load ratings, behavioral logs \\
Study 2 & RQ2 & Field study; product managers reporting AI adoption and decision practices in context & AI tool use, workflow shifts, decision-support patterns, governance/ethics and perceived responsibility, USA--China comparison, open-ended responses \\
Integration & RQ3 & Cross-study mapping (mechanisms $\rightarrow$ practices) & Mechanism-based explanation of observed organizational patterns and boundary conditions \\
\hline
\end{tabularx}
\caption{Multi-level research design linking RQ1--RQ3.}
\label{tab:research-design-overview}
\end{table}

\section{Study 1: Controlled Experiment (Cognitive Level -- RQ1)}
\label{sec:study1-method}
Study~1 answers RQ1: \emph{How do AI-generated external representations influence core human memory processes during
knowledge-intensive tasks?} The study uses a controlled reading paradigm in which an AI-generated summary
constitutes a persistent external representation (an AI Buffer) that participants can consult before, during, or
after reading. The design targets five operational dimensions aligned with RQ1: memory encoding, recall and
recognition, source monitoring, confidence calibration, and cognitive load.

\subsection{Participants and Setting}
Thirty-six adults were retained for analysis (\(N_{\mathrm{AI}}=24\), \(N_{\mathrm{NoAI}}=12\)), including university
students and early-career professionals. Participants were 18--35 years old (\(M=24.33\), \(SD=3.83\)) and were
balanced overall by gender (18 women, 18 men). All participants reported Chinese as their native language.

The study was delivered in a browser-based environment. Participant-facing materials were presented in Simplified
Chinese to match participants’ native language, while English descriptions are used in this thesis for exposition.

\begin{table}[t]
    \centering
    \small
    \caption{Participant demographics by group (Study~1).}
    \label{tab:participant-demographics}
    \begin{tabular}{lccc}
        \toprule
        & AI (\(n=24\)) & No-AI (\(n=12\)) & Total (\(N=36\)) \\
        \midrule
        Age, \(M\) (\(SD\)) & 24.50 (4.31) & 24.00 (2.76) & 24.33 (3.83) \\
        Age range & 18--35 & 20--30 & 18--35 \\
        Gender (F/M) & 13/11 & 5/7 & 18/18 \\
        Native language (Chinese), \(n\) (\%) & 24 (100\%) & 12 (100\%) & 36 (100\%) \\
        \bottomrule
    \end{tabular}
\end{table}

Participants received a base compensation of 60 RMB plus a performance-based bonus of up to 60 RMB linked to
recognition-test accuracy.

\subsection{Experimental Design and Conditions}
AI assistance was operationalized as an AI-generated summary of each article. Within the AI-assisted sample, the
core design forms a \(2 \times 3\) matrix:
\begin{itemize}
  \item \textbf{Summary structure (between-subjects):} integrated paragraph vs.\ segmented bullet points.
  \item \textbf{Summary timing (within-subjects):} pre-reading vs.\ synchronous during reading vs.\ post-reading.
\end{itemize}
A separate No-AI baseline group provides an external comparison but does not include the timing manipulation,
yielding an asymmetric mixed design that balances experimental control and feasibility.

\paragraph{Structure manipulation.}
The integrated condition presents the buffer as a coherent paragraph-style summary. The segmented condition
presents the same informational content as a set of bullet points. The manipulation targets coherence cues and
coordination demands: integrated presentation should reduce split-attention and verification costs relative to
segmented presentation \citep{Sweller1988CognitiveLoad,ChandlerSweller1992SplitAttention}.

\paragraph{Timing manipulation.}
Timing determines whether the buffer functions primarily as an encoding scaffold (pre-reading), as an interruption
and on-demand aid during comprehension (synchronous), or as a review cue after initial encoding (post-reading).
This manipulation is theoretically grounded in advance-organizer logic and interruption/switching-cost accounts
\citep{Ausubel1960AdvanceOrganizers,Ausubel1968EducationalPsychology}.

\subsection{Materials and Task Procedure}
\subsubsection{Materials}
\paragraph{Texts.}
Participants read three expository articles of approximately 1,300 words each (topics included urban heat islands,
CRISPR gene editing, and semiconductor supply chains). Articles were selected to be comparable in difficulty and
conceptual density and were administered in Simplified Chinese using fixed, pre-generated stimulus content. English
descriptions are used throughout the thesis for exposition, and study materials are provided in the Appendices.

\paragraph{AI summaries (structure and incompleteness).}
Summaries were generated and refined \emph{in advance} and were approximately 250 words. They were identical in
informational content across the two AI structure conditions, differing only in formatting (integrated paragraph vs.\
segmented bullets). Summaries were intentionally incomplete (approximately 15--20\% of article information omitted)
to discourage reliance on summaries alone and to ensure that some recognition items required article-based encoding.

\paragraph{False lures.}
To assess susceptibility to misinformation and source-monitoring errors, each article’s summary contained two
plausible but incorrect statements (\emph{false lures}). These lure concepts also appeared as distractor options in
the recognition test; selecting them was counted as false-lure acceptance.

\subsubsection{Platform and implementation}
The experiment was delivered via a custom browser-based application with a Python (Flask) backend and HTML/CSS/JavaScript
front-end pages. The platform was designed to support (i) strict enforcement of timing constraints across
conditions, (ii) automated counterbalancing and randomization, and (iii) fine-grained behavioral logging of reading
and summary exposure. All stimuli (articles, summaries, and MCQs) were preloaded, ensuring identical content across
participants and removing runtime variability due to on-the-fly AI generation.

\subsubsection{Procedure and timing constraints}
Participants completed three article blocks. In each block, participants encountered the summary according to their
assigned timing condition, read the article under a fixed time window, and then completed memory assessments and
post-block ratings. Article order was randomized. For AI participants, the mapping between article and timing
condition was counterbalanced through permutations so that each participant experienced each timing condition exactly
once.

\begin{table}[t]
    \centering
    \small
    \setlength{\tabcolsep}{5pt}
    \begin{tabularx}{\textwidth}{l l X l}
        \toprule
        \textbf{Condition} & \textbf{Summary timing} & \textbf{Summary access} & \textbf{Reading cap} \\
        \midrule
        No-AI & -- & None & 15 min \\
        Pre-reading & Before reading & Separate page, $\leq$ 3 min (continue early allowed) & 12 min \\
        Synchronous & During reading & On-demand open/close during reading & 15 min \\
        Post-reading & After reading & Separate page, $\leq$ 3 min (continue early allowed) & 12 min \\
        \bottomrule
    \end{tabularx}
    \caption{Implementation of timing conditions and exposure windows (Study~1).}
    \label{tab:timing-implementation}
\end{table}

After each reading phase, a short enforced break preceded testing to reduce immediate carryover and to standardize
the transition to retrieval. Free recall lasted up to 5 minutes and the MCQ block lasted up to 7 minutes, with
automatic submission at timeout. Back-navigation was disabled to prevent revisiting prior pages or materials.

\subsection{Measures and Operationalization of RQ1}
Study~1 operationalizes RQ1 through complementary behavioral, performance, and self-report measures aligned with the
AI Buffer Model (\Cref{sec:ai-buffer-model}).

\paragraph{Free recall (generative retrieval).}
After reading each article, participants produced a written free recall. Free recall was scored as an index of
episodic--semantic reconstruction of expository text rather than verbatim reproduction. In line with constructive
memory theory \citep{Bartlett1932Remembering}, responses were evaluated for fidelity to the original conceptual
structure (and penalized for plausible distortions), not for length or fluency.

Each recall received a single ordinal score from 0 to 10 based on five rubric dimensions: (i) factual accuracy,
(ii) mechanistic reconstruction, (iii) structural alignment, (iv) specificity vs.\ gist, and (v) source-monitoring
integrity (penalizing false-lure intrusions). To improve scoring consistency, anchor bands were used (9--10 high
fidelity; 7--8 mostly accurate; 5--6 gist-level; 3--4 headline-level; 1--2 fragmentary).

To reduce variability associated with qualitative rating, free-recall responses were initially scored using an LLM
rater applying the pre-specified rubric, and all scores were subsequently reviewed by a human rater (100\% of
responses) to confirm rubric adherence and correct questionable cases.

\paragraph{Recognition (cue-supported retrieval).}
Recognition was measured as proportion correct on 14 multiple-choice items per article. Items were categorized by
information source (summary-sourced vs.\ article-only) to separate recognition of summary-covered information from
comprehension of the underlying text. Misinformation susceptibility was indexed by the number of false-lure options
selected (0--2 per article; 0--6 overall).

\paragraph{Source monitoring and epistemic risk.}
Source monitoring is assessed primarily through false-lure endorsement and related misattributions, motivated by the
Source Monitoring Framework \citep{Johnson1993SourceMonitoring} and evidence that fluent AI interaction can amplify
false memories under some conditions \citep{Chan2024ConversationalAIFalseMemories}.

\paragraph{Confidence and cognitive load.}
Participants provided recall confidence and post-block ratings of mental effort and perceived difficulty using 7-point
scales. Confidence calibration is analyzed by relating confidence to accuracy, testing whether AI assistance
increases confidence without commensurate improvement in correctness.

\paragraph{Trust, dependence, and behavioral logs (process measures).}
AI-group participants provided baseline (general) measures of AI trust and AI dependence and post-block (state)
ratings of trust and dependence. Behavioral measures were computed from server logs, including reading time, summary
view time, number of summary openings, and interaction timestamps. These process indicators are used to triangulate
how timing and structure shape reliance and attention allocation.

\subsection{Data Quality and Exclusion Logic}
To preserve internal validity, sessions were excluded prior to analysis if participants failed to complete the full
protocol or showed clear non-compliance (e.g., implausibly short reading times indicating rushing or non-substantive
responses). The resulting dataset is structured at the block level (participant \(\times\) article), yielding 108
block-level observations (36 participants $\times$ 3 articles). This structure supports within-participant
comparisons across timing conditions in the AI group while retaining a No-AI baseline for reference.

\section{Study 2: Field Study on Product Managers (Organizational Level -- RQ2)}
\label{sec:study2-method}
Study~2 answers RQ2: \emph{How does the adoption of AI tools shape decision-making practices, role configuration,
and perceived responsibility in product management?} The study uses a cross-national field survey to capture how
product managers incorporate AI tools into their workflow, how they perceive role transformation and skill shifts,
and how they understand governance and ethical responsibility when AI influences decisions.

\subsection{Design and Sample}
The field study is based on a structured survey administered in two languages (English and Chinese) to support
participation across contexts. The final dataset comprises 74 validated responses, evenly split across the United
States (\(n=37\)) and China (\(n=37\)). This balanced structure enables both (i) estimation of overall patterns of
AI adoption in product management and (ii) a comparative lens on how adoption and responsibility perceptions vary
across contexts (USA--China), directly addressing the cross-context component of RQ2.

The focus on the United States and China is motivated by their central roles in the global AI ecosystem and by
differences in organizational environments that may shape adoption (e.g., governance norms, tool ecosystems, and
data-driven practices). The comparative design is therefore not treated as a mere ``case split'', but as a way to
identify which adoption patterns are robust across contexts and which patterns are context-contingent. This logic
directly supports RQ2 and also provides a foundation for the integrative explanation in RQ3.

Responses were screened for completeness and coherence (e.g., removing incomplete submissions and invalid patterns).
The thesis treats the resulting dataset as an analytic snapshot of AI adoption in a decision-intensive managerial
role rather than as a representative census of all product managers.

\subsection{Survey Structure and Measurement Strategy}
The survey combines Likert-scale items, multiple-choice items, and open-ended questions. This combination is
intentional: structured items quantify adoption patterns and perceived impacts, while open-ended responses capture
contextual nuance (e.g., examples of AI-supported decisions, perceived risks, and organizational constraints).

At a high level, the instrument is organized around four measurement domains aligned with RQ2:
\begin{itemize}
  \item \textbf{AI tool usage and task coverage:} frequency and type of AI applications used in daily PM work,
  including automation tools, analytics, and generative AI for drafting and synthesis.
  \item \textbf{Role configuration and workflow reallocation:} perceived changes in responsibilities, time saved
  through AI support, and whether time is reallocated toward more strategic work.
  \item \textbf{Skill shift and AI literacy:} perceived need for new competencies (e.g., interpreting AI outputs,
  prompt practices, data literacy) and organizational support for upskilling.
  \item \textbf{Governance, ethics, and responsibility:} perceptions of accountability, transparency, bias
  management, and customer-trust implications when AI informs decisions or product features.
\end{itemize}

Operationally, the survey is structured in four sections (A--D). Section A measures AI tool usage and organizational
adoption context (e.g., which tools are used, who drives adoption, and what goals motivate integration). Section B
measures workload reduction, tasks that become more complex due to AI, and how time savings are reinvested (e.g.,
strategic planning, innovation). Section C focuses on skill development, confidence, and training resources. Section
D focuses on governance and ethics (e.g., responsibility for AI-mediated decisions, guideline structures, and ethical
concerns). This structure is aligned with the conceptual framework’s organizational layer and supports both
descriptive analysis and cross-context comparisons.

In terms of measurement design, Section A combines (i) frequency scales for distinct AI application categories
(e.g., automation and workflow tools, analytics, NLP-based feedback analysis, generative AI drafting tools, and
chatbots) with (ii) adoption-maturity items (e.g., duration of use) and (iii) open-ended prompts for tool names and
examples. Sections B--D use a mix of Likert-scale judgments and categorical items to capture both intensity (how
strongly an impact is perceived) and structure (who owns responsibility, whether guidelines exist, what forms of
training are available). This combination supports the thesis’s interpretation that AI adoption is not only a level
of usage, but a configuration of practices and governance structures \citep{Pinzauti2025ReinventingPM}.

\section{Data Collection Instruments and Materials}
\label{sec:instruments-materials}
Across both studies, the thesis uses instruments designed to capture behavior, judgment, and subjective
experience around AI-generated external representations.

\paragraph{Study 1 instruments (experiment).}
Study~1 uses (i) expository reading materials, (ii) AI-generated summaries in two presentation formats (integrated
vs.\ segmented), (iii) a free-recall prompt, (iv) a recognition test per article including false-lure distractors,
and (v) post-block rating scales for perceived cognitive load and confidence. The experimental platform records
fine-grained interaction logs (e.g., time spent reading and time spent consulting the summary) to provide
behavioral indicators of reliance.

\paragraph{Study 2 instruments (field survey).}
Study~2 uses a bilingual structured survey comprising Likert-scale items, multiple-choice items, and open-ended
questions. The instrument measures frequency of AI use across task categories, perceived role transformation,
time savings and reallocation, skill development and training, and governance/ethics perceptions. The bilingual
format is intended to reduce language barriers and support valid cross-context comparison.

\paragraph{Integration materials.}
For RQ3, the key ``instrument'' is the mapping logic specified by the conceptual framework in
\Cref{ch:conceptual-framework}. Cognitive constructs from Study~1 (offloading, cue-dependent retrieval, source
monitoring, confidence calibration, cognitive load) are used as explanatory lenses for interpreting adoption and
governance patterns observed in Study~2.

\section{Ethical Considerations}
\label{sec:ethics}
Both studies involve minimal-risk procedures and follow standard research ethics principles: voluntary
participation, informed consent, the right to withdraw, and privacy-preserving data handling.

\paragraph{Study 1 (experiment).}
Participants were informed about the study procedures, data collection, and compensation, and provided consent
before participation. Personal identifiers were not stored with behavioral data. Because the experiment includes
plausible but incorrect statements in AI summaries to test source monitoring and misinformation susceptibility,
the study requires careful debriefing: participants should be informed after participation that some summary
content was intentionally inaccurate for research purposes, to avoid leaving lasting misconceptions.

\paragraph{Study 2 (field survey).}
Survey participation was voluntary and responses were collected and analyzed in aggregated form. The survey was
designed to avoid collection of sensitive corporate information (e.g., proprietary product details). Any
open-ended responses are treated as confidential and are reported only in anonymized, non-identifying form.

\paragraph{Data management.}
Across studies, datasets are stored in access-controlled locations and are used only for research purposes.
Reporting focuses on patterns rather than on identifying individuals or organizations, consistent with the thesis’s
goal of developing generalizable insights about AI-assisted cognition and decision-making.

\chapter{Data Analysis}
\label{ch:data-analysis}

This chapter specifies how data from Study~1 (controlled experiment; RQ1) and Study~2 (field study on product
managers; RQ2) are transformed into analytic constructs, prepared for analysis, and analyzed. It also defines the
cross-study integration logic used to address RQ3 and summarizes how validity, reliability, and robustness are
handled across the full thesis.

\section{Constructs and Operationalization}
\label{sec:constructs-operationalization}
The guiding principle for operationalization is alignment with the three research questions. RQ1 requires
measuring memory mechanisms under AI-assisted external representations (the AI Buffer). RQ2 requires measuring how
AI adoption reshapes decision practices and perceived responsibility in product management. RQ3 requires a mapping
between the constructs: organizational patterns are interpreted through cognitive mechanisms.

\subsection{Study 1 Constructs (RQ1)}
Study~1 operationalizes the AI Buffer as an AI-generated summary and defines experimental conditions via two design
factors:
\begin{itemize}
  \item \textbf{AI Buffer presence (between):} No-AI baseline vs.\ AI-assisted.
  \item \textbf{AI Buffer properties (AI participants):} \emph{timing} (pre-reading, synchronous, post-reading) and
  \emph{structure} (integrated paragraph, segmented bullet points).
\end{itemize}

Study~1 outcomes map directly to the RQ1 components:
\begin{itemize}
  \item \textbf{Recall and recognition.} Free-recall responses capture reconstruction from internal traces, while
  recognition items capture cue-supported memory performance.
  \item \textbf{Source monitoring.} False-lure endorsement in recognition provides an index of epistemic risk and
  misattribution under AI assistance, motivated by source-monitoring theory \citep{Johnson1993SourceMonitoring}.
  \item \textbf{Confidence calibration.} Confidence ratings are used to assess whether AI assistance increases
  confidence and whether confidence aligns with accuracy (calibration).
  \item \textbf{Cognitive load.} Post-block ratings of mental effort and perceived difficulty operationalize
  subjective cognitive load under each condition.
  \item \textbf{Behavioral reliance (supporting indicator).} Platform logs (e.g., time spent viewing the summary,
  number of openings, and reading time) operationalize reliance behavior and exposure to the AI Buffer.
\end{itemize}

\subsection{Study 2 Constructs (RQ2)}
Study~2 operationalizes product-management adoption of AI tools through survey measures that cover four domains:
\begin{itemize}
  \item \textbf{AI usage and task coverage.} Frequency and types of AI tools used in daily PM work (automation,
  analytics, generative drafting/synthesis, decision support).
  \item \textbf{Workflow and role configuration.} Perceived task reallocation and role transformation (e.g., shift
  toward strategic work; changes in coordination, analysis, and leadership responsibilities).
  \item \textbf{Perceived responsibility and governance.} Accountability, transparency, oversight practices,
  fairness/bias concerns, and customer-trust implications when AI informs product decisions
  \citep{Pasquale2015BlackBoxSociety}.
  \item \textbf{Context and moderation.} Region (USA vs.\ China) as a key grouping variable, allowing cross-context
  comparison aligned with RQ2’s comparative component \citep{Pinzauti2025ReinventingPM}.
\end{itemize}

When multiple items measure a shared latent concept (e.g., perceived responsibility or AI literacy), items are
combined into composite indices by averaging standardized item scores, subject to internal-consistency checks (see
\Cref{sec:validity-reliability-robustness}).

\subsection{Cross-Study Constructs (RQ3)}
To answer RQ3, Study~2 patterns are interpreted through the cognitive mechanisms measured in Study~1. The mapping
uses the AI Buffer Model constructs from Chapter~3:
\begin{itemize}
  \item \textbf{Offloading and cue-dependent retrieval} (effort substitution and reliance norms).
  \item \textbf{Source monitoring and epistemic risk} (traceability and accountability).
  \item \textbf{Confidence calibration} (trust and escalation behavior).
  \item \textbf{Cognitive load and split attention} (interface costs and workflow friction).
\end{itemize}

\section{Data Preparation and Quality Checks}
\label{sec:data-prep}
Data preparation follows two goals: (i) ensure the dataset represents compliant task execution and valid responses,
and (ii) ensure constructs reflect the intended operational definitions.

\subsection{Study 1 Data Preparation}
Study~1 data are structured at the \textbf{block level} (participant \(\times\) article), enabling within-person
comparisons across timing conditions for AI participants while retaining a No-AI baseline group.

Preparation includes:
\begin{itemize}
  \item \textbf{Session completeness.} Exclude incomplete sessions and blocks with missing core outcomes (recall or
  recognition).
  \item \textbf{Compliance screening.} Flag and exclude clearly non-compliant behavior (e.g., implausibly short
  reading time suggesting rushing, non-substantive recall text).
  \item \textbf{Derived variables.} Compute block-level indices: recall score, recognition accuracy, false-lure
  endorsement count, confidence measures, cognitive-load ratings, and behavioral reliance metrics from logs.
  \item \textbf{Randomization checks.} Verify balance in article assignment and timing order (AI group), and confirm
  that key participant characteristics do not differ systematically across between-subject structure conditions.
\end{itemize}

\subsection{Study 2 Data Preparation}
Study~2 data are structured at the \textbf{respondent level} (one row per product manager). Preparation includes:
\begin{itemize}
  \item \textbf{Validity screening.} Retain only complete and internally consistent responses; remove duplicate or
  corrupted entries when identifiable.
  \item \textbf{Missing data handling.} For item non-response, apply listwise deletion for analyses requiring the
  item, and report the effective sample size for each test. If missingness is non-trivial for a composite
  construct, compute the index only when a minimum proportion of items is present.
  \item \textbf{Coding and harmonization.} Harmonize categorical response labels across languages; code open-ended
  responses into qualitative themes for interpretive analysis.
  \item \textbf{Cross-context consistency.} Ensure that the USA and China cohorts are treated symmetrically in
  variable coding, scaling, and aggregation to support valid comparison.
\end{itemize}

\section{Analysis Strategy for Study 1 (Experiment)}
\label{sec:analysis-study1}
The Study~1 analysis strategy mirrors the experimental design and the RQ1 decomposition into memory mechanisms.
Because AI participants contribute repeated measures across three timing conditions, analyses account for
within-participant dependence.

Analyses were conducted in \textbf{R} using a combination of mixed-design ANOVA and mixed-effects modelling. For
ANOVA-style tests, sum-to-zero contrasts and Type-III tests were used to align inference with the factorial design.
For mixed-effects models, random intercepts were specified for participants and (where applicable) for articles to
account for repeated measurement and stimulus heterogeneity. Post-hoc contrasts were estimated using marginal-means
frameworks with Holm correction for multiple comparisons.

\subsection{Primary comparisons}
The primary comparisons are:
\begin{itemize}
  \item \textbf{AI vs.\ No-AI (baseline effect).} Compare AI-assisted blocks vs.\ No-AI blocks on recognition
  accuracy, recall quality, cognitive load, confidence, and epistemic risk.
  \item \textbf{Timing effects (AI only).} Compare pre-reading vs.\ synchronous vs.\ post-reading timing within the
  AI group.
  \item \textbf{Structure effects (AI only).} Compare integrated vs.\ segmented structures between AI participants.
  \item \textbf{Timing $\times$ structure (AI only).} Test whether timing effects differ by summary structure.
\end{itemize}

\subsection{Models and outcome-specific approaches}
Analyses are performed using regression models appropriate to outcome type:
\begin{itemize}
  \item \textbf{Recall quality.} Modeled as a continuous or ordinal outcome at the block level (depending on score
  distribution), with predictors for timing and structure and participant-level random effects.
  \item \textbf{Recognition accuracy.} Modeled either at the item level (binary correct/incorrect) using logistic
  mixed-effects models, or at the block level using binomial or proportion models, with predictors for timing and
  structure and participant-level random effects. Where feasible, recognition items are partitioned into
  summary-aligned vs.\ article-only subsets to distinguish buffer-supported performance from article-dependent
  performance.
  \item \textbf{Source monitoring / false-lure endorsement.} Modeled as a count (e.g., 0--2 per article) or as a
  binary item-level event using count models or logistic mixed-effects models. The main inferential focus is on
  whether segmented structure increases false-lure endorsement relative to integrated structure.
  \item \textbf{Confidence and calibration.} Confidence is modeled as an outcome and also used as a predictor of
  accuracy to test calibration. Calibration is assessed by relating confidence to correctness (e.g., via
  regression or correlation-based indices) and by comparing calibration across conditions.
  \item \textbf{Cognitive load.} Mental-effort and difficulty ratings are modeled as continuous outcomes; timing is
  expected to increase load when it introduces interruptions and split attention, while integrated structure is
  expected to reduce extraneous load.
  \item \textbf{Behavioral reliance.} Summary view time and opening counts are analyzed descriptively and via
  mixed-effects models to test whether timing and structure change reliance behavior. These metrics are also used
  as covariates or sensitivity controls to assess whether performance effects persist beyond differential exposure.
\end{itemize}

\subsection{Inference and reporting}
Unless otherwise stated, tests are two-sided with \(\alpha = 0.05\). Results are reported with effect sizes and
confidence intervals where appropriate. Because multiple outcomes are analyzed, robustness checks include assessing
whether key conclusions persist under reasonable alternative specifications (e.g., alternative outcome
transformations and exclusion thresholds).

Reporting follows two principles. First, because outcome types differ (continuous, proportions, and binary events),
models are chosen to respect the measurement scale and to avoid artefactual conclusions. Second, because the thesis
aims to explain mechanisms rather than to maximize the number of significant results, interpretation emphasizes
pattern coherence across measures (e.g., timing effects on recognition supported by process measures), not only
p-values.

\section{Analysis Strategy for Study 2 (Field Study)}
\label{sec:analysis-study2}
The Study~2 analysis strategy is designed to characterize adoption patterns, role transformation, and perceived
responsibility in product management (RQ2) and to support cross-context comparison (USA--China).

Following the logic of \Cref{sec:study2-method}, each survey item is interpreted through two linked lenses: an
\emph{overall} lens (what pattern is present across the full sample) and a \emph{comparative} lens (whether the USA
and China cohorts differ). This dual emphasis ensures that the study can report both (i) general adoption patterns
that plausibly reflect role-level dynamics in product management and (ii) context-contingent differences that may
reflect organizational environments and governance norms \citep{Pinzauti2025ReinventingPM}.

\subsection{Quantitative analysis of structured items}
For Likert-scale items, the analysis proceeds in three steps:
\begin{itemize}
  \item \textbf{Descriptive statistics.} Compute means, standard deviations, and distribution plots for each item
  and for composite indices (where applicable).
  \item \textbf{Overall effects.} Test whether the overall sample indicates above-neutral endorsement of key
  statements (e.g., increased strategic focus, increased governance responsibility) using one-sample tests against
  the neutral midpoint (typically 3 on a 1--5 scale).
  \item \textbf{Cross-context comparison.} Compare the USA and China cohorts using independent-samples tests for
  continuous-like outcomes and contingency analyses for categorical outcomes.
\end{itemize}

For categorical or multiple-choice items (e.g., tool selection, governance ownership, training modes), chi-square
tests are used to evaluate (i) whether observed distributions differ from uniform or expected baselines (goodness of
fit) and (ii) whether response distributions differ by region (tests of independence). When distributions are
sparse, categories are consolidated to preserve interpretability.

\subsection{Qualitative analysis of open-ended responses}
Open-ended responses are analyzed using thematic coding to identify recurring patterns relevant to RQ2, including:
examples of AI-supported decisions, perceived risks (bias, opacity, over-reliance), governance practices, and
skill-development narratives. Themes are compared across contexts (USA--China) to identify convergences and
divergences, and qualitative insights are used to contextualize quantitative patterns.

To improve transparency and replicability, coding follows a staged approach: (i) open coding to identify candidate
themes, (ii) consolidation into a codebook aligned with the measurement domains in \Cref{sec:study2-method}, and
(iii) axial coding to connect themes to role transformation and governance narratives. Where themes differ by
region, interpretation is tied back to the comparative logic of RQ2 (context-contingent adoption patterns).

\section{Cross-Study Integration Logic (RQ3)}
\label{sec:cross-study-integration}
RQ3 asks how changes in organizational decision-making practices enabled by AI can be explained through underlying
cognitive mechanisms of AI-assisted memory. The integration logic follows a mechanism-to-practice mapping approach:

\begin{enumerate}
  \item \textbf{Extract mechanism signatures from Study~1.} Identify which AI Buffer properties (timing, structure)
  change (i) performance (recall/recognition), (ii) epistemic risk (false-lure endorsement/source monitoring), and
  (iii) metacognitive and effort signals (confidence and cognitive load).

  \item \textbf{Extract practice patterns from Study~2.} Identify how AI tools are used in product management
  (automation vs.\ augmentation vs.\ decision support), what role shifts are reported, and how responsibility and
  governance are perceived and implemented.

  \item \textbf{Map patterns to mechanisms.} Interpret practice patterns through the AI Buffer mechanisms:
  \begin{itemize}
    \item Workflow acceleration and increased strategic focus are mapped to \emph{offloading and cue-supported
    cognition} (effort substitution and faster evidence access).
    \item Governance emphasis and accountability concerns are mapped to \emph{source monitoring} and \emph{epistemic
    risk} (traceability failures under misattribution).
    \item Trust and reliance narratives are mapped to \emph{confidence calibration} (when confidence is increased
    by fluency rather than by accuracy).
    \item Adoption friction and interface concerns are mapped to \emph{cognitive load} and \emph{split attention}
    (coordination costs and interruptions).
  \end{itemize}

  \item \textbf{Specify boundary conditions.} Use Study~2 context differences (USA--China) and reported skill
  differences (AI literacy and training) as candidate moderators of the mechanism-to-practice link, consistent with
  the framework in \Cref{sec:integrated-framework}.
\end{enumerate}

The goal of integration is explanatory coherence: the thesis does not treat Study~1 and Study~2 as two separate
projects, but as complementary evidentiary sources that jointly support the AI Buffer Model as an account of how
AI-generated external representations reshape knowledge work.

\section{Validity, Reliability, and Robustness}
\label{sec:validity-reliability-robustness}
Because the thesis combines experimental and field designs, validity and robustness are addressed at multiple
levels.

\subsection{Study 1}
\textbf{Internal validity.} Randomization and controlled timing constraints support causal interpretation of timing
and structure effects. Compliance screening reduces noise from non-compliance.

\textbf{Construct validity.} RQ1 constructs are measured with complementary outcomes (recall, recognition, false
lures, confidence, cognitive load, logs), reducing reliance on any single metric. False-lure endorsement provides a
direct operationalization of epistemic risk rooted in source-monitoring theory \citep{Johnson1993SourceMonitoring}.

\textbf{Reliability.} When qualitative scoring is required (e.g., recall quality), reliability is supported by a
pre-specified rubric and by systematic review procedures. Sensitivity checks include verifying that results do not
hinge on a small number of ambiguous scoring cases.

\textbf{Robustness.} Robustness checks include alternative model specifications (e.g., item-level vs.\ block-level
recognition models), inclusion/exclusion of covariates (e.g., reading time and summary exposure), and alternative
exclusion thresholds for compliance.

\subsection{Study 2}
\textbf{External validity.} The field study measures AI adoption in a real decision-intensive role. The cross-national
design increases generalizability by comparing two influential contexts (USA and China).

\textbf{Measurement reliability.} For multi-item constructs, internal consistency is assessed (e.g., Cronbach’s
alpha) and items are combined only when coherence is acceptable. For qualitative themes, coding consistency is
supported through structured codebooks and review.

\textbf{Cross-context comparability.} The bilingual instrument design is intended to reduce language barriers and
support semantic equivalence across cohorts. Analyses check that observed differences are not driven solely by
systematic missingness or coding artifacts.

\subsection{Cross-study robustness}
Cross-study claims are treated as \emph{explanatory} rather than purely statistical. Robustness is evaluated by
triangulation: an integrative claim is considered stronger when (i) it is consistent with Study~1 mechanisms, (ii)
it aligns with Study~2 reported practices, and (iii) it remains plausible under alternative boundary conditions
(e.g., differences in AI literacy, governance maturity, or time pressure).

\chapter{Results}
\label{ch:results}

\section{Study 1 Results (RQ1)}
\label{sec:results-study1}

This section reports the outcomes of the controlled experiment and addresses \textbf{RQ1}: \emph{How do AI-generated
external representations influence core human memory processes during knowledge-intensive tasks?} Results are
organized around the mechanisms specified in the AI Buffer Model (encoding scaffolding, cue-dependent retrieval,
source monitoring, confidence calibration, and cognitive load; \Cref{sec:ai-buffer-model}).

\subsection{Sample overview and outcome structure}
The experimental dataset includes $N=36$ participants (AI: $n=24$; No-AI: $n=12$), each completing three reading
blocks with comprehension and memory assessments, yielding $108$ block-level observations. Within the AI group, the
experimental manipulation followed a $2$ (summary structure: integrated vs segmented) $\times$ $3$ (summary timing:
pre-reading vs synchronous vs post-reading) mixed design (details in \Cref{sec:study1-method}).

\subsection{Reporting conventions}
For each outcome, we report descriptive statistics (mean and standard deviation) followed by inferential tests. For
tests within the AI group, timing is treated as a within-participant factor and structure as a between-participant
factor. Where post-hoc timing contrasts are reported, p-values are Holm-corrected within the relevant family of
comparisons. Unless stated otherwise, analyses use the complete-case dataset described in
\Cref{sec:study1-method,sec:analysis-study1}.

\subsection{Recognition and learning outcomes (MCQ accuracy)}
\label{sec:study1-results-mcq}

\paragraph{AI vs No-AI baseline.}
Across all multiple-choice items, the AI condition outperformed the No-AI condition (AI: $M=0.598$, $SD=0.085$;
No-AI: $M=0.510$, $SD=0.098$), $F(1,34)=7.86$, $p=.008$, $\eta^2_p=0.188$. This establishes a global recognition
benefit associated with AI summaries. In absolute terms, this corresponds to an 8.8 percentage-point gain in
recognition accuracy (Cohen’s $d \approx 0.98$).

\begin{table}[H]
    \centering
    \small
    \begin{tabular}{lcc}
        \toprule
        \textbf{Group} & \textbf{Overall MCQ accuracy (Mean $\pm$ SD)} & \textbf{$N$} \\
        \midrule
        AI-assisted & $0.598 \pm 0.085$ & 24 \\
        No-AI & $0.510 \pm 0.098$ & 12 \\
        \bottomrule
    \end{tabular}
    \caption{Overall MCQ accuracy by AI availability (Study~1).}
    \label{tab:study1-baseline-ai-noai}
\end{table}

\paragraph{Timing effects within the AI group.}
When recognition is indexed specifically to information covered by the AI summary (\emph{AI-summary-sourced}
questions), pre-reading access yields the highest accuracy. AI-summary-sourced accuracy by timing is:
pre-reading $M=0.833$ ($SD=0.141$), synchronous $M=0.568$ ($SD=0.239$), and post-reading $M=0.641$ ($SD=0.196$).
A mixed ANOVA (structure $\times$ timing; AI group) shows a strong timing main effect, $F(2,44)=14.00$,
$p<.001$, $\eta^2_p=0.389$, with no structure main effect and no interaction. Holm-corrected comparisons indicate
pre-reading outperforms synchronous ($p=.00052$, $d_z=0.91$) and post-reading ($p=.00057$, $d_z=0.87$); the
synchronous vs post-reading contrast is not significant ($p=.148$).

\paragraph{Overall MCQ accuracy (all questions).}
The same timing ordering appears in overall MCQ accuracy (pre-reading $M=0.699$, $SD=0.125$; synchronous
$M=0.533$, $SD=0.147$; post-reading $M=0.562$, $SD=0.127$), with a significant timing effect,
$F(1.77,38.87)=11.77$, $p<.001$.

\begin{figure}[H]
    \centering
    \subfloat[AI-summary-sourced accuracy by timing (AI group).]{
        \includegraphics[width=0.49\textwidth]{slide1_main_finding_1_ai_summary_accuracy.png}
        \label{fig:study1-ai-summary-accuracy}
    }
    \hfill
    \subfloat[Overall MCQ accuracy by structure and timing (AI group).]{
        \includegraphics[width=0.49\textwidth]{A1_plot_mcq_accuracy.png}
        \label{fig:study1-mcq-accuracy}
    }
    \caption{Recognition outcomes in Study~1. Error bars indicate $\pm$SE.}
    \label{fig:study1-recognition}
\end{figure}

\paragraph{Boundary condition: article-only comprehension.}
Timing does not meaningfully affect accuracy on questions that are not covered by the AI summary (article-only
accuracy), indicating that the timing manipulation primarily changes learning for summary-covered information rather
than broadly improving comprehension of the underlying article.

\begin{figure}[H]
    \centering
    \subfloat[Article-only accuracy by timing (AI group).]{
        \includegraphics[width=0.49\textwidth]{A1_plot_article_accuracy.png}
        \label{fig:study1-article-accuracy}
    }
    \hfill
    \subfloat[Decomposition of timing effects into summary- vs article-sourced accuracy (AI group).]{
        \includegraphics[width=0.49\textwidth]{timing_decomposition.png}
        \label{fig:study1-timing-decomposition}
    }
    \caption{Timing diagnostics for Study~1: timing primarily affects learning of summary-covered content.}
    \label{fig:study1-timing-diagnostics}
\end{figure}

\subsubsection{Mechanism-oriented robustness checks for the timing advantage}
To test whether the timing advantage is explained by summary-aligned encoding and/or by increased exposure to the
summary, two complementary model-based checks were conducted.

\paragraph{Summary-alignment mechanism.}
AI-summary-sourced accuracy strongly predicts overall MCQ accuracy in a mixed-effects model controlling for timing
and structure ($\beta = 0.472$, $p < .001$). When AI-summary-sourced accuracy is included as a predictor, the
pre-reading advantage on overall MCQ accuracy shrinks substantially (pre--synchronous: $\Delta = 0.172 \rightarrow
0.043$; pre--post: $\Delta = 0.143 \rightarrow 0.044$) and is no longer significant after Holm correction
($p = .352$). Model fit improves markedly ($\chi^2(1) = 46.38$, $p < .001$), supporting the interpretation that
timing primarily increases learning of summary-covered content rather than broadly raising comprehension.

\paragraph{Exposure control (summary time).}
Controlling for log-transformed summary exposure reduces the timing estimates only modestly, and the timing
contrasts remain significant, suggesting that pre-reading advantages are not explained solely by ``more time'' on
the summary. Summary time itself is a positive predictor of AI-summary-sourced accuracy ($\beta = 0.062$, $p = .031$).

\begin{table}[H]
    \centering
    \small
    \begin{tabular}{lcc}
        \toprule
        \textbf{Model} & \textbf{Pre--Synchronous estimate} & \textbf{Pre--Post estimate} \\
        \midrule
        Base (timing only) & 0.271*** & 0.214*** \\
        $+$ log(summary\_time) & 0.258*** & 0.164** \\
        \textbf{Reduction} & \textbf{5\%} & \textbf{23\%} \\
        \bottomrule
    \end{tabular}
    \caption{Timing contrasts on AI-summary-sourced accuracy with and without controlling for summary exposure.}
    \label{tab:study1-timing-contrast-summary-time}
\end{table}

\noindent{\small \textit{Note:} *** $p<.001$, ** $p<.01$.}

\subsection{Generative retrieval: free recall is unaffected by timing}
\label{sec:study1-results-recall}

In contrast to recognition performance, free recall does not vary by timing in the AI group: pre-reading $M=5.50$
($SD=1.92$), synchronous $M=5.54$ ($SD=1.99$), and post-reading $M=5.56$ ($SD=2.17$). A mixed ANOVA shows no
timing effect, $F(1.86,40.88)=0.03$, $p=.969$. Additionally, an AI vs No-AI comparison indicates no meaningful
recall difference (AI: $M=5.535$; No-AI: $M=5.403$; $p>.50$). Together, these findings indicate that AI assistance
primarily affects cue-supported recognition rather than internally generated retrieval.

\begin{figure}[H]
    \centering
    \subfloat[Recall total score by timing (AI group).]{
        \includegraphics[width=0.49\textwidth]{A1_plot_recall_total_score.png}
        \label{fig:study1-recall}
    }
    \hfill
    \subfloat[Mental effort by condition (AI and No-AI).]{
        \includegraphics[width=0.49\textwidth]{A1_plot_mental_effort.png}
        \label{fig:study1-mental-effort}
    }
    \caption{Secondary outcomes in Study~1: recall and cognitive load. Error bars indicate $\pm$SE.}
    \label{fig:study1-secondary-recall-effort}
\end{figure}

\subsection{Source monitoring and misinformation: structure shapes false-lure endorsement}
\label{sec:study1-results-source-monitoring}

Susceptibility to false AI-generated claims is primarily driven by summary structure. In the AI group, segmented
summaries increase false-lure endorsement relative to integrated summaries. Participants exposed to segmented
summaries select more false lures (integrated: $M=0.58$, $SD=0.69$; segmented: $M=1.06$, $SD=0.79$), and show lower
false-lure accuracy (integrated: $M=0.556$, $SD=0.354$; segmented: $M=0.375$, $SD=0.302$). A binomial GLMM
predicting false-lure endorsement yields an odds ratio of $5.93$ for segmented structure (95\% CI $[1.63,21.5]$;
$p=.007$).

\begin{figure}[H]
    \centering
    \subfloat[False-lure accuracy by structure and timing (AI group).]{
        \includegraphics[width=0.49\textwidth]{A1_plot_false_lure_accuracy.png}
        \label{fig:study1-false-lure-accuracy}
    }
    \hfill
    \subfloat[False-lure endorsement probability by structure (AI group).]{
        \includegraphics[width=0.49\textwidth]{ORD_plot2_lure_prob_by_structure.png}
        \label{fig:study1-false-lure-prob}
    }
    \caption{Source-monitoring outcomes in Study~1. Error bars indicate $\pm$SE.}
    \label{fig:study1-source-monitoring}
\end{figure}

\subsection{Confidence calibration}
\label{sec:study1-results-calibration}

Metacognitive calibration (confidence relative to actual performance) is imperfect overall, but AI does not worsen
overconfidence. Recall confidence is similar across AI availability (AI: $M=4.25$; No-AI: $M=4.08$; scale 1--7),
and an independent-samples t-test on overconfidence indicates no difference between groups, $t(31.7)=0.16$,
$p=.873$.

\begin{figure}[H]
    \centering
    \subfloat[Overconfidence by group (AI vs No-AI).]{
        \includegraphics[width=0.49\textwidth]{F2_plot_overconfidence.png}
        \label{fig:study1-overconfidence}
    }
    \hfill
    \subfloat[Recall calibration by group.]{
        \includegraphics[width=0.49\textwidth]{H3_plot_recall_calibration_by_group.png}
        \label{fig:study1-recall-calibration}
    }
    \caption{Confidence calibration in Study~1. Error bars indicate $\pm$SE.}
    \label{fig:study1-calibration}
\end{figure}

\subsection{Process evidence: timing redistributes attention without increasing total time}
\label{sec:study1-results-process}

The timing manipulation changes how attention is allocated between summary and article. In the AI group, pre-reading
produces the highest summary viewing time (mean $132.5$ seconds) and summary share ($24.9$\%), while post-reading is
lowest (mean $69.5$ seconds; $13.7$\%). Importantly, total time-on-task remains stable across timing conditions
(approximately $531$--$536$ seconds), indicating a redistribution of attention rather than more time spent overall.

\begin{table}[H]
    \centering
    \small
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        \textbf{Timing (AI group)} & \textbf{Summary time (s)} & \textbf{Reading time (min)} & \textbf{Total time (s)} & \textbf{Summary share (\%)} \\
        \hline
        Pre-reading & 132.5 & 6.72 & 535.8 & 24.9 \\
        Synchronous & 100.3 & 7.19 & 531.6 & 19.5 \\
        Post-reading & 69.5 & 7.69 & 530.8 & 13.7 \\
        \hline
    \end{tabular}
    \caption{Time allocation by summary timing (Study~1, AI group; means).}
    \label{tab:study1-time-allocation}
\end{table}

\begin{figure}[H]
    \centering
    \subfloat[Summary time (seconds).]{
        \includegraphics[width=0.24\textwidth]{A1_plot_summary_time_sec.png}
        \label{fig:study1-summary-time}
    }
    \hfill
    \subfloat[Summary share of total time.]{
        \includegraphics[width=0.24\textwidth]{A1_plot_summary_prop.png}
        \label{fig:study1-summary-share}
    }
    \hfill
    \subfloat[Reading time (minutes).]{
        \includegraphics[width=0.24\textwidth]{A1_plot_reading_time_min.png}
        \label{fig:study1-reading-time}
    }
    \hfill
    \subfloat[Total time (seconds).]{
        \includegraphics[width=0.24\textwidth]{A1_plot_total_time_sec.png}
        \label{fig:study1-total-time}
    }
    \caption{Process measures in Study~1 (AI group). Error bars indicate $\pm$SE.}
    \label{fig:study1-process}
\end{figure}

\paragraph{Trust and dependence.}
Post-block ratings indicate that perceived reliance tracks timing and, for dependence, also differs by structure.
Mixed-design analyses show a timing effect for trust ($F(2,43)=7.90$, $p=.001$) and both structure and timing
effects for dependence (structure: $F(1,22)=6.21$, $p=.021$; timing: $F(2,43)=7.74$, $p=.001$). These subjective
patterns converge with the process evidence that pre-reading increases engagement with the external representation at
the point where it can shape encoding.

\begin{figure}[H]
    \centering
    \subfloat[Trust by structure and timing (AI group).]{
        \includegraphics[width=0.49\textwidth]{A1_plot_ai_trust.png}
        \label{fig:study1-trust}
    }
    \hfill
    \subfloat[Dependence by structure and timing (AI group).]{
        \includegraphics[width=0.49\textwidth]{A1_plot_ai_dependence.png}
        \label{fig:study1-dependence}
    }
    \caption{Subjective reliance in Study~1 (AI group). Error bars indicate $\pm$SE.}
    \label{fig:study1-reliance}
\end{figure}

\subsection{Robustness checks and supplementary diagnostics}
\label{sec:study1-results-robustness}

Robustness checks support the stability of the main conclusions. Counterbalancing diagnostics indicate that timing
conditions were distributed across articles as intended; leave-one-article-out analyses show that the timing
advantage for AI-summary-sourced accuracy persists when any single article is removed. Supplementary plots also
characterize potential stimulus heterogeneity (e.g., article difficulty) and the decomposition of timing effects.

\begin{figure}[H]
    \centering
    \subfloat[Counterbalancing check: timing by article.]{
        \includegraphics[width=0.32\textwidth]{EXP_fig_counterbalancing_timing_by_article.png}
        \label{fig:study1-counterbalancing}
    }
    \hfill
    \subfloat[Leave-one-article-out robustness (timing effect).]{
        \includegraphics[width=0.32\textwidth]{slide4_robustness_leave_one_article_out.png}
        \label{fig:study1-loo}
    }
    \hfill
    \subfloat[Article difficulty diagnostic.]{
        \includegraphics[width=0.32\textwidth]{ORD_plot3_article_difficulty.png}
        \label{fig:study1-article-difficulty}
    }
    \caption{Robustness and diagnostic checks for Study~1.}
    \label{fig:study1-robustness}
\end{figure}

\section{Study 2 Results (RQ2)}
\label{sec:results-study2}

This section reports results from the field study and addresses \textbf{RQ2}: \emph{How does the adoption of AI tools
shape decision-making practices, role configuration, and perceived responsibility in product management?} The final
dataset comprises $N=74$ validated responses, evenly split between the United States ($n=37$) and China ($n=37$)
(\Cref{sec:study2-method}).

\subsection{Adoption patterns and tool portfolio}
\label{sec:study2-results-adoption}

Respondents report moderate-to-high use of AI tools across several categories, with generative AI tools showing the
highest average usage intensity. Expectations are generally met or exceeded (mean $3.45$ on a 1--5 scale), and
respondents anticipate continued growth in AI usage over the next 12 months (mean $3.66$).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{study2_plot_tool_usage.png}
    \caption{Self-reported AI tool usage intensity in product management (Study~2; error bars show $\pm$SD).}
    \label{fig:study2-tool-usage}
\end{figure}

\subsection{Workload shifts and role reconfiguration}
\label{sec:study2-results-workload}

AI adoption is associated with differentiated workload reduction across task categories. The strongest perceived
reductions occur for routine tasks and documentation/content creation, while routine communications show the weakest
reduction. Consistent with the role-reconfiguration claims in \Cref{sec:pm-decision-domain}, this pattern suggests
that AI is primarily used to automate or accelerate operational work, freeing capacity for higher-level activities.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{study2_plot_workload_reduction.png}
    \caption{Perceived workload reduction by product-management task (Study~2; error bars show $\pm$SD).}
    \label{fig:study2-workload-reduction}
\end{figure}

\subsection{Decision-making support and strategic impact}
\label{sec:study2-results-decision}

Respondents report positive perceived impacts on productivity and decision-making. Perceived productivity gain is
above the neutral benchmark ($M=3.47$, $SD=1.10$; $t(73)=4.77$, $p<.001$), and decision-making improvement is also
positive ($M=3.45$, $SD\approx 1.05$; $t(73)=3.34$, $p=.0013$). Reported strategic time gained through automation is
modest but significant (mean $0.38$ hours per week; $t(73)=5.33$, $p<.001$). Cross-national comparisons indicate a
significant difference in perceived decision-making improvement, with Chinese respondents reporting higher perceived
benefits ($t(72)=-2.63$, $p=.0104$).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{study2_plot_strategic_impacts.png}
    \caption{Strategic and governance impacts of AI adoption (Study~2; error bars show $\pm$SD).}
    \label{fig:study2-strategic-impacts}
\end{figure}

\subsection{Responsibility, governance, and ethics}
\label{sec:study2-results-governance}

Ethical issues are reported as occurring less than ``occasionally'' on average ($M=2.65$, $SD\approx 1.09$;
$t(73)=-2.54$, $p=.013$). However, governance and accountability remain unresolved for many respondents. The most
frequent response for ethical responsibility indicates no clear ownership (33.8\%), and the most frequent response
for the existence of ethical AI guidelines indicates no clear guidelines (36.5\%). Distributional tests further show
non-uniformity in ethical guideline structures (overall $\chi^2(3)=15.56$, $p<.001$) and ethical responsibility
distribution (overall $\chi^2(4)=10.26$, $p=.037$), with cross-national differences for both responsibility
distribution ($p=.003$) and guideline structures ($p=.0017$). These findings align with the thesis claim that AI
adoption changes not only task execution but also accountability boundaries in decision-intensive roles.

\begin{table}[H]
    \centering
    \small
    \begin{tabularx}{\textwidth}{|X|X|c|}
        \hline
        \textbf{Survey item} & \textbf{Most frequent response} & \textbf{\%} \\
        \hline
        AI tools used by product managers & ChatGPT & 78.4\% \\
        Comparison of AI integration goals in product management & Reducing operational costs & 52.7\% \\
        Top factors selecting AI tools & Cost-effectiveness & 58.1\% \\
        Tasks that became more complex due to AI & Human-in-the-loop checks & 75.7\% \\
        Tasks prioritized with time saved by AI & Product innovation & 85.1\% \\
        Primary responsibility for ethical AI decisions & No clear ownership/responsibility defined & 33.8\% \\
        Existence of ethical AI guidelines in organizations & No clear guidelines exist & 36.5\% \\
        \hline
    \end{tabularx}
    \caption{Selected categorical highlights from Study~2 (most frequent response by item).}
    \label{tab:study2-categorical-highlights}
\end{table}

\subsubsection{Cross-national governance patterns}
To contextualize the ``no clear ownership'' and ``no clear guidelines'' patterns, Tables~\ref{tab:study2-ethics-responsibility}
and \ref{tab:study2-ethics-guidelines} report the full response distributions by region. Two patterns are salient.
First, the U.S.\ cohort reports substantially higher ambiguity in responsibility ownership (18 vs.\ 7 reporting ``no
clear ownership''), while the China cohort more frequently attributes responsibility to product managers
individually. Second, the U.S.\ cohort reports more formal internal guideline structures (18 vs.\ 4), while the
China cohort reports higher reliance on informal guidance.

\begin{table}[H]
    \centering
    \small
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Responsibility structure} & \textbf{China} & \textbf{USA} & \textbf{Overall} & \textbf{\% Overall} \\
        \midrule
        Formal organizational guidelines/policies & 7 & 6 & 13 & 17.6\% \\
        Product managers individually & 11 & 0 & 11 & 14.9\% \\
        Dedicated AI/Ethics committee & 4 & 6 & 10 & 13.5\% \\
        Cross-functional team/collaboration & 7 & 6 & 13 & 17.6\% \\
        No clear ownership/responsibility defined & 7 & 18 & 25 & 33.8\% \\
        \bottomrule
    \end{tabular}
    \caption{Responsibility for ethical AI decisions by region (Study~2; counts and overall percentages).}
    \label{tab:study2-ethics-responsibility}
\end{table}

\begin{table}[H]
    \centering
    \small
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Guideline structure} & \textbf{China} & \textbf{USA} & \textbf{Overall} & \textbf{\% Overall} \\
        \midrule
        Formal internal AI ethics guidelines exist & 4 & 18 & 22 & 29.7\% \\
        No clear guidelines exist & 16 & 11 & 27 & 36.5\% \\
        Informal ethical guidance only & 12 & 7 & 19 & 25.7\% \\
        External standards followed (e.g., GDPR, EU AI Act) & 4 & 0 & 4 & 5.4\% \\
        \bottomrule
    \end{tabular}
    \caption{Existence of ethical AI guidelines in product management by region (Study~2).}
    \label{tab:study2-ethics-guidelines}
\end{table}

\section{Cross-Study Integration (RQ3)}
\label{sec:results-cross-study}

This section addresses \textbf{RQ3}: \emph{How can changes in organizational decision-making practices enabled by AI
be explained through underlying cognitive mechanisms of AI-assisted memory?} The cross-study logic treats Study~1 as
mechanistic evidence about \emph{how} external representations shape memory and metacognition, and Study~2 as field
evidence about \emph{where} those mechanisms become operational in decision-intensive work settings.

\subsection{Integrated interpretation across levels}
The combined findings support three integration claims consistent with the AI Buffer Model and the multi-level
framework introduced in \Cref{sec:integrated-framework}.

\paragraph{Claim 1: ``AI-first framing'' is effective when it shapes encoding, not when it replaces it.}
Study~1 shows a strong timing advantage for pre-reading summaries on recognition outcomes, supported by process
evidence that timing redistributes attention toward the summary without increasing total time. Study~2 indicates that
PMs use AI to reduce routine workload and report improved decision-making. Together, these results suggest that
organizational benefits are most plausible when AI is used early to frame problems and structure attention, while
humans still perform critical integration and judgement.

\paragraph{Claim 2: Fragmentation increases misattribution risk, motivating governance mechanisms.}
Study~1 demonstrates that segmented presentations substantially increase false-lure endorsement (odds ratio $5.93$),
pointing to a source-monitoring vulnerability when AI-provided information is fragmented. Study~2 shows that
accountability and ethical governance are often unclear and that guidelines are frequently absent. The integration
implication is that organizations should treat the \emph{provenance} and \emph{structure} of AI outputs as part of
decision governance: integrated, traceable artifacts should reduce misattribution and support responsibility
assignment.

\paragraph{Claim 3: Reliance is not purely ``overconfidence''; it is a shift in cognitive infrastructure.}
Study~1 finds that AI does not worsen overconfidence but does change reliance and attention allocation (trust and
dependence track timing). Study~2 complements this by showing that AI adoption reallocates work toward higher-level
activities while introducing new responsibilities (e.g., human-in-the-loop checks). The integrated interpretation is
that reliance operates through infrastructural shifts in the external representation: who curates it, when it is
consulted, and how it is audited.

\begin{table}[H]
    \centering
    \small
    \begin{tabularx}{\textwidth}{|X|X|X|}
        \hline
        \textbf{Cognitive mechanism (Study~1)} & \textbf{Empirical pattern (Study~1)} & \textbf{Organizational implication (Study~2)} \\
        \hline
        Encoding scaffold (advance organizer) & Pre-reading yields highest AI-summary-sourced accuracy; timing is a strong driver of recognition & AI is most valuable when used early to frame options and guide subsequent work, rather than as a late-stage ``answer engine'' \\
        \hline
        Cue-dependent retrieval and offloading & MCQ improves while free recall is unchanged (recognition--recall dissociation) & Decision practices may become artifact-driven (easy retrieval of AI-framed options) without equivalent deep internalization; verification practices become essential \\
        \hline
        Source monitoring & Segmented structure increases false-lure endorsement (OR $5.93$) & Fragmented AI usage across tools/channels can increase misattribution and accountability diffusion; integrated documentation and provenance controls are risk mitigations \\
        \hline
        Confidence calibration & AI does not increase overconfidence relative to No-AI & Governance should not rely on self-reported confidence; instead it should require explicit checks (e.g., references, validation, sign-off) \\
        \hline
        Load allocation and reliance & Pre-reading increases summary engagement without increasing total time; reliance measures track timing & Adoption shifts time from operational to strategic work, but also shifts responsibility to monitoring, auditing, and escalation of AI outputs \\
        \hline
    \end{tabularx}
    \caption{Cross-study integration linking cognitive mechanisms (Study~1) to product-management practices (Study~2).}
    \label{tab:cross-study-integration}
\end{table}

\chapter{Discussion}
\label{ch:discussion}

\section{Answering the Research Questions}
\label{sec:discussion-answering-rqs}

\subsection{RQ1: How do AI-generated external representations influence core human memory processes?}
\label{sec:discussion-rq1}

Study~1 provides converging evidence that AI-generated summaries function as \emph{external representations} that
selectively reshape memory performance depending on \emph{when} and \emph{how} they are presented
(\Cref{sec:results-study1}). Three implications are central for RQ1.

\paragraph{First, AI assistance improves cue-supported recognition, but does not strengthen generative retrieval.}
The AI condition outperforms No-AI on MCQ accuracy (\Cref{tab:study1-baseline-ai-noai}), yet free recall remains
largely unchanged across timing conditions and does not meaningfully differ between AI and No-AI
(\Cref{sec:study1-results-recall}). This pattern is consistent with the AI Buffer Model’s prediction that external
representations primarily support \emph{cue-dependent retrieval} and may not translate into stronger internally
generated retrieval traces (\Cref{sec:ai-buffer-model}). In other words, AI changes the \emph{availability} of
retrieval cues more than the \emph{durability} of self-generated memory.

\paragraph{Second, timing shapes encoding by changing attention allocation, not time-on-task.}
Pre-reading access yields the highest recognition performance on summary-covered information
(\Cref{sec:study1-results-mcq,fig:study1-recognition}), and process measures indicate that pre-reading increases
summary engagement without increasing total time (\Cref{sec:study1-results-process,tab:study1-time-allocation}).
Together, these findings support an \emph{encoding scaffold} interpretation: the external representation is most
effective when encountered early enough to structure subsequent processing, consistent with advance organizer and
cognitive load accounts \citep{Ausubel1960AdvanceOrganizers,Sweller1988CognitiveLoad}.

\paragraph{Third, structure shapes epistemic risk through source monitoring.}
Integrated summaries reduce false-lure endorsement relative to segmented summaries
(\Cref{sec:study1-results-source-monitoring,fig:study1-source-monitoring}). This supports the claim that the
structure of an external representation can either preserve or degrade cues that enable source attribution and
cross-checking \citep{Johnson1993SourceMonitoring,ChandlerSweller1992SplitAttention}. Importantly, these risks are
not well explained by a simple ``overconfidence'' mechanism: calibration is imperfect overall, but AI does not
increase overconfidence (\Cref{sec:study1-results-calibration,fig:study1-calibration}).

Overall, RQ1 is answered by a mechanism-specific conclusion: AI-generated representations can improve recognition
outcomes when they function as early scaffolds, but they can also increase misattribution risks when fragmented, and
they do not automatically improve generative recall.

\subsection{RQ2: How does AI adoption reshape decision-making practices, roles, and responsibility in product management?}
\label{sec:discussion-rq2}

Study~2 suggests that AI adoption in product management is best understood as a dual shift: a \emph{task shift} toward
automation of operational work and a \emph{governance shift} toward monitoring, accountability, and escalation
(\Cref{sec:results-study2}).

\paragraph{Task shift and role reconfiguration.}
Respondents report the strongest workload reductions for routine work and document drafting/content creation
(\Cref{fig:study2-workload-reduction}). In parallel, respondents report positive productivity and decision-making
impacts (\Cref{fig:study2-strategic-impacts}). This aligns with the thesis positioning of AI as an external
representational layer that accelerates synthesis and documentation, enabling more time for prioritization and
strategic work (\Cref{sec:pm-decision-domain}).

\paragraph{Decision support with cross-context variation.}
Perceived decision-making improvement is positive overall and differs across contexts, with Chinese respondents
reporting higher perceived gains than U.S.\ respondents (\Cref{sec:study2-results-decision}). This suggests that the
organizational value of AI may depend on local adoption maturity, tool ecosystems, and governance constraints, even
when overall usage patterns converge.

\paragraph{Responsibility and governance are frequently unresolved.}
Despite widespread tool usage (e.g., ChatGPT as the most frequently reported tool),
survey highlights show that many respondents report unclear ethical ownership and missing guidelines
(\Cref{tab:study2-categorical-highlights}). In practical terms, adoption changes not only \emph{what} PMs do, but
\emph{who} is accountable for failures and for the verification of AI-mediated evidence---a core organizational
dimension of assisted decision-making.

In sum, RQ2 is answered by a practice-level conclusion: AI adoption is associated with operational efficiency and
perceived decision support, but also with a redistribution of responsibility toward evaluation and governance
activities that many organizations have not yet institutionalized.

\subsection{RQ3: How can organizational changes be explained through cognitive mechanisms of AI-assisted memory?}
\label{sec:discussion-rq3}

RQ3 is addressed by integrating the mechanisms observed in Study~1 with the adoption patterns observed in Study~2
(\Cref{sec:results-cross-study,tab:cross-study-integration}). Three bridges are particularly salient.

\paragraph{Bridge 1: Early framing translates into organizational value.}
The pre-reading advantage in Study~1 indicates that external representations deliver the highest benefits when they
shape encoding and attention allocation. In product management, this maps onto practices in which AI is used early to
frame decision spaces (e.g., synthesizing signals into initial hypotheses), after which humans perform integration and
judgement. The implication is not that AI should ``replace'' decision-making, but that its organizational value is
highest when it functions as cognitive infrastructure for problem framing.

\paragraph{Bridge 2: Artifact-driven work emerges from cue-dependent retrieval.}
Study~1 shows a recognition--recall dissociation, consistent with AI supporting cue-based retrieval more than deep
internalization. Study~2 shows that AI reduces documentation and routine workload and supports decision-making.
Together, these findings imply that organizations may increasingly rely on AI-produced artifacts as shared memory
objects. This strengthens the case for systematic documentation practices (what was generated, why it was trusted,
and how it was validated), because the artifact itself becomes part of the decision trace.

\paragraph{Bridge 3: Fragmentation produces misattribution risk and accountability diffusion.}
Structure-driven false memory effects in Study~1 show that fragmented AI representations increase misattribution risk.
Study~2 indicates that accountability is often unclear and governance policies are frequently missing. The integrated
interpretation is that fragmentation at the cognitive level (weak source cues) can scale into fragmentation at the
organizational level (diffuse ownership). This links the AI Buffer Model’s source-monitoring pathway to governance
requirements for traceability and responsibility assignment \citep{Pasquale2015BlackBoxSociety}.

Taken together, the thesis answers RQ3 by showing how micro-level cognitive mechanisms (scaffolding, cue dependence,
source monitoring, and reliance) provide a coherent explanation for macro-level changes in decision practices and
accountability boundaries.

\section{Theoretical Implications}
\label{sec:discussion-theoretical}

The thesis contributes to theory at three levels: cognition, human--AI interaction, and organizational decision-making.

\paragraph{External representations as designable cognitive infrastructure.}
Study~1 shows that timing and structure are not superficial interface features: they are parameters that modulate
memory outcomes. The pre-reading advantage is consistent with the idea that an AI-generated representation can act as
an advance organizer that structures encoding \citep{Ausubel1960AdvanceOrganizers}, while the segmented-structure risk
connects to split-attention and source-monitoring accounts \citep{ChandlerSweller1992SplitAttention,Johnson1993SourceMonitoring}.
This supports a design-oriented view of AI assistance: the cognitive consequences of ``using AI'' depend on how the
representation is integrated into the task pipeline.

\paragraph{AI-assisted memory is cue-dependent and may not generalize to durable internalization.}
The recognition--recall dissociation refines cognitive offloading arguments by distinguishing between (i) enhanced
performance under cue-rich retrieval conditions and (ii) durable generative retrieval without cues. This aligns with
the broader perspective that external memory aids can change what is encoded and how it is retrieved, rather than
uniformly strengthening memory \citep{Tulving1973EncodingSpecificity,Risko2016CognitiveOffloading,Sparrow2011GoogleEffect}.

\paragraph{From individual reliance to organizational responsibility.}
Study~2 extends the cognitive story into organizational contexts by showing that AI adoption reorganizes work toward
artifact-mediated decision processes while simultaneously creating governance gaps. The cross-study integration
provides theoretical support for the thesis claim that reliance is not reducible to subjective trust alone; it is a
structural shift in how cognitive work is distributed across people, tools, and documentation (\Cref{sec:integrated-framework}).

\section{Practical Implications}
\label{sec:discussion-practical}

The results suggest actionable implications for AI tool design, for product managers, and for organizations.

\paragraph{Implications for AI tool design.}
Design choices should explicitly support (i) early-stage scaffolding and (ii) strong provenance cues. Concretely,
tools can provide ``preview'' modes (pre-reading outlines), integrated views that reduce split attention, and
traceability features (links to sources, persistent citations, and structured decision logs). Because fragmented
representations increase misattribution risk, interface patterns that scatter outputs across channels without
provenance should be treated as a safety concern rather than a convenience feature.

\paragraph{Implications for product managers.}
PMs can treat AI as a framing and synthesis tool rather than a substitute for judgement. When using AI-generated
summaries, the findings support a workflow in which AI outputs are used \emph{before} deeper analysis to structure
attention (analogous to pre-reading), followed by explicit verification steps for high-stakes claims. The results
also motivate maintaining integrated artifacts (one consolidated brief, one decision record) rather than multiple
uncoordinated outputs, to reduce source confusion during later decision reviews.

\paragraph{Implications for organizations and governance.}
Organizations should formalize accountability for AI-mediated decisions: who validates AI-generated evidence, what
counts as sufficient verification, and how exceptions are escalated. Training programs can focus on source
monitoring, prompt hygiene, and documentation standards. Finally, because responsibility and guideline structures
vary across contexts in Study~2, governance frameworks should be adapted to local regulatory and cultural settings
while preserving a common core: provenance, auditability, and clear ownership.

\section{Limitations}
\label{sec:discussion-limitations}

Several limitations constrain the interpretation and generalization of the findings.

\paragraph{Study 1 limitations.}
The experiment uses a controlled task environment with a limited sample and a finite set of stimuli. The AI
assistance is operationalized as summaries rather than fully interactive dialogue, and the outcomes focus on
short-term performance. These choices strengthen internal validity but limit conclusions about long-term retention,
expert populations, and interactive AI workflows.

\paragraph{Study 2 limitations.}
The field study relies on self-reported perceptions in a cross-sectional survey. Causal claims about productivity or
decision quality cannot be inferred, and results may reflect selection bias (e.g., respondents who adopt AI more
actively may be more likely to participate) and measurement limitations in cross-cultural survey design.

\paragraph{Cross-study limitations.}
The multi-level integration is explanatory rather than a statistical mediation test: the studies involve different
tasks and populations, and cognitive variables are not directly measured in the field context. The integration
therefore provides a coherent account of plausible mechanisms, but it should be treated as a framework for future
multi-level validation.

\section{Future Research Directions}
\label{sec:discussion-future}

The thesis motivates several directions for future work.

\begin{itemize}
    \item \textbf{Longitudinal cognitive outcomes:} test whether timing and structure effects persist over longer
    retention intervals and in repeated-use settings where offloading may accumulate.
    \item \textbf{Interactive AI and provenance interventions:} evaluate whether inline citations, uncertainty cues,
    and structured verification prompts reduce false-lure endorsement without reducing productivity.
    \item \textbf{Decision-quality measures in the field:} complement self-reports with behavioral logs, decision
    audit outcomes, and objective performance metrics in product teams.
    \item \textbf{Multi-level causal designs:} run field experiments that manipulate AI representation design in
    real workflows (e.g., pre-reading vs post-reading briefs; integrated vs fragmented outputs) while measuring both
    cognitive and organizational outcomes.
    \item \textbf{Boundary conditions:} examine moderators such as domain expertise, time pressure, governance
    maturity, and regulatory context to specify when AI functions as beneficial cognitive infrastructure versus a
    source of epistemic risk.
\end{itemize}

\chapter{Conclusion}
\label{ch:conclusion}

\section{Summary of Findings}
This thesis investigated how generative AI reshapes knowledge-intensive decision-making by simultaneously affecting
individual cognitive processes and organizational practices. The multi-level design combined a controlled experiment
on AI-assisted memory (Study~1) with a cross-national field study of product managers (Study~2), integrated through
the AI Buffer Model (\Cref{sec:ai-buffer-model}).

At the cognitive level, AI-generated summaries improve recognition performance but do not automatically strengthen
generative retrieval: MCQ accuracy increases with AI assistance, while free recall remains largely unchanged
(\Cref{sec:results-study1}). Timing is a key design parameter: pre-reading access yields the strongest learning
effects for summary-covered information and changes attention allocation toward the summary without increasing total
time on task. Structure is a key safety parameter: segmented presentations increase false-lure endorsement and thus
raise epistemic risk through source-monitoring vulnerabilities.

At the organizational level, product managers report moderate-to-high adoption of AI tools and substantial perceived
workload reduction for routine work and documentation (\Cref{sec:results-study2}). AI use is also associated with
perceived improvements in productivity and decision-making, alongside persistent governance and accountability gaps:
many respondents report unclear ownership for ethical AI decisions and missing guidelines, with cross-national
differences in responsibility and guideline structures (\Cref{sec:study2-results-governance}).

\section{Contributions of the Thesis}
The thesis contributes in three ways. First, it provides an integrative conceptual contribution by framing AI
assistance as \emph{AI-assisted external representation} and by proposing the AI Buffer Model as a mechanism-based
account of how timing and structure shape both performance and epistemic risk. Second, it offers empirical
contributions across levels: controlled evidence that design parameters (timing and structure) drive distinct memory
outcomes, and field evidence that adoption reshapes product-management work and governance perceptions in practice.
Third, it provides actionable implications: designing AI representations to support early-stage scaffolding and
provenance cues, and institutionalizing organizational practices for verification, traceability, and clear
accountability.

\section{Final Answers to RQ1--RQ3}
\textbf{RQ1.} AI-generated external representations influence memory through design-sensitive mechanisms: they improve
cue-supported recognition when encountered early as encoding scaffolds, but they do not necessarily improve free
recall and can increase misattribution risk when fragmented.

\textbf{RQ2.} In product management, AI adoption is associated with a shift from operational workload toward more
strategic activities and perceived decision support, alongside a governance shift in which responsibility and
guideline structures are frequently unclear and vary across contexts.

\textbf{RQ3.} Organizational changes can be explained through cognitive mechanisms: AI-driven shifts toward
artifact-mediated decision-making map onto cue-dependent retrieval and offloading, while governance and
accountability challenges map onto source-monitoring vulnerabilities and the propagation of AI-generated artifacts
through organizational memory systems.

\section{Final Remarks}
Taken together, the findings support a central thesis claim: the impact of generative AI on knowledge work is not
determined solely by whether AI is used, but by how AI-generated representations are embedded into cognitive and
organizational processes. Designing for early scaffolding, integrated presentation, and provenance transparency---and
matching these design choices with organizational verification and accountability practices---is essential for
realizing the benefits of AI assistance while reducing epistemic and governance risks.

\cleardoublepage
\bibliography{Thesis_bibliography} % The references information are stored in the file named "Thesis_bibliography.bib"
\cleardoublepage

\appendix
\chapter{Appendix A: Experimental Materials}
\chapter{Appendix B: Survey Instrument}
\chapter{Appendix C: Statistical Outputs}
\chapter{Appendix D: Ethical Protocol}

\chapter{Appendix A}
If you need to include an appendix to support the research in your thesis, you can place it at the end of the manuscript.
An appendix contains supplementary material (figures, tables, data, codes, mathematical proofs, surveys, \dots)
which supplement the main results contained in the previous chapters.

\chapter{Appendix B}
It may be necessary to include another appendix to better organize the presentation of supplementary material.


% LIST OF FIGURES
\listoffigures

% LIST OF TABLES
\listoftables

% LIST OF SYMBOLS
% Write out the List of Symbols in this page
\chapter*{List of Symbols} % You have to include a chapter for your list of symbols (
\begin{table}[H]
    \centering
    \begin{tabular}{lll}
        \textbf{Variable} & \textbf{Description} & \textbf{SI unit} \\\hline\\[-9px]
        $\bm{u}$ & solid displacement & m \\[2px]
        $\bm{u}_f$ & fluid displacement & m \\[2px]
    \end{tabular}
\end{table}

% ACKNOWLEDGEMENTS
\chapter*{Acknowledgements}
Here you might want to acknowledge someone.

\cleardoublepage

\end{document}
